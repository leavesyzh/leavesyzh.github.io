[{"title":"GNSS基础知识总结","url":"/2024/05/22/GNSS%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/","content":"一.时间系统时间系统规定了时间测量的标准，包括时刻的参考基准（起点）和时间间隔测量的尺度基准。\n时间系统是由定义和相应的规定从理论上来进行阐述的，而时间系统框架则是通过守时、授时以及时间频率测量和比对技术在全球范围内或某一区域内来实现和维持统一的时间系统。\n1.时间系统基本概念（1）常用的时间基准有：地球自转（稳定度为）、行星绕日公转（稳定度为）、原子中电子的能级跃迁的振荡频率（稳定度为）、脉冲星的自转周期（稳定度为）。\n（2）守时系统被用来建立和维持时间频率基准，确定任一时刻的时间。守时系统通过时间频率测量和比对技术来评价系统内不同钟的稳定度和准确度，并据此给各个时钟以不同的权重，从而使用多台钟来建立和维持时间系统框架。\n（3）授时系统使用电话、电视、计算机网络、专用的长波和短波无线电信号、搬运钟和卫星等设备将时间系统所维持的时间信息和频率信息传递给用户。\n2.常见的时间系统（1）恒星时以春分点作为参考点，由于地球自转使春分点连续两次经过地方上子午圈的时间间隔为一恒星日，是一种地方时。由于岁差和章动的影响，地球自转轴在空间的方向会不断变化，因此春分点分为真春分点和平春分点，恒星时分为真恒星时和平恒星时。常见的有格林尼治真恒星时（CAST）和格林尼治平恒星时（GMST），均为春分点与经度零点的夹角，其中平恒星时经过了岁差和章动的修正。\n（2）太阳时以太阳中心作为参考点，太阳中心连续两次通过某地的子午圈的时间间隔为一太阳日。太阳时以地球自转为基础，是一种地方时。由于地球公转轨道为椭圆，且地球公转和自转不在一个平面上，真太阳时不具备作为一个时间系统的基本条件。平太阳时以地球自转为基础，以虚拟的平太阳中心作为参考点，其周年视运动轨迹位于赤道面而不是黄道面，且其在赤道上的角速度恒定，等于真太阳的平均角速度。平太阳时也是一种地方时，位于不同经线上的地区的平太阳时是不同的。\n（3）世界时以地球自转周期作为时间基准，是格林尼治起始子午线处的平太阳时。直接根据天文观测测定的世界时为（），经过极移改正后的世界时为（），再经过地球自转速度季节性改正的世界为（）。\n（4）位于海平面上的铯133原子基态两个超精细能级间在零磁场中跃迁辐射振荡9192631770周所持续的时间定义为原子时的1s，原子时的起点规定为1958年1月1日0h整，此时原子时AT和世界时UT之间相差0.0039s。国际原子时（）由国际计量局（）根据全球各个时间实验室的原子钟的数据统一处理得到。协调世界时（）的秒长严格等于原子时的秒长，且通过跳秒的方式将其与世界时（）的时刻差保持在0.9s以内。\n（5）GPS时由GPS的地面监控系统和GPS卫星中的原子钟建立和维持，起点为1980年1月6日0h0m0s，在起始时刻与对齐，与国际原子时之间相差19s。\n二.空间系统坐标系统是由一系列的原则、规定，从理论上来加以定义，其具体的实现则被称为坐标框架或参考框架。\n1.岁差和章动（1）岁差是由于天球赤道和天球黄道的长期运动而导致的春分点的进动，岁差的存在会导致每个时刻的瞬时天球坐标系的三个坐标轴的指向出现偏差。\n（2）章动是一种比岁差微小的周期性变化，由于月球、太阳和各大行星与地球间的相对位置存在周期性变化，作用在地球赤道隆起部分的力矩也在发生变化，地月系质心绕日公转的轨道面也存在周期性的摄动。\n2.常见的坐标系（1）天球坐标系根据所选的坐标原点不同可以分为站心天球坐标系，地心天球坐标系和太阳质心天球坐标系。在GPS测量中使用较多的是地心天球赤道坐标系，该坐标系的原点位于地球质心，X轴指向春分点，Z轴与地球自转轴重合，指向北天极，Y轴垂直于X轴和Z轴，组成右手直角坐标系。仅顾及岁差而不顾及章动的北天极和春分点为平北天极和平春分点，同时顾及岁差和章动的北天极和春分点为真北天极和真春分点。由于岁差和章动的存在，真天球坐标系中的三个坐标轴的指向在不断变化，因此能反映真实的位置，但是天体的位置需要在一个固定不变的坐标系中描述，因此需要协议天球坐标系（）。的坐标轴分别指向空间三个固定的方向，虽然坐标原点在绕日公转，但仍被称为准惯性坐标系。\n（2）地球坐标系也称大地坐标系，地固坐标系，与地球一起自转，用于描述地球上和近地空间中的物体的位置。极移是指由于地球表面和地球内部的物质运动使极点的位置产生变化。极移的存在会使瞬时地球坐标系中三个坐标轴在地球本体内的指向出现变化，因此出现了协议地球坐标系（），的精密星历就是采用这一框架。GPS卫星的轨道运动方程通常是在中建立和解算，然后再将坐标转换到中。\n三.载波、测距码与导航电文1.载波可运载调制信号的高频振荡波称为载波。GPS卫星使用的载波分别称为L1载波和L2 载波。其中L1 载波是由卫星上的原子钟所产生的基准频率( ) 倍频 154 倍后形成的，即 , 其波长为19.3cm，L2载波是由基准频率倍频120倍后形成的,即，其波长 为24.42cm。\n采用两个不同频率载波的主要目的是为了较完善地消除电离层延迟。采用高频率载波的目的是为了更精确地测定多普勒频移和载波相位(对应的距离值) ，从而提高测速和定位的精度，减少信号的电离层延迟，因为电离层延迟与信号频率f的平方成反比，同时较高频率的信号受到水汽吸收和氧气吸收谐振严重。\n2.测距码测距码是用于测定从卫星至接收机间的距离的二进制码。测距码是由若干个多级反馈移位寄存器所产生的m序列经平移、截短、求模二和等一系列复杂处理后形成的伪随机噪声码。\n（1）C/A码C/A码是由两个周期性的二进制码序列G1和G2进行模二相加后形成的，C/A码的周期为1ms，一个周期中含有1023个码元，每个码元持续的时间均为1ms/1023=0.97752us，对应的码元宽度为293.05m，测距精度一般为一个码元宽度的1/100，即测距精度为2.93m。\n（2）P码P码是由X1和X2两个二进制码序列模二相加后产生的，其中X1又是由X1A和X1B两个子序列求模二和后产生的。P码的实际周期为一星期，一个周期中约含6.2万亿个码元。每个码元持续的时间为C/A码的1/10,对应的码元宽度为29.3m，所有P码的测距精度约为0.3m。\n（3）调制在载波上的民用测距码载波的频率为1176345MHz，是由原子钟频1023.MHz倍频115倍后形成的，载波由两个相互正交的分量组成，一个是同相分量，一个是正交分量，两个互相同步、几乎正交、结构不同的测距码被分别调制在这两个载波分量上。\n3.导航电文导航电文是由GPS卫星向用户播发的一组反映卫星在空间的位置、卫星的工作状态、卫星钟的修正参数、电离层延迟修正参数等重要数据的二进制代码，也称数据码(D码)。\n导航电文以 帧为单位向外发送。一个主帧的长度为1500bit，发送速率为50bit/s，播发完一个主帧需要30s。一个主帧包括5个子帧，每个子帧均为300bit，播发时间为6s。每个子帧由10个字组成，每个字均为30bit，播发时间为0.6s。其中第四、五两个子帧各有25个不同的页面，25个页面全部播发完需要750s，其中第一、 二、三子帧每 30s 重复一次,其内容每隔 2h 更新一次，第四、五子帧每 30s 翻转一页，内容在地面站输入新的历书后才更换。\n（1）第一子帧第一子帧包含遥测字（），交接字（），星期数（），用户测距精度（）的指数，卫星健康状况（），L1和L2的群延之差（）,卫星钟参数的数据龄期（），卫星钟误差系数（），以及卫星钟参数的参考时刻（）等。\n注意事项：\n1）信号群延是从信号生成到离开卫星天线相位中心的时间。由于L1和L2是通过不同的电路产生的，它们的群延不同，其公共部分会被自动吸收到卫星钟差中，两者之间的平均群延差即，与信号在电离层中的传播无关。\n2）由于L1和L2信号离开卫星天线的时间不同，用两者测定的卫星钟差也不同，而导航电文中给出的卫星钟差系数则是全球定位系统的地面控制系统利用双频接收机的观测数据计算的，双频用户可以直接使用这些数据，而L1和L2的单频用户需要额外的改正。\n（2）第二子帧和第三子帧第二子帧和第三子帧用来描述GPS卫星轨道，利用这些参数可以求出导航电文有效时间段内任一时刻t卫星在空间中的位置和速度。具体包括星历参考时刻时的轨道根数和9个轨道摄动参数。\n1）卫星轨道的表示方法包括：\n（1 按规定的时间间隔直接给出不同历元卫星在空间中的位置以及运动速度，IGS的精密星历采用的就是这种方式，后面需要使用切比雪夫多项式或者拉格朗日多项式拟合和内插；\n（2 用一个高阶多项式（10阶）来拟合某一时间段内的卫星轨道（1~2h），该方法也叫 Collections法，Bernese软件采用的是这种方式，优点是计算速度快，轨道连续，但是几何意义不明确；\n（ 3 用开普勒轨道根数及其变化率来描述卫星轨道，广播星历采用的是这种方式，需要用多项式拟合来计算观测瞬间的坐标。\n2）第二子帧和第三子帧给出的轨道根数包括平近点角，轨道偏心率，轨道长半径的平方根，轨道倾角，近地点角距，并未直接给出参考时刻的升交点赤经，而是给出了升交点赤经与格林尼治真恒星时（CAST）之差，进行转换才能使用。\n（3）第四子帧和第五子帧第四子帧和第五子帧各含25页，以较少的比特数给出了其他GPS卫星的概略轨道，概略的卫星钟误差参数，以及其他卫星的健康状况，称为卫星历书。除此之外，还包括电离层延迟参数，有关UTC的参数等。\n（4）用广播星历计算卫星位置卫星运动的平均角速度—&gt;观测瞬间卫星的平近点角—&gt;偏近点角—&gt;计算真近点角—&gt;升交角距—&gt;摄动改正项—&gt;卫星在轨道面坐标系中的位置—&gt;观测瞬间升交点的经度—&gt;卫星在瞬时地球坐标系中的位置—&gt;卫星在协议地球坐标系中的位置\n四.GPS定位中的误差源1.与卫星有关的误差（1）卫星轨道误差卫星的轨道误差是指由卫星星历计算的卫星位置与卫星的真实轨道位置之间的差异。星历误差的大小主要取决于卫星定轨系统的质量，如定轨站的数量及其地理分布、观测值的数量及精度、定轨时所用的数学力学模型和定轨软件的完善程度等。此外，与星历的外推时间间隔也有直接关系。\n一般情况下，卫星单点定位误差的量级大体上与卫星星历误差的量级相同，因而广播星历通常只能满足导航和低精度单点定位的需要，进行厘米级的精密单点定位时必须要使用高精度的精密星历。\n（2）卫星钟差卫星钟差是指卫星的星载原子钟的钟面时与GNSS的系统时之间的差异，主要由于卫星钟的频率漂移和不稳定等因素引起。卫星钟差对精密单点定位的影响主要是由卫星钟差引起的站星几何距离误差和卫星位置的计算误差。\n精密单点定位中通常将IGS发布的精密星历计算的卫星位置和钟差当成已知值，因此卫星轨道的误差和卫星钟差误差直接体现在站星几何距离中，最终影响用户的定位精度。\n某一时钟在时刻 t 的钟差一般可表示为：\n$$\\Delta\\mathrm{t}:=:\\mathrm{a}_0:+:\\mathrm{a}_1:(:\\mathrm{t}:-:\\mathrm{t}_0:):+:\\mathrm{a}_2:(:\\mathrm{t}:-:\\mathrm{t}0:)^2:+:\\int{:t_0}^t\\mathrm{y}(:\\mathrm{t}):\\mathrm{d}\\mathrm{t}$$式中：为  时刻该钟的钟差； 为  时刻该钟的钟速( 频偏)； 为  时刻该钟的加速度的一半(也称钟的老化率或频漂项)。数值可由地面控制系统依据前一段时间的跟踪资料(将卫星钟的钟面时与标准的 GPS 时进行比对) 得到，然后再根据该钟的特性来加以预报，并编入卫星导航电文播发给用户。\n精密星历中的轨道和钟差都是以等间隔给出，而用户接收机的采样间隔往往小于星历产品，因此需要进行星历插值。对于卫星轨道的插值，通常采用拉格朗日多项式或切比雪夫多项式进行插值，而卫星钟差采用低阶多项式插值即可满足精度要求。\n（3）卫星天线相位中心偏差和变化GNSS 观测值量测的距离是卫星天线相位中心到接收机天线相位中心之间的距离，而 精密星历计算的卫星轨道是卫星质量中心的位置，卫星质量中心与天线相位中心之间偏差即为天线相位偏差（Phase Center Offset, PCO）。卫星的天线相位中心的位置并非固定不变，会随着卫星的姿态和位置的不同而变化，称为天线相位中心变化（Phase Center  Variation, PCV）。天线相位中心和发送信号的频率有关。\n当前，IGS推荐采用绝对天线相位中心改正模型，最新版本为igs14_wwww.atx。其中wwww表示GPS周，以ANTEX（Antenna Exchange Format）文件格式发布，包含各卫星 导航系统的PCO和PCV改正值。用户进行精密单点定位时需顾及卫星的天线相位中心改正，所采用的ANTEX文本版本应该与IGS分析中心生成精密星历时所用的ANTEX文本版本相匹配。\n（4）相对论效应由于卫星钟和接收机钟所处的状态不同而引起两台钟之间产生相对钟误差的现象。相对论效应主要取决于卫星的运动速度和所处位置的重力位，而且是以卫星钟的钟误差的形式出现的，这些误差对测距码伪距观测值和载波相位观测值的影响是相同的。\n根据广义相对论，由于地球重力场的存在，地球周围的时空是弯曲的，因此 GNSS 信号在空间中的传播的路径会发生弯曲，该部分影响被称为引力延迟改正：式中，为地心引力常数， 为卫星至地心的距离， 为测站到地心的距离，为测站到卫星的距离，为光速。\n根据广义相对论和狭义相对论，由于卫星和地面测站两处的重力场差异以及卫星的运动会对卫星原子钟的频率产生影响，相对论效应对卫星钟的影响可分为两项，对于确定的卫星轨道，第一项为常数，在卫星发射前已经对卫星中频率进行校正，用户不必考虑此项改正；第二项为偏心改正，其改正公式为：式中，为卫星的位置向量，为卫星的速度向量。计算 GNSS 信号发射时刻的卫星位置和站星几何距离时必须顾及偏心改正。\n（5）信号在卫星内的时延GNSS距离测量测定的是从卫星发射天线的相位中心至接收机接收天线相位中心之间的距离，通常把在卫星钟驱动下开始生成测距信号至信号生成并离开发射天线相位中心的时间称为信号在卫星内部的时延。信号内部时延的存在使距离观测值与卫星钟差无法实现无缝对接。\n不同的测距信号是通过不同的电子元器件和电子线路生成，因此不同的信号在卫星内部的时延不同，其差异称为差分码偏差（DCB）或内部频间偏差（IFB）。码偏差主要由硬件偏差和固件偏差两部分引起，通常将其统称为硬件延迟偏差，其中硬件偏差由卫星/接收机天线通道时延引起，固件偏差由卫星/接收机数字/模拟滤波器等时延引起。\n因为难以精确测定每种信号的内部时延，而测定每种信号的内部时延的差值（DCB）相对简单，通常选择一种经常使用的测距信号，如无电离层组合，来测定吸收了信号内部时延的卫星钟差，并通过导航电文播发给用户，用户在使用这种被选择的测距信号定位时就不需要考虑信号内部时延，而在使用其他测距信号时，需要根据这种测距信号与被选择的测距信号间的时延差对卫星钟差加以修正来获得与测距信号相对应的卫星钟差。\n在GPS系统中，导航电文中播发给用户的卫星钟差参数是用由码和码组成的无电离层组合观测值来测定和预报的。用户若用组合观测值来进行导航定位，则可以直接使用导航电文中的卫星钟差，且无需考虑信号内部时延。而对单独采用码的用户，测距时的卫星钟差与播发的卫星钟差之差为：式中，称为群延迟，该参数在卫星导航电文中给出。用户户可依据导航电文中给出的卫星钟差  及参数  来求得 。\n差分码偏差（DCB）是两种信号内部时延的差值，和群时延（TGD）存在比例关系，具体表示为：\n2.与信号传播有关的误差（1）电离层延迟误差电离层延迟一直是影响GNSS导航定位精度的主要误差源，其大小从几米到几十米不等，在单天内变化明显，且卫星的高度角越低，电离层延迟越大，在导航定位中必须进行电离层延迟改正。\n对于单频用户通常采用电离层模型进行改正，如Klobuchar模型，NeQuick 模型，GIM格网等。电离层延迟的改正精度直接影响伪距单点定位及单频精密单点定位的定位精度，是仅次于接收机钟差的对定位结果影响第二大的误差源，可以造成30m的误差。\n由于电离层模型的改正精度有限，对于精密单点定位，双频用户可采用消电离层组合来消除电离层一阶项的影响，或者将电离层斜向延迟当成参数直接估计。\n（2）对流层延迟误差一般认为高度50km以下的中性大气层为对流层，GNSS信号穿过对流层时会发生折射，由此造成的信号延迟即为对流层延迟。对于GNSS载波频段，对流层属于非弥散介质， 即不同频率的信号所产生的对流层延迟是一致的。对流层延迟的大小与测站的位置、温度、 湿度和大气压有关，其中天顶方向对流层延迟较小，约2-3m，但随着卫星高度角降低逐渐增大，对流层延迟斜向延迟可达几十米。\n天顶方向的对流层延迟可以分为对流层天顶干延迟（ZHD） 和对流层天顶湿延迟（ZWD），然后通过映射函数投影到各个方向。将对流层延迟表示为天顶延迟和映射函数乘积的形式：式中，为天顶干延迟，为天顶湿延迟，分别为干延迟和湿延迟映射函数。 对流层天顶干延迟 ZHD 约占对流层延迟总量的 80~90%, 主要由大气中的干燥气体引起，可以由对流层折射模型精确改正。常用的对流层模型有 Sasstamoinen 模型、Hopfield 模型、 和 UNB3 模型等。对流层天顶湿延迟 ZWD 主要由大气中的水汽引起，由于水汽含量变化具有复杂性和随机性，对流层天顶湿延迟变化较大，很难用模型准确计算。在精密单点定位中，通常将天顶湿延迟当成参数进行估计。\n（3）多路径效应卫星信号传播时若遇到遮挡，会经过一系列反射后再被接收机天线接收，正常信号和经过反射的信号叠加会使得观测值发生偏差，称作多路径效应。由于测站周围环境难以预测，多路径效应在GNSS定位过程中很难控制。目前消除或减弱多路径的方法有硬件处理方法和软件处理方法，硬件方法通过合适选址、抑制 天线、增长观测时间等物理方法来削弱影响，软件方法通过小波分析、半参数法等算法来消除或减弱影响。其中，表示：\n（4）地球自转改正GNSS数据处理一般都在协议地球坐标系中进行，即地面测站和卫星均用地固坐标系来表示。卫星在空间的位置如果是根据信号的发射时刻来计算的，那么求得的是卫星在时刻的协议地球坐标系中的位置（）。当信号于时刻到达接收机时，协议地球坐标系将围绕地球自转轴（z轴）旋转一个角度：式中，为地球自转角速度，此时卫星坐标将产生下列变化：将上述改正加到（）上即可得到卫星在时刻的协议坐标系中的坐标，因为所有的计算都是在时刻的协议坐标系中进行的。式中的（）即为卫星位置的地球自转改正。\n由于卫星位置改变，相应的卫星与接收机之间的距离也会产生变化：当卫星的截止高度角取15°时，赤道上的测站由于地球自转引起的卫星与接收机间距离的误差可以达到36m。\n（5）天线相位缠绕GPS卫星信号采用右旋极化波，当卫星发射天线或接收机天线自己的纵轴旋转时，发射天线和接收机天线之间存在相对旋转，此时载波相位观测值会发生变化，其值最大可达1周。\n在静态定位中接收机天线固定不变，在动态定位中接收机天线旋转导致的天线相位缠绕误差会被自动吸收到接收机钟差中，因此一般不考虑接收机端的天线相位缠绕。而卫星由于需要将太阳能帆板对准太阳，会不断地进行旋转，因此天线相位缠绕一般指由于卫星天线旋转引起的相位误差。相位缠绕对单点定位的影响可以达到分米级，具体改正公式为：式中，其中，是卫星到接收机的单位向量，是由星固坐标系下的单位向量（）计算得到的卫星有效的偶极向量，是由地方坐标系下的单位向量（）计算得到的接收机有效的偶极向量。\n3.与接收机有关的误差（1）接收机钟差在PPP数据处理中，GNSS接收机携带的钟通常为稳定性差、精度低的石英钟，接收机钟面与标准时间必定存在一个偏差。在PPP数据处理过程中，接收机钟差一般不能利用星间差分来消除，其对精密单点定位的影响主要体现在对计算卫星坐标的影响和对站星间距离观测值的影响，对前者的影响可以通过在标准单点定位过程中先求出接收机钟差的大概值来消除，对后者的影响，可将其作为未知参数求解。\n（2）接收机天线相位中心偏差类似卫星天线相位中心改正，接收机天线相位中心改正包含PCO和PCV改正。其中，接收机PCO为接收机平均天线相位中心与天线参考点之间的偏差。接收机PCV为天线相 位中心在不同高度角和方位角的瞬时位置与其平均位置的差值。  接收机天线相位中心改正采用IGS推荐的绝对天线相位中心改正模型，最新版本为 igs14_wwww.atx，其中wwww表示GPS周。\n（3）信号在接收机内时延卫星测距信号在到达接收机天线的相位中心后还需花费一段时间来进行信号的放大、滤波及各种处理之后才能进入码相关器与来自接收机的复制码进行相关处理以获得测码伪距观测值，以及进入载波跟踪回路以获取载波相位观测值。\n同样从在接收机钟信号的驱动下开始生成复制码至复制码生成并最终进入相关器进行相关处理也需花费一段时间，和一般并不相等，两者之差称为接收机内的时延。信号在接收机内的时延的存在使距离观测和接收机钟差无法实现无缝对接。\n（4）地球潮汐改正1）固体潮改正由于地球并非刚体，在太阳和月球的引力作用下，固体地球会产生弹性形变，即固体潮。固体地球潮汐引起的测站缓慢变化与测站的地理位置有关，对测站的高程和水平方向的影响分别可达30cm和5cm，因此精密单点定位中必须考虑该项影响。由于固体潮包含多种周期项影响，无法通过全天的位置序列取平均来消除其影响，可以通过IERS Conventions推荐的模型进行改正，改正精度可达毫米级。$$\\begin{aligned}\\Delta\\vec{r}&amp;=\\sum_{j=1}^2\\frac{GM_jr^4}{GMR_j^3}\\left{\\left[3l_2\\left(\\vec{R}_j\\cdot\\vec{r}\\right)\\right]\\vec{R}j+\\left[3(\\frac{h_2}2-l_2)\\left(\\vec{R}j\\cdot\\vec{r}\\right)^2-\\frac{h_2}2\\right]\\vec{r}\\right}+\\&amp;\\left[-0.025\\sin\\phi\\cos\\phi\\sin(\\theta_g+\\lambda)\\right]\\vec{r}\\end{aligned}$$式中，为测站位移向量；下标 表示太阳(j=1)或月亮(j=2)； 表示地球引力质量常数( 3.986005×10^{14}m^3/s^2)；表示太阳 (  ) 或月亮 (  ) 引力质量常数；为地球半径；$\\vec{R}j表示太阳j=1或月亮j=2的位置向量；\\vec{r}表示测站位置向量；l{2}为第二勒夫数，h{2}为第二Shida数，取值分别为和；\\lambda和\\phi为测站经纬度东经为正；\\theta{g}$为格林尼治平恒星时。\n2）海潮负荷改正在日月引力的作用下，海洋潮汐产生周期性涨落，从而引起海床和海岸的形变和地球质量分布的变化，即海潮负荷效应。海潮负荷主要影响近海岸地区的测站，它对测站在水平和高程方向影响可达2-5cm。对于远离海边 （&gt;1000KM）的测站可以不用考虑该项影响。海潮负荷主要包含日周期和半日周期，对测站的高程和水平方向的影响在厘米量级，可以通过IERS Conventions推荐的模型进行改正。式中，下标表示 11 个不同的分潮(、、、、、、、、$M_{f}、M{m}、S{{sa}}\\quad f{j}、A{{cj}}、\\omega{j}、\\chi{j}、\\mu{j}、\\Phi{_{cj}}$为各分潮的模型系数。不同测站的模型系数可以通过网站 http://holt.oso.chalmers.se/loading 在线生成。\n3）极潮改正由于极移的存在，地球瞬时自转轴在地球表面的位置是缓慢变化的，导致地球的重力场发生细微变化，由此引起地球表面的弹性响应称为极潮。极潮对测站位置的影响可达厘米量级，可以通过IERS Conventions推荐的模型进行改正。$$\\Delta\\vec{r}=\\ m_{1}=x_{P}-0.054-0.00083(t-t_{0})\\m_{2}=-\\mathrm{y}_{P}+0.357+0.00359(t-t{_0})$$式中，t为改正时刻，()为对应时刻的极移，表示参考历元​。\n4.误差项总览\n五.GNSS中的硬件偏差GNSS伪距和载波相位观测方程中，设备时延通常作为方程中的附加项，包括接收机和卫星相关的设备时延参数。而在实际应用中，卫星和接收机的设备时延与相应的钟差高度相关，相位设备时延也与模糊度有关，不能完全分离，同时卫星与接收机的设备时延也无法完全区分开。因此，一般在数据处理过程中，设备时延作为相对偏差进行解算。在不同的应用中需要考虑的设备时延略有差异。\n在单个GNSS系统中，涉及到两个及以上信号的伪距处理时，由于不同信号/频率在通道中的传输时间不一样，需要考虑信号间相对伪距设备时延，即码间偏差（DCB）。在进行模糊度解算时，为了固定整周模糊度，出现了关于未校准的相位时延（UPD）的研究。当使用多个频率的载波信号时，不同频率组合下的卫星钟差存在一定差异，即频间钟差偏差（IFCB）。在进行多个系统联合处理时，需要考虑系统间偏差（ISB）。\n（1）码偏差（DCB）根据信号机制，可将DCB分为频内DCB和频间DCB，前者是指在同一个载波上因测距码不同造成的设备时延偏差，而后者指由于载波频率不同产生的设备时延偏差。由于延迟产生的位置不同，又可分为接收机DCB和卫星DCB。\n关于频内DCB的处理，可通过伪距观测值直接做差解算。因为两个码信号调制在相 同的载波频率上，可以忽略电离层的影响，伪距观测方程为：将两个码P1, P2的伪距观测值直接做差可得DCB：（）由于伪距观测值形成的观测噪声较大，无法完全忽略。可将上式观测值在一天内取平均值，削弱噪声，得到综合DCB观测值。\n频间DCB参数可采用事先标定和软件估算两种方式得到。卫星和接收机在出厂前，通常会标定设备时延，用户可直接使用。随着设备使用时间的增加，卫星和接收机老化导致硬件性能会发生变化，同时在外界环境等多种因素的影响下，实际值与标定值产生差异 。\n（2）未校准的相位时延（UPD）在初始相位、卫星和接收机设备时延及其他因素的影响下，非差处理算出的载波相位模糊度包含了其他偏差，被模糊度吸收的这部分相位偏差表示为UPD。由于整周模糊度计数不明确，UPD的整数部分与模糊度完全耦合，无法分离，一般只能测定不足载波信号一周的部分，将模糊度不为整周的小数部分称为小数周偏差（FCB）。\n由于UPD的存在，精密单点定位直接计算出来的模糊度不具有整数特性，是一个浮点解。UPD产生的原因主要有：\n1）由于卫星和接收机产生的载波信号并非从零相位开始，并且初始相位未知，无法将其从模糊度中分离；\n2）不同信号在卫星端和接收机端传输中，由于信号的差异，会产生不同信号间设备时延，该延迟会与模糊度耦合；\n3）PPP使用的精密钟差产品主要是IGS或者是与IGS解算方式相近的精密产品，它们在计算钟差时一般使用无电离层组合形式并以伪距观测方程的钟差为基准；由于其精度不高，只有厘米级，在载波相位观测 方程中，使用该钟差基准时，模糊度参数也会吸收部分钟差。\n使用FCB进行模糊度固定的方法主要有星间单差法和整数钟法。\n星间单差法利用多个(100左右)IGS地面观测网算出非差模糊度的浮点解，然后在卫星间做差消除接收机端的FCB；再利用MW组合估计宽巷模糊度，将其取整得到宽巷FCB改正数；再将宽巷模糊度带入无电离层组合中，固定窄巷模糊度，分离窄巷 FCB 改正数。\n整数钟法则是计算非差宽巷的FCB改正数，利用宽巷FCB改正数来固定宽巷模糊度，然后估计包含了窄巷FCB的卫星钟差。当获得FCB后，直接对无电离层组合下的宽巷和窄巷模糊度进行固定，得到PPP的模糊度整数解，获得高精度的结果。整数钟法可更好地应用到实际中 。\n（3）频间钟差偏差（IFCB）在三频处理中，GNSS三频观测可以组合成两组无电离层组合的形式为：$$\\begin{aligned}&amp;P_{IF_{1,2}}=\\rho+(c\\mathrm{d}t_{\\mathrm{r}}+d_{\\mathrm{r}{1,2}})-(c\\mathrm{d}t^{\\mathrm{s}}+d{1,2}^{\\mathrm{s}})+T+\\epsilon_{P1}\\&amp;L_{IF_{1,2}}=\\rho+(c\\mathrm{d}t_{\\mathrm{r}}+d_{\\mathrm{r}{1,2}})-(c\\mathrm{d}t^{\\mathrm{s}}+d{1,2}^{\\mathrm{s}})+(b_{\\mathrm{r}{1,2}}-d{r_{1,2}}-b_{1,2}^{\\mathrm{s}}+d_{1,2}^{\\mathrm{s}})+\\lambda_{1,2}N_{1,2}+T+\\epsilon_{L1}\\&amp;P_{IF_{1,5}}=\\rho+(c\\mathrm{d}t_{\\mathrm{r}}+d_{\\mathrm{r}{1,5}})-(c\\mathrm{d}t^{\\mathrm{s}}+d{1,5}^{\\mathrm{s}})+T+\\epsilon_{P2}\\&amp;L_{IF_{1,5}}=\\rho+(c\\mathrm{d}t_{r}+d_{r_{1,5}})-(c\\mathrm{d}t^{s}+d_{1,5}^{\\mathrm{s}})+(b_{\\mathrm{r}{1,5}}-d{\\mathrm{r}{1,5}}-b^{\\mathrm{s}}{1,5}+d^{\\mathrm{s}}{1,5})+\\lambda{1,5}N_{1,5}+T+\\epsilon_{L2}\\end{aligned}$$其中1, 2, 5代表频率，,  分别表示无电离层组合下的伪距和载波相位观测值，，，分别表示卫星与接收机间的几何距离，接收机的伪距硬件延迟和卫星的伪距硬件延迟，，分别为接收机和卫星的相位硬件延迟，为无电离层组合载波相位的波长。\n当采用两组无电离层组合进行卫星钟差解算时，参数化的卫星和接收机钟差会吸收对应的硬件偏差，所以会产生两组卫星、接收机钟差，，，，。$$\\left{\\begin{array}{l}IFCB^\\mathrm{s}=\\delta^\\mathrm{s}{}{1,5}-\\delta^\\mathrm{s}{}{1,2}\\[2ex]IFCB_\\mathrm{r}=\\delta_\\mathrm{r}{1,5}-\\delta_\\mathrm{r}{1,2}\\end{array}\\right.$$\n（4）系统偏差（ISB）系统间偏差是多个卫星导航系统进行联合导航、定位等应用时，必须要考虑的偏差 。因为不同的GNSS系统采用的坐标和时间基准不同，不同系统信号的结构、体制等有差异延偏差，当不同GNSS信号在多模接收机通道中传输时，会产生接收机设备相关的时延偏差 。所以利用多系统GNSS数据进行融合解算时，需要考虑这些系统性的偏差。 \n一般认为ISB是一个卫星系统观测值与参考卫星系统(如GPS)的观测值放一起处理时所需考虑的一个改正。在实际应用中，由于系统间坐标基准的差异主要体现在卫星位置上，在多模GNSS数据处理时一般转换成相同的坐标系，可以忽略坐标基准差对ISB的影响。\n所以，ISB主要包括了时间基准之差和接收机设备时延。\n当多模GNSS联合网解时，整网内只估算一个基准钟差。但不同的信号频率和不同的卫星系统会产生不同的设备时延，而这些时延会被接收机钟差吸收，因此不同卫星系统的接收机钟差实际上存在差异，所以需要在不同系统中引入一个偏差参数。当ISB参数与其他轨道产品联合解算时会产生秩亏，因此需要引入一个额外的约束条件。\n目前有两种约束方法：一是选择一个特定的测站，令其ISB为零；二是令整个测站网的ISB和为零。\n六、精密单点定位观测模型七、精密单点定位函数模型八、数据预处理和数据质量的评估方法","categories":["GNSS"],"tags":["GNSS","阅读笔记"]},{"title":"Rtklib使用说明总结","url":"/2024/07/29/Rtklib%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/","content":"1.RTKPOST需要的文件说明\n\n1.观测值文件（observation data）：必须有，可以是自己接收机测得的实际数据，也可以是直接网上下载的观测站的观测值文件。\n2.基准站观测值文件：单点定位不需要\n3.导航电文(navigation data)：必须有。\n4/5：SP3 精密轨道文件、CLK 精密钟差文件，精密星历改正需要。\n6：解算结果，勾选前面的对号可以自己选择输出文件路径。\n7：结算结果文件名，选好输出文件路径后会自动生成结算结果文件名，默认为.pos文件，可以修改名称。\n另一部分更精细的改正，需要在options中进行参数设置。\n\n\n8：卫星天线或接收机天线相位改正。\n9：大地水准面模型，选择外部模型为大地水准面模型后需提供文件。\n10：差分码偏差。\n11：地球定位定向参数。\n12：海潮负荷文件，选择潮汐改正后需提供文件。\n13：电离层格网数据。\n2.网站汇总[GNSS数据下载网址 汇总大全 2020.9.22更新]_卫星观测数据下载-CSDN博客\nCDDIS网站下 GNSS 相关的数据产品下载+命名方式解读+文件格式说明文件下载地址-CSDN博客\n（1）一般观测文件（o、n、sp3）\n\n观测数据\nftp://cddis.gsfc.nasa.gov/pub/gps/data/daily （包括 m文件）\nftp://cddis.nasa.gov/gnss/data/daily/\nhttp://garner.ucsd.edu/pub/（导航电文）\nftp://igs.ign.fr/pub/igs/data/campaign/mgex/daily/rinex3/\nftp://igs.ign.fr/pub/igs/data/campaign/mgex/daily/rinex3/\nftp://cddis.gsfc.nasa.gov/pub/gps/data/daily/\nftp://cddis.gsfc.nasa.gov/pub/gps/data/campaign/mgex/daily/rinex3/\n\n\n广播星历(brdc/brdm)\n(1) ftp下载广播星历文件\nhttp://garner.ucsd.edu/pub/\n(2) brdc 广播星历下载网址\nhttps://cddis.nasa.gov/pub/archive/gnss/daily/\n(3) Code数据\nhttp://ftp.aiub.unibe.ch/CODE/2021/\n(4) 广播星历下载（根据所需的文件下载）\nftp://ftp.pecny.cz/LDC/orbits_brd/gop2/2021/\n(5) 通用广播星历\nftp://epncb.oma.be/pub/obs/BRDC\n\n\n精密星历\nftp://cddis.gsfc.nasa.gov/pub/gps/products\nftp://garner.ucsd.edu/pub/products/ （按周）\nftp://igs.ign.fr/pub/igs/products\nhttp://ftp.aiub.unibe.ch/ \n\n\n电离层改正\n(1) CODE中心：\nhttp://ftp.aiub.unibe.ch/CODE/\nCODE是欧洲定轨中心，提供了全球电离层电子分布图（GIM）；全球电离层球谐系数模型的每小时15阶球谐系数，文件后缀为.ION.Z；全球电离层总电子含量2.5*2逐小时格网值（CODG），文件名为”CODG+3位年积日+0.YYI.Z”；以及1天、2天的预报产品，位于/IONO/目录下。CODE数据集部分时间段的数据有误。\n(2) 中国科学院数据集：\nftp://182.92.166.182/product/final/\n数据集提供全球电离层球谐系数模型每2h的15阶球谐系数。\n\n\n对流层改正\n(1) CDDIS中心：\nhttps://cddis.nasa.gov/Data_and_Derived_Products/GNSS/atmospheric_products.html\nNASA的空间测量数据档案，产品包括300s分辨率的天顶路径延迟（ZPD）估计，IGS网络中有350多个GNSS站点按站点每天提供文件数据。同时还包括GNSS观测站30s分辨率的气象数据，包括温度/℃、大气压/hpa、水汽压/0.1mbar（不同观测站单位不同）。\n(2) 武汉大学IGS数据中心：\nftp://igs.gnsswhu.cn/\nIGS数据中心包含了观测数据（data）和产品（products）两个数据集，data数据集里面包含了许多观测文件，其中m文件即为气象观测文件；products数据集也包含了很多产品数据，各测站的ZPD值位于troposphere子数据集中。\n(3) VMF数据:\nhttps://vmf.geo.tuwien.ac.at/\n提供天顶对流层延迟（ZTD）数据和大气压负荷（APL）数据。ZTD数据包含5种数据来源和格式，后缀为_OP的为开放的数据集。\n\n\n站坐标文件(snx)\nftp://igs.ign.fr/pub/igs/products\nftp://cddis.gsfc.nasa.gov/pub/gps/products\n\n\n地球自转参数EOP\nhttps://www.iers.org/IERS/EN/DataProducts/EarthOrientationData/eop.html\n\n\n天线改正文件(atx文件)下载\nwww.igs.org/pub/station/general\nftp://ftp.igs.org/pub/station/general\nhttps://files.igs.org/pub/station/general/\nATX文件下载(进入pcv_archive文件夹中，即可下载GPS周，每周的ATX文件）\nhttp://www.beidou.gov.cn/yw/gfgg/201912/t20191209_19613.html?from=singlemessage \n\n\n海洋潮汐模型\nhttp://holt.oso.chalmers.se/loading/?tdsourcetag=s_pctim_aiomsg\n\n\n码偏差DCB\nGNSS差分码偏差（DCB，Differential Code Bias）是由不同类型的GNSS信号在卫星和接收机不同通道产生的时间延迟（硬件延迟/码偏差）差异，按照频率相同或者不同又可以细分为频内偏差（例如GPS P1-C1）和频间偏差（例如GPS P1-P2）。DCB文件主要提供机构：CODE（欧洲定轨中心）、DLR（德国宇航中心）、CAS（中科院测地所）。\n作为IGS Multi-GNSS实验（MGEX）的一部分，由两个分析小组确定了两种不同的DCB产品：中国科学院武汉大地测量与地球物理研究所（IGG）和德国宇航中心（Deutsche Forschungsanstalt für Luftund Raumfahrt [Deutsches Zentrum für Luft- und Raumfahrt]，DLR）在德国。\nhttp://ftp.aiub.unibe.ch/CODE/YYYY/\nhttp://ftp.aiub.unibe.ch/BSWUSER52/ORB/2024/\n\n\nCODE 分析中心使用的各种信息文件（配合BERNESE）\nftp://ftp.unibe.ch/BSWUSER52/\n包含C04，EOP，DCB产品，卫星 PCO，PCV 及各种接收机天线相位信息等。\n\n\n\n\n（2）MGEX/北斗观测数据文件（RINEX3.x格式）\n\n北斗卫星发射时间、原子钟相关信息\nhttp://mgex.igs.org/IGS_MGEX_Status_BDS.php#Satelliteshttps://en.wikipedia.org/wiki/List_of_BeiDou_satellites\n\n\n北斗星座状态\nhttp://www.csno-tarc.cn/system/constellation\n\n\n国际GNSS监测评估系统IGMAS\nhttp://www.igmas.org/Product/Search/search/\n\n\nIGS MGEX项目简介及说明\nhttp://mgex.igs.org/IGS_MGEX_Data.php\n\n\n武汉大学IGS数据中心（最近的数据找不到）\nhttp://www.igs.gnsswhu.cn/index.php/Home/DataProduct/mgex.html\n\n\n可以接收北斗三号新信号的MGEX观测站\nPOTS SGOC SUTM ULAB URUM WIND WUH2\n\n\nMGEX 观测数据\nBKG网址\n\nftp://igs.bkg.bund.de/IGS/obs/2020/\n\nING网址（不稳定）\n\n ftp://igs.ign.fr/pub/igs/data/campaign/mgex/daily/rinex3/2020/\n\nCDDIS网址（不是RINEX3.x格式） \n\nftp://ftp.cddis.eosdis.nasa.gov/pub/gnss/data/daily/2020/ftp://cddis.gsfc.nasa.gov/pub/gps/data/dailyftp://cddis.gsfc.nasa.gov/pub/gps/data/campaign/mgex/daily/rinex3/\n\n\n\nIGS/MGEX 1s观测数据\nftp://cddis.gsfc.nasa.gov/pub/gps/data/highrate\n\n\nMGEX广播星历\nhttps://igs.bkg.bund.de/root_ftp/IGS/BRDC/2020/ftp://cddis.gsfc.nasa.gov/pub/gps/data/campaign/mgex/daily/rinex3/2020/brdm\n\n\nMGEX 精密轨道/钟差（1）CDDIS 机构提供\n\nftp://cddis.gsfc.nasa.gov/pub/gps/products/mgex/\nCODFIN、GFZRAP、GRGFIN、JAXFIN、SHARAP、WUMFIN、WUMULA、wumsp3\n\n（2）IGN 机构提供\n\nftp://igs.ign.fr/pub/igs/products/mgex\nCODFIN、GFZRAP、GRGFIN、JAXFIN、SHARAP、WUMFIN、WUMULA、wumsp3\n\n（3）北三星历 GFZ GBM\n\nftp://ftp.gfz-potsdam.de/pub/GNSS/products/mgex\n\n\n\n\n3.文件类型说明在GNSS定位中，会使用到多种文件类型，每种文件都有特定的后缀和命名方法。以下是一些常见的GNSS文件类型及其后缀和命名规则：\n1. RINEX 文件\nRINEX 文件命名规则\n\nRINEX 文件完整的文件名由用于表示文件归属的 8 字符长度的主文件名和用于表示文件类型的 3 位字符长度的扩展名组成：\n$$_.$$\n\n文件名各部分释义：\n\n&lt;SITE&gt; 为四个字符的观测站点名；\n&lt;RN&gt; 为接收机的编号；\n&lt;CRC&gt; 为三位 ISO 3166-1 标准的国家和地区代码，标识站点位置；\n&lt;S&gt; 为数据源，即数据来源于接收机（R）还是数据流（S）；\n&lt;YEARDOYHRMN&gt; 为观测开始时刻：年、年积日、时、分；\n&lt;LEN&gt; 为观测时段的长度；\n&lt;FRQ&gt; 为观测时的采样间隔或采样频率（星历文件无此项）；\n&lt;ST&gt; 为包含的卫星系统和数据类型，第一位表示卫星系统（M、G、R、C、E、J、I）；第二位为数据类型，即观测文件（O）、导航文件（N）或气象文件（M）；\n&lt;FMT&gt; 为扩展名，扩展名只有两种：rnx 或 crx。\n\n文件名示例：\n\nALGO00CAN_R_20170420000_01D_30S_MO.rnx 表示数据是来自加拿大的 ALGO 站 0 号接收机，于 2017 年第 42 日 0 点开始观测的，时长 1 天，采样间隔为 30 秒的多系统混合观测数据；\nBJFS00CHN_S_20170420100_15M_01S_GO.rnx 表示数据是来自中国的 BJFS 站 0 号接收机的实时数据流，观测开始于 2017 年第 42 日 1 点，时长为 15 分钟，采样间隔 1 秒的 GPS 观测数据；\nALGO00CAN_R_20170420100_01H_05Z_MO.crx 表示数据是来自加拿大的 ALGO 站 0 号接收机，于 2017 年第 42 日 1 点开始观测的，时长 1 小时，采样间隔为 5 Hz 的多系统混合 Compact RINEX 观测数据；\nALGO00CAN_R_20170420000_01D_MN.rnx 表示数据是来自加拿大的 ALGO 站 0 号接收机，于 2017 年第 42 日 0 点开始观测，时长 1 天的多系统混合的导航数据；\nBRDC00IGS_R_20170420000_01D_MN.rnx 表示数据来自 IGS，由 IGS 合并生成的包含多系统所有可用卫星的混合导航数据；\nALGO00CAN_R_20170420000_01D_RN.rnx 表示数据是来自加拿大的 ALGO 站 0 号接收机，于 2017 年第 42 日 0 点开始观测，时长 1 天的 GLONASS 系统的导航数据；\nDAVS00ATA_R_20170420000_01D_30S_MM.rnx 表示数据来自南极洲 DAVS 站 0 号接收机，于 2017 年第 42 日 0 点开始观测，时长为 1 天的混合气象数据。\n\n\n参考：\n\nRTKLIB(一)——GNSS测量中的数据格式_gnss数据-CSDN博客\n[RINEX 3 格式简介 (gnss.help)](https://www.gnss.help/2017/04/22/rinex3-introduction/index.html#:~:text=RINEX 3 格式简介 1,观测数据文件； 2 导航（星历）文件； 3 气象数据文件。)\n\n\n\n\n\nRINEX 文件内容解读.24o(obs)：观测值文件\n\n\n\n**.24n(nav)**文件：导航电文\n\n\n\n\n\n2. 轨道和时钟产品等文件\n.sp3: 精密轨道和钟差文件，IGS和其他机构发布。\n\nIGS精密星历采用sp3格式，其存储方式为ASCII文本文件，内容包括表头信息以及文件体，文件体中每隔15 min给出1个卫星的位置，有时还给出卫星的速度。它的特点就是提供卫星精确的轨道位置。采样率为15分钟，实际解算中可以进行精密钟差的估计或内插，以提高其可使用的历元数。\n\n常用的sp3格式的命名规则为：\n\nttt：表示精密星历的类型，包括IGS(事后精密星历)、IGR(快速精密星历)、IGU(超快速精密星历文件)三种\n\nwwww：表示GPS周；\n\nd：表示星期，0表示星期日，1～6表示星期一至星期六。\n\n文件名如：igs12901．sp3，其中igs为计算单位名，1290为GPS周，1为星期一。\n\n参考：RTKLIB(一)——GNSS测量中的数据格式_gnss数据-CSDN博客\n\n\n\n\n.clk: 精密钟差文件，包含卫星钟差和钟漂的时钟校正数据。\n\n.erp: 地球自转参数文件，包含极移和世界时数据，这些参数可以影响地球表面点的准确定位和时间标准的同步。\n\n\n极移（Polar Motion）: 描述地球旋转轴相对于地球表面的移动。\n世界时（UT1-UTC）: 描述世界时与协调世界时（UTC）之间的差异。\n日长（Length of Day, LOD）: 描述地球自转速率的变化\n\n\n\n\n.ion: 电离层文件，提供电离层总电子含量数据。\n\n.atx：天线文件。\n\nGPS从入门到放弃（二十） — 天线偏移 | 航行学园 (voycn.com)\n\n\n.gcc：用于存储GNSS协调系统之间的转换参数，包含了坐标转换参数、误差模型以及其他与不同坐标系统之间转换相关的信息。\n\n\n\n\n.dcb：用于存储GNSS卫星和接收机的码偏差数据。码偏差是由不同频率的信号在接收机和卫星设备中经历不同的延迟引起的。主要用途包括：\n\n\n校正GNSS观测数据: 减少由于码偏差导致的误差，提升定位精度。\n电离层研究: 提供准确的码偏差信息，支持电离层总电子含量（TEC）的计算和研究。\n多频GNSS数据处理: 提供不同频率信号间的偏差校正，提高多频定位解算的准确性。\n\n\n以 GPS 双频 C1-P2 数据为例，进行无电离层组合时需加入 P1-C1 DCB 值将基准修正到 GPS 无电离层组合的钟差基准。通过引入一定的基准，可将差分码偏差还原为绝对码偏差。用户进行处理时，则只需要根据选择的观测值类型选择相应的绝对码偏差，如同卫星钟差和接收机钟差修正一样，不涉及到其他类型的码偏差参数，因而也不存在组合问题。\n取 GPS C1W-C2W，即 GPS P1-P2 为基准作为约束：又已知：则 C1W 与 C2W 的绝对码偏差 OSB 为：通过对其他差分码偏差值进行解构，最终可以推出所有绝对码偏差值，即为 OSB 中对应观测码的偏差。\n\n\n.bia：多系统码偏差和相位偏差校正文件。\n\n\n\n\n.obx：包含卫星的姿态四元数\n\n\n\n\n\n3. 气象和电离层文件\n.met: 气象数据文件，包含温度、气压、湿度等数据。\n.ionex: 电离层网格文件，提供电离层总电子含量的网格化数据。\n\n4. 辅助文件\n.log: 日志文件，记录接收机的操作和观测过程。\n.sum: 总结文件，提供观测数据的摘要信息。\n.snx: SINEX（Solution Independent Exchange Format）文件，包含站点坐标和速率等数据，用于GNSS解算结果的交换。\n\n4.IGS 分析中心全称\nCOD Centre for Orbit Determination in Europe, Bern, Switzerland\t欧洲定轨中心ESA European Space Agency, Darmstadt, Germany\t欧洲航天局GFZ Geoforschungszentrum, Potsdam, Germany\t德国地学中心GRG Groupe de Recherche en Geodesie Spatiale, Toulouse, France\t空间大地测量研究小组JPL Jet Propulsion Labs, Pasadena, California, U.S.A.\t喷气推进实验室MIT Massachusetts Institute of Technology, Cambridge, Mass., U.S.A.\t麻省理工学院NGS National Geodetic Survey, Silver Springs, Maryland, U.S.A.\t美国国家测绘局NRC Natural Resources Canada, Ottawa, Ontario, Canada\t加拿大自然资源部SIO Scripps Institute of Oceanography, San Diego, California, U.S.A.\t斯克利普斯海洋研究所WHU Wuhan University\t武汉大学\n\n5.RINEX 文件后缀解释\n\n\n\n\n\n6.Bernese 软件的匿名 FTP 服务器（AIUB）介绍参考：Bernese软件的匿名FTP服务器（AIUB）介绍 - 简书 (jianshu.com)\n网址：ftp://ftp.aiub.unibe.ch 或者 http://www.aiub.unibe.ch/download\n\nATM 目录包含了 Bernese 格式的对流层和电离层文件，采用年方式的子目录：\nyyyy/    CODyyddd.TRP.Z      最终的对流层延迟    COEyyddd.TRP.Z      EUREF解的对流层延迟    COEyyddd.INX.Z      EUREF解的IONEX格式电离层信息    COEyyddd.ION.Z      EUREF解的Bernese格式的电离层信息\n\nGEN 目录包含了通用文件，例如 DATUM.、RECEIVER.、SAT_yyyy.CRX ，重力场文件、nutation 模型和 subdaily pole 模型等。可替换的模型输入文件在子目录 GEN/ALTERNATIVE_MODELS。\n另外，卫星信息（SATELLIT.I14）、天线相位中心偏差（PCV_COD.I14）和ANTEX（I14.ATX）也在这个目录。\n\nORB 目录包含了日更新文件，包括 P1-P2 和 P1-C1 DCB 值，还有 Bernese 格式的 ERP 文件。年子目录包括如下文件：\nyyyy/  CODwwww7.ERP.Z      从GPS周0978开始的最终周COED ERP文件  CODwwww7.GCC.Z      Weekly CODE final GCC files as from week 1400  CODyyddd.ERP.Z      从GPS周1706开始的最终天CODE ERP文件。  CODyyddd.GCC.Z      Daily CODE final GCC files as from week 1706  CODyyddd.CLK.Z      High rate (30 sec) satellite clock corrections                      from the CODE final IGS solution  CODyyddd.CLK_05S.Z  High rate ( 5 sec) satellite clock corrections                      from the CODE final IGS solution  CORyyddd.DCB.Z      Daily P1-P2 DCB estimates of rapid where                      final information is not yet available  CODyyddd.DCB.Z      最终天P1-P2 DCB 估值文件  BRDyyddd.CLK.Z      Broadcast clock information\n\nSTA 目录包括如下站相关的文件：\n\nCODE.STA        Bernese 站信息文件，在CODE处理时包含的所有站FES2004.BLQ     FES2004海洋潮汐负荷改正，在CODE处理时用于所有站IGS.STA         转换的igs.SNX站信息                (http://www.igs.org/igscb/station/general/igs.snx)IGS_FULL.STA    和IGS.STA一样，但是保留了单独的天线改正号EUREF.STA       转换的euref.SNX 站信息                (ftp://ftp.epncb.oma.be/pub/station/general/euref.snx)EUREF_FULL.STA  As EUREF.STA but keeping individual antenna calibration                number\n\nCODE 目录中产品文件采用国际标准格，例如，精密轨道文件采用 sp3格式、钟信息采用 Clock RINEX 格式，解文件采用 SINEX 格式，地球自转文件采用 IERS 格式，电离层信息采用 IONEX 和导航 RINEX 格式，对流层信息采用 Troposphere SINEX 格式\n目录中包括如下 CODE 的快速、超快速和预报产品（predicted GNSS based products）:\nCODE.ACN          分析策略总结  COD.EPH_U         CODE ultra-rapid GNSS orbitsCOD.ERP_U         CODE ultra-rapid ERPs belonging to the                  ultra-rapid orbit productCOD.TRO_U         CODE ultra-rapid troposphere product in SINEX                  formatCOD.SNX_U.Z       SINEX file from the CODE ultra-rapid solution                   containing station coordinates, ERPs, and satellite                  antenna offsetsCOD.SUM_U         Summary of stations used for the latest                  ultra-rapid orbitCOD.ION_U         Last update of CODE rapid ionosphere product                  (1 day) complemented with ionosphere                  predictions (2 days)COD.EPH_5D        Last update of CODE 5-day orbit predictions, from                  rapid analysis, including all active GLONASS                  satellites  CODwwwwd.EPH_U    CODE ultra-rapid GNSS orbits from the 24UT solution                  available until the corresponding early rapid orbit                  is available (to ensure a complete coverage of orbits                  even if the early rapid solution is delayed after the                  first ultra-rapid solutions of the day)CODwwwwd.ERP_U    CODE ultra-rapid ERPs belonging to the ultra-rapid orbits     CODwwwwd.EPH_M    CODE final rapid GNSS orbits                  (middle day of a long-arc solution where the rapid                  observations were completed by a subsequent                  ultra-rapid dataset)CODwwwwd.EPH_R    CODE early rapid GNSS orbits                  (third day of a 72-hour solution)CODwwwwd.EPH_P    CODE 24-hour GNSS orbit predictionsCODwwwwd.EPH_P2   CODE 48-hour GNSS orbit predictionsCODwwwwd.EPH_5D   CODE 5-day GNSS orbit predictionsCODwwwwd.ERP_M    CODE final rapid ERPs belonging to the final rapid orbits CODwwwwd.ERP_R    CODE early rapid ERPs belonging to the early rapid orbits CODwwwwd.ERP_P    CODE predicted ERPs belonging to the predicted                  24-hour orbitsCODwwwwd.ERP_P2   CODE predicted ERPs belonging to the predicted                  48-hour orbitsCODwwwwd.ERP_5D   CODE predicted ERPs belonging to the predicted                  5-day orbitsCODwwwwd.CLK_M    CODE clock product related to the final rapid orbit,                   clock RINEX formatCODwwwwd.CLK_R    CODE early rapid clock product, clock RINEX formatCODwwwwd.TRO_R    CODE troposphere product from the early rapid                   solution, SINEX formatCODwwwwd.SNX_R.Z  CODE early rapid solution, SINEX formatCORGddd0.yyI.Z    CODE rapid ionosphere product, IONEX formatCOPGddd0.yyI.Z    CODE 1-day or 2-day ionosphere predictions,                  in IONEX formatCODwwwwd.ION_R    CODE rapid ionosphere product, Bernese formatCODwwwwd.ION_P    CODE 1-day ionosphere predictions, Bernese formatCODwwwwd.ION_P2   CODE 2-day ionosphere predictions, Bernese formatCODwwwwd.ION_P5   CODE 5-day ionosphere predictions, Bernese formatCGIMddd0.yyN_R    Improved Klobuchar-style coefficients, RINEX                  formatCGIMddd0.yyN_P    1-day predictions of improved Klobuchar-style                  coefficientsCGIMddd0.yyN_P2   2-day predictions of improved Klobuchar-style                  coefficientsCGIMddd0.yyN_P5   5-day predictions of improved Klobuchar-style                  coefficientsP1C1.DCB          CODE sliding 30-day P1-C1 DCB solution, Bernese                  format, containing only the GPS satellitesP1P2.DCB          CODE sliding 30-day P1-P2 DCB solution, Bernese                  format, containing all GPS and GLONASS satellitesP1P2_ALL.DCB      CODE sliding 30-day P1-P2 DCB solution, Bernese                  format, containing all GPS and GLONASS satellites                  and all stations usedP1P2_GPS.DCB      CODE sliding 30-day P1-P2 DCB solution, Bernese                  format, containing only the GPS satellitesP1C1_RINEX.DCB    CODE sliding 30-day P1-C1 DCB values directly                   extracted from RINEX observation files, Bernese                   format, containing the GPS and GLONASS satellites                  and all stations usedP2C2_RINEX.DCB    CODE sliding 30-day P2-C2 DCB values directly                  extracted from RINEX observation files, Bernese                   format, containing the GPS and GLONASS satellites                  and all stations usedCODE.DCB          Combination of P1P2.DCB and P1C1.DCBCODE_FULL.DCB     Combination of P1P2.DCB, P1C1.DCB (GPS satellites),                  P1C1_RINEX.DCB (GLONASS satellites), and P2C2_RINEX.DCB\n\n年子目录中包括 CODE 最终产品：\nCODwwwwd.EPH.Z  CODE final GNSS orbitsCODwwwwd.ERP.Z  CODE final ERPs belonging to the final orbitsCODwwwwd.CLK.Z  CODE final clock product(GPS+GLONASS since GPS                 week 1934, 29-Jan-2017), clock RINEX format,                with a sampling of 30 sec for the GNSS satellite                 and reference (station) clock corrections and                5 minutes for all other station clock                correctionsCODwwwwd.CLK_05S.Z                CODE final clock product (GPS+GLONASS since GPS                 week 1934, 29-Jan-2017), clock RINEX format,                with a sampling of  5 sec for the GNSS satellite                 and reference (station) clock corrections and                5 minutes for all other station clock                correctionsCODwwwwd.SNX.Z  CODE daily final solution, SINEX formatCODwwwwd.TRO.Z  CODE final troposphere product, SINEX formatCODGddd0.yyI.Z  CODE final ionosphere product, IONEX formatCODwwwwd.ION.Z  CODE final ionosphere product, Bernese formatCODwwww7.SNX.Z  CODE weekly final solution, SINEX formatCODwwww7.SUM.Z  CODE weekly summary fileCODwwww7.ERP.Z  collection of the 7 daily COD-ERP solutions                of the weekCOXwwwwd.EPH.Z  CODE final GLONASS orbits (for GPS weeks                0990 to 1066)COXwwww7.SUM.Z  CODE weekly summary files of GLONASS analysisCGIMddd0.yyN.Z  Navigation messages containing improved                Klobuchar-style ionosphere coefficientsP1C1yymm.DCB.Z  CODE monthly P1-C1 DCB solution, Bernese format,                containing only the GPS satellitesP1P2yymm.DCB.Z  CODE monthly P1-P2 DCB solution, Bernese format,                containing all GPS and GLONASS satellitesP1P2yymm_ALL.DCB.Z                CODE monthly P1-P2 DCB solution, Bernese format,                containing all GPS and GLONASS satellites and all                stations usedP1C1yymm_RINEX.DCB                CODE monthly P1-C1 DCB values directly extracted                from RINEX observation files, Bernese format,                 containing the GPS and GLONASS satellites and all                 stations usedP2C2yymm_RINEX.DCB                CODE monthly P2-C2 DCB values directly extracted                from RINEX observation files, Bernese format,                containing the GPS and GLONASS satellites and all                 stations used\n\n","categories":["GNSS","阅读笔记"],"tags":["GNSS","Rtklib"]},{"title":"GNSS高精度数据处理方法","url":"/2024/05/31/GNSS%E9%AB%98%E7%B2%BE%E5%BA%A6%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/","content":"一、GNSS与IGS产品概述1.IGS组成和产品介绍国际 GPS 服务是国际大地测量协会 IAG 为支持大地测量和地球动力学研究与 1993 年组建、1994 年开始运转的一个国际协助组织。近年来随着国际 GPS 服务范围的不断拓宽，尤其是俄罗斯的 GLONASS 的纳入以及中国的 BDS 和欧盟的 Galileo 系统的发展，2005 年 3 月14 日，正式更名为 IGS。目前在全球范围内已有超过 200 个组织和机构加入其中，共同提供和分享 GNSS 数据、产品和服务，以支持高精度的GNSS应用领域。\nIGS 目前共有 12 个分析中心，它们是：\n\n瑞士欧洲定轨中心（CODE）；\n欧洲空间局（ESA）\n美国国家大地测量局（NGS）；\n美国海军天文台（USNO）；\n加拿大的能源矿山与资源部（EMR）;\n德国地学中心（GFZ）；\n美国加州的喷气推进实验室（JPL）；\n美国加州的斯克里普斯海洋研究所（SIO）；\n麻省理工学院（MIT）；\n捷克大地天文台（COP-RIGTC）；\n法国国家太空研究中心（CNES）空间大地测量团队（CRG）；\n武汉大学（WU）。\n\n目前，IGS 提供的产品主要包括：\n\n高精度的卫星星历及其相关产品；\n地球自转参数，如极移、日长变化；\nIGS 跟踪站的坐标及其速率；\n卫星及其跟踪站的钟差信息、时间尺度产品；\n全球电离层、天顶对流层延迟信息。\n\n目前 IGS 提供超快速（包括预报 P 与实测 O 两部分）、快速以及最终 3 种 GPS 精密星历。其精度指标、滞后性、更新率以及采样间隔如下表所示：\n\n\n\n星历类型\n精度\n延迟时间\n更新率\n采样间隔\n\n\n\n超快（P）\n约 5 cm / 3 ns\n实时\n一天4 次\n15 min\n\n\n超快（O）\n约 3 cm / 0.15 ns\n3~9 h\n一天4 次\n15 min\n\n\n快速\n约 2.5 cm / 0.075 ns\n17~41 h\n每天 1 次\n15 min / 5 min\n\n\n最终\n约 2.5 cm / 0.075 ns\n12~18 h\n每周发布\n15 min / 5 min / 30 s\n\n\n2.产品精度评估方法（1）一致性检验通过比较不同分析中心提供产品之间的一致性来反映精密卫星轨道和钟差产品的质量是一种最为直观的评估方法。对于 GPS，由于 IGS 最终合成产品的精度和可靠性最高，因此将其作为参考轨道和钟差。\n\n将卫星轨道按 15 min 间隔进行求差，并转换至切向、法向和径向 3 个分量进行比较。3 维轨道的一致性可通过下式求得：式中： 表示轨道 3 维互差；、 和  依次表示轨道切向、径向和径向互差。\n\n将卫星钟差按 5 min 间隔进行求差（得到一次差），然后再选择其中某颗卫星为参考，将其他所有卫星的一次差减去参考卫星的一次差，得到二次差。求一次差的过程中消除了不同分析中心在估值时引入的不同基准影响，即系统性偏差。\n\n由于不同分析中心质量控制的严密程度不同，导致相互之间的差值有时会过大，即产生了粗差。因此需要先将粗差剔除掉，例如给定一个插值序列，若某一个差值大于序列中位数的 3 倍，则可以认为该值是粗差。在剔除粗差后，可以统计该互差序列的均方根差，将其作为评估产品一致性的指标。\n\n\n（2）重复性检验如果相邻的弧段轨道存在 48 h 的重叠，则可以利用弧段轨道之间的重叠弧段来分析轨道与钟差产品天与天之间的重叠精度。\n\n\n（3）激光测卫检验Galileo 系统、BDS 系统和 QZSS 均搭载了高精度激光测距系统。利用双向激光测距可实现导航卫星的轨道复核。通过比较卫星激光测距的结果和 MEGX 轨道计算的距离值，可以得到两者之间的互差，通常称为 SLR 残差，该值的大小反映了轨道径向的精度。\n（4）阿伦方差检验阿伦方差是度量频率稳定度的重要指标之一，可将其用于评估在轨 GNSS 卫星钟的性能。阿伦方差的计算公式如下：式中： 为  历元时刻的精密卫星钟差改正数； 为钟差改正数的样本大小；​ 为相邻钟差改正数的间隔。\n二、PPP误差处理方法GNSS导航定位中的误差源主要可以分为以下 3 类：\n\n与卫星有关的误差：主要包括卫星轨道误差、卫星钟差、地球自转与相对论效应、卫星天线相位中心偏差、相位缠绕误差以及卫星硬件延迟；\n与信号传播路径有关的误差：主要包括对流层延迟误差、电离层延迟误差和多路径效应；\n与接收机有关的误差：主要包括接收机钟差、接收机天线相位中心偏差、地球固体潮汐与海洋潮汐以及接收机硬件延迟。\n\n由于 PPP 通常采用非差观测值，不能通过差分的方法来消除某项误差，因此需要对每种误差都进行考虑，目前主要有以下 3 种误差处理策略：\n\n采用精密产品，例如卫星轨道误差和卫星钟差可以使用 IGS 发布的精密卫星星历和精密卫星钟差进行改正；\n模型改正，对于能够精确模型化的误差，如天线相位中心偏差，地球固体潮汐和海洋潮汐、相对论效应等都可以采用现有的模型根据已知参数进行精确改正；\n对于无法精确模型化的误差，可以通常增加一个未知参数进行估计或者使用线性组合观测值来消除，如天顶对流层延迟一般采用附加参数进行估计，而电离层延迟则可以通过双频无电离层组合消除一阶项，或者附近参数估计倾斜路径电离层延迟。\n\n1.与卫星有关的误差（1）卫星轨道误差和卫星钟差在 PPP 中，通常使用 IGS 分析中心提供的精密星历和精密钟差来消除或削弱卫星轨道误差和钟差误差。由于精密星历和钟差产品给出的是节点上的卫星位置和卫星钟差，实际定位时，需根据观测时刻内插出信号发射时刻的卫星位置和钟差。另一方面，为了保存和钟差产品基准的一致性，还需要利用差分码偏差（DCB）产品进行码偏差改正。\n精密星历是以离散的位置形式给出的，即仅提供等间隔时间点上的卫星坐标，其时间间隔通常为 15 min，而 GNSS 观测值的采样间隔一般为 30 s，10 s，1 s，因此需要通过对精密星历进行内插或者拟合等来获取所需历元时刻的卫星坐标。一般采用 9~11 阶的拉格朗日多项式进行插值，其插值精度可以达到毫米级，远高于精密卫星轨道本身的精度，满足 PPP 的精度要求。\n由于卫星钟差变化较快且更加复杂，精密钟差产品的采样间隔一般为 5 min，30 s，5 s，由于采样间隔较小，线性插值法即可满足要求。\n（2）地球自转改正地球自转引起的卫星与地面的几何距离偏差最大可达 30 m，在 PPP 中，该项偏差将会导致东西方向 20~30m，高程方向 10m 左右的定位偏差，因此在地固系中计算卫星位置或者卫星到接收机的距离时必须要考虑地球自转的影响。\nGNSS数据处理通常在协议地球坐标系中进行，也就是卫星和地面站的位置均用地心地固坐标系表示，因此当卫星在  时刻发射的信号经过  的时间在  时刻到达地球之后，协议地球坐标系将围绕地球自转轴旋绕一个角度 ，即此时的接收机坐标是在  时刻的协议地球坐标系中计算的，所以为了保持坐标系一致，需要对卫星在  时刻的坐标进行旋转：进一步推算可以得到由地球旋转引起的卫星至接收机的距离影响：\n（3）相对论效应改正相对论效应引起的测距误差可以达到 10 m。由于狭义相对论和广义相对论的共同作用，卫星钟和地面钟的频率差将包含周期性偏差和常数项偏差两部分。其中常数项偏差可以通过在卫星生产时对卫星钟的频率进行调整来改正，而周期性偏差则使用模型进行改正：\n当采用广播星历计算时，改正公式为：式中： 为 GNSS 卫星椭圆轨道的偏心率； 为卫星的偏近地点角； 为卫星椭圆轨道的长半轴。当采用精密星历计算时，改正公式为：式中： 为卫星的位置矢量； 为卫星的速度矢量。\n（4）天线相位缠绕改正卫星天线相位缠绕引起的误差在高程方向和平面上可以达到分米级，在精密单点定位中不可以忽略。在动态定位中接收机的天线同样存在相位缠绕问题，但是该项误差可以被接收机钟差参数吸收，所以一般不需要考虑。而卫星的天线相位缠绕改正和卫星的姿态有关：其中：式中： 为卫星指向接收机的单位矢量； 为星固坐标系下的单位矢量； 为 测 站地平坐标系下的单位矢量。\n（5）天线相位中心改正在精密单点定位中，卫星天线相位中心偏差改正包括两部分：首先将卫星质心改正至卫星天线相位中心，即在精密星历中的卫星坐标上进行 PCO 改正（卫星质心位置 = 卫星天线相位中心位置 - PCO）；然后将 PCV 改正至站-星几何距离上（站-星几何距离=观测距离-PCV+其他改正）。对于 PCO 改正，由于 IGS 提供的是基于星固系下的 PCO 值，因此需要首先将其转换至地固系。接收机天线相位中心的改正与卫星端的类似，但是 IGS 天线文件中给出的 PCO 值是在测站地平坐标系下的，需要首先将其转换到地心地固坐标系，再进行 PCO 改正。\n\n\n2.与接收机有关的误差（1）地球固体潮汐改正地球固体潮汐对测距精度的影响在厘米级或者分米级，一般可以通过球谐函数来表示固体潮汐在水平和高程方向上引起的测站位移。如在精密单点定位中可以下面的公式来改正：$$\\Delta r=\\sum_{j=2}^{3}\\frac{GM_{j}}{GM}\\frac{r^{4}}{R_{j}^{3}}\\left[\\left[3l_{2}\\left(\\hat{R}{j}\\cdot\\hat{r}\\right)\\right]\\hat{R}{j}+\\left[3\\left(\\frac{h_{2}}{2}-l_{2}\\right)\\left(\\hat{R}{j}\\cdot\\hat{r}\\right)^{2}-\\frac{h^{2}}{2}\\right]\\hat{r}\\right]+\\left[-0.025\\mathrm{sin}\\phi\\mathrm{cos}\\phi\\mathrm{sin}\\left(\\theta{\\varepsilon}+\\lambda\\right)\\right]•\\hat{r}\\left(3.8\\right)$$式中： 为万有引力常数； 和摄动天体的质量  ( 表示月球， 表示太阳)的乘积， 为万有引力常数和地球质量  之乘积； 为地球半径；$\\hat{\\boldsymbol{R}}j为摄动天体在地心坐标系中的位置矢量；\\hat{r}为地心坐标系中的测站位置矢量；h_2为第二数，一般取h_2=0.6090;l_2为第二勒夫数，一般取l_2=0.0852；\\phi，\\lambda分别为测站纬度和经度；\\theta{s}$ 为格林尼治恒星时。\n（2）海洋潮汐改正海洋潮汐对测距精度的影响比固体潮汐小一个数量级，在厘米或者毫米级别，而且测站的位置有关，如果测站远离海岸，则一般可以忽略不计。海洋潮汐的改正与时间、方向还有潮波有关，具体改正模型如下：式中： 和   为分潮波的频率和历元时刻的天文幅角； 为以秒计的世界时； 为阶数，目前仅考虑到 11 阶。\n3.与信号传播路径有关的误差（1）电离层延迟改正电离层延迟是影响 GNSS 导航定位精度的主要误差源，其大小从几米到几十米不等，是仅次于接收机钟差的对定位结果影响第二大的误差源，会受到太阳辐射强度的影响，而且卫星的高度角越低，电离层延迟越大，所以在导航定位中必须进行电离层延迟改正。\n单频用户通常采用电离层模型进行改正，通过建立测站上空的电离层电子含量的空间分布模型，根据信号传播路径来直接计算得到电离层延迟。电离层模型分为经验模型和实测模型，其中经验模型是根据电离层观测站长期积累的观测资料建立的经验公式，例如 Klobuchar模型，NeQuick 模型，GIM格网，Bent模型等，实测模型则是根据 GNSS 双频观测值反算得到测站上空的电子总含量（TEC）。\n由于电离层模型的改正精度有限，对于精密单点定位，双频用户可采用消电离层组合来消除电离层一阶项的影响，或者将电离层斜向延迟当成参数和测站三维坐标、接收机钟差等一起估计。除此之外，还可以使用先验的电离层信息作为虚拟观测值来增强 PPP 的性能。\n（2）对流层延迟改正与电离层不同，对流层是一种非色散性介质，即相同路径上的对流层延迟误差与卫星信号的频率无关。这就意味着在保持几何距离不变的情形下，无法通过双频或多频线性组合来消除对流层延迟误差。对于一个中纬度测站，对流层延迟在天顶方向可达 2.3m，在低高度角 (小于 10°) 时可达 30- 40m。因此，对流层延迟误差也是卫星导航定位中的重要误差源之一。在精密单点定位中， 通常采用模型改正、参数估计、先验信息约束等方法处理对流层延迟误差。\n1）对流层改正模型天顶对流层延迟 ZPD 由干延迟和湿延迟两部分组成，可表示为\n式中：、​ 分别表示天顶方向的对流层干延迟和湿延迟。其中干延迟是由空气中的干性气体所致，占总延迟量的 80% ~ 90%，主要受到温度和气压的影响，其变化相对稳定，一般采用模型改正的方法就可以基本消除对流层天顶方向的干分量。而对流层湿延迟是由空气中的水汽或冷凝水所致，尽管其占比较小，但受天气变化的影响显著，且变化规律较复杂，难以用模型描述。在精密单点定位中通常采用经验模型改正对流层延迟的干分量，而湿延迟则作为待估参数进行估计。\n常用的对流层延迟改正模型有 Saastamoinen  模型和 Hopfield 模型等。其干分量延迟的计算公式如下：\n Saastamoinen 模型\n式中： 为测站气压； 为测站纬度（rad）；H$ 为测站海拔高（m）。上式计算得到的是测站天顶方向的对流层延迟，将上式结果与投影函数相乘即可将其换算为信号传播路径上的对流层延迟。\nHopfiled 模型\n式中：​ 为测站绝对温度（K）；其余符号同上。类似地，采用投影函数可将其映射至倾斜路径，从而得到每一颗卫星的对流层延迟干分量。当卫星高度角在 10° 以上时， Saastamoinen 模型和 Hopfiled 模型Hopfiled 模型的差距不大，当卫星高度角达到 30° 以上时，两者的差异为 Icm 左右。两种对流层延迟改正模型仅在低高度角时差异较为明显。目前，Saastamoinen 模型是国际上较为公认且使用最广泛的对流层延迟改正模型。\n2）对流层投影函数信号传播路径上的对流层延迟  可表示为天顶方向的对流层延迟与投影函数的乘积，即\n式中： 和  分别表示对流层延迟干分量和湿分量所对应的投影函数，与卫星高度角密切相关。投影函数决定了倾斜路径上的对流层延迟总量。对流层延迟改正的投影函数模型主要包括 Chao 模型、Niell 投影函数（NMF）模型和全球投影函数（GMF）模型等。这些投影函数模型大致可以被分为两类：一类是利用以前的观测资料建立的经验模型，如 NMF 模型和 GMF 模型；另一类是需要实际气象资料的模型，如维也纳投影函数（VMF）模型。这三种模型均采用三项连分式的形式表示投影函数，其差别主要在于计算系数 、 、​ 时采用的方法不同。\n3）对流层参数估计若顾及对流层延迟的各向异性，即对流层延迟不仅与卫星高度角相关，还与卫星方位角有关，则完整的对流层延迟误差表达形式如下：\n式中： 为梯度映射函数，， 为卫星方位角； 和  分别为南北向、东西向梯度分量。由于静力学延迟  采用模型进行改正，因此，仅剩天顶对流层湿延迟 、水平梯度  和  需设为未知参数进行估计。\n在GNSS 精密数据处理中，根据观测时段的长度、气象状况等因素可对这些待估参数作如下处理：\n\n在整个时段中只引入一个天顶方向对流层延迟参数。这种方法的优点是引入的未知参数少，适用于时段长度较短、气候稳定的场合。\n将整个时段分为若干个子区间，每个区间各引人一个 ZPD 参数。该方法通常用于时段长、天气变化不太规则的场合，但引入的参数个数较多。\n采用线性函数模型来拟合整个时段的 ZPD，其待估参数为线性函数模型的两个系数。该方法适用于时段较长、天气变化均匀的场合。\n采用一阶高斯-马尔可夫过程来描述天顶方向对流层湿延迟的变化规律。当相关时间趋于无穷大时，一阶高斯-马尔可夫过程对应为随机游走模型；当相关时间趋于 0 时，则对应为白噪声模型。\n在采用 1- 4 方法估计 ZPD 的基础上，同时引入两个水平梯度参数。该方法活用于各向非均匀分布的对流层延迟估计，但引入的参数个数较多。\n\n4）先验对流层信息约束IGS 分析中心在解算全球跟踪站网坐标的同时，也给出了这些测站的 ZPD 作为其副产品之一。但要从这些全球稀疏分布的参考站网中提取得到用户站所在位置的准确对流层延迟信息还比较困难。近年来，为了加快 PPPP 的收敛速度，有学者提出利用 CORS 网建立区域的大气延迟误差模型（包括电离层延迟和对流层延迟），并将大气延迟改正信息发送给用户端，实现 PPP 的增强。与电离层先验信息约束处理方法类似，可将这些外部的对流层延迟先验信息作为虚拟观测值，并附加一定的约束（精度信息）。\n4.硬件延迟误差改正测码伪距和载波相位硬件延迟引起的偏差分别称为码偏差和相位偏差，相位偏差通常又被称为未校验的相位硬件延迟（UPD）。在浮点解 PPP 的结算过程中，UPD 会被浮点解的模糊度参数吸收，所以一般不需要考虑直接改正 UPD，而对于固定解 PPP，为了固定整周模糊度必须要解算出 UPD。除了采用频分多址的 GLONASS 卫星的信号，接收机端的码偏差对所有卫星的影响相同，因此接收机的端码偏差会被接收机钟差参数吸收。目前在硬件延迟误差方面主要改正的卫星端的码偏差，主要改正方法分为两种：利用导航电文中提供的群延迟参数改正，以及采用 IGS 等机构提供的高精度差分码偏差（DCB）产品进行改正。\n1）广播群延迟参数改正以 GPS 为例，假设 P1 码的硬件延迟时间为 ，P2 码的硬件延迟时间为，C/A 码的硬件延迟时间为，L2C 码的硬件延迟时间为。由于P1 码和 P2 码观测值的双频无电离层组合为：（）式中：。因此，双频 P 码无电离层组合的卫星端测码伪距硬件延迟为：由于卫星导航电文中提供的卫星钟差改正产品是以双频无电离层组合为基准得到的，因此改正产品提供的钟差改正数中除了卫星钟差改正数之外还有双频无电离层组合的卫星端测码伪距硬件延迟 ​，所以使用双频无电离层组合进行定位解算的用户不需要再考虑卫星端的测码伪距硬件延迟，而使用其他信号的用户则需要进行额外改正。\n\n对于单频用户，需要对 P1 码的伪距硬件延迟  与双频无电离层组合的卫星端测码伪距硬件延迟  的差值进行改正：同样的对于 P2 码用户有：对于 C/A 码用户有：对于 L2C 码用户有：式中的 ISC 是信号间改正，表示卫星信号间硬件延迟改正，同样由导航电文提供，用于信号间偏差改正。采用不用的下标分别表示 P1 码与 C/A 码的卫星硬件延迟之差（）以及 P1 码与 L2C 码的卫星硬件延迟之差（）。\n2）差分码偏差产品改正如果是事后处理模式，可以使用由 IGS 提供的精度更高的 DCB 产品来直接消除码偏差的影响。以 GPS 的 C1码、P1码 和 P2码 这 3 个观测值为例，存在两个独立的差分码偏差  和 ，其表达式为：\n双频 P码组合的差分码偏差可表示为：当采用 P1 码定位时，未改正的硬件延迟为：当采用 P2 码定位时，未改正的硬件延迟为：当采用 C/A 码定位时，未改正的硬件延迟为：\n3）其他硬件延迟误差频间钟差偏差（IFCB）在三频处理中，GNSS三频观测可以组合成两组无电离层组合的形式为：$$\\begin{aligned}&amp;P_{IF_{1,2}}=\\rho+(c\\mathrm{d}t_{\\mathrm{r}}+d_{\\mathrm{r}{1,2}})-(c\\mathrm{d}t^{\\mathrm{s}}+d{1,2}^{\\mathrm{s}})+T+\\epsilon_{P1}\\&amp;L_{IF_{1,2}}=\\rho+(c\\mathrm{d}t_{\\mathrm{r}}+d_{\\mathrm{r}{1,2}})-(c\\mathrm{d}t^{\\mathrm{s}}+d{1,2}^{\\mathrm{s}})+(b_{\\mathrm{r}{1,2}}-d{r_{1,2}}-b_{1,2}^{\\mathrm{s}}+d_{1,2}^{\\mathrm{s}})+\\lambda_{1,2}N_{1,2}+T+\\epsilon_{L1}\\&amp;P_{IF_{1,5}}=\\rho+(c\\mathrm{d}t_{\\mathrm{r}}+d_{\\mathrm{r}{1,5}})-(c\\mathrm{d}t^{\\mathrm{s}}+d{1,5}^{\\mathrm{s}})+T+\\epsilon_{P2}\\&amp;L_{IF_{1,5}}=\\rho+(c\\mathrm{d}t_{r}+d_{r_{1,5}})-(c\\mathrm{d}t^{s}+d_{1,5}^{\\mathrm{s}})+(b_{\\mathrm{r}{1,5}}-d{\\mathrm{r}{1,5}}-b^{\\mathrm{s}}{1,5}+d^{\\mathrm{s}}{1,5})+\\lambda{1,5}N_{1,5}+T+\\epsilon_{L2}\\end{aligned}$$其中1, 2, 5代表频率，,  分别表示无电离层组合下的伪距和载波相位观测值，，，分别表示卫星与接收机间的几何距离，接收机的伪距硬件延迟和卫星的伪距硬件延迟，，分别为接收机和卫星的相位硬件延迟，为无电离层组合载波相位的波长。\n当采用两组无电离层组合进行卫星钟差解算时，参数化的卫星和接收机钟差会吸收对应的硬件偏差，所以会产生两组卫星、接收机钟差，，，，。$$\\left{\\begin{array}{l}IFCB^\\mathrm{s}=\\delta^\\mathrm{s}{}{1,5}-\\delta^\\mathrm{s}{}{1,2}\\[2ex]IFCB_\\mathrm{r}=\\delta_\\mathrm{r}{1,5}-\\delta_\\mathrm{r}{1,2}\\end{array}\\right.$$\n系统偏差（ISB）系统间偏差是多个卫星导航系统进行联合导航、定位等应用时，必须要考虑的偏差 。因为不同的GNSS系统采用的坐标和时间基准不同，不同系统信号的结构、体制等有差异延偏差，当不同GNSS信号在多模接收机通道中传输时，会产生接收机设备相关的时延偏差 。所以利用多系统GNSS数据进行融合解算时，需要考虑这些系统性的偏差。 \n一般认为ISB是一个卫星系统观测值与参考卫星系统(如GPS)的观测值放一起处理时所需考虑的一个改正。在实际应用中，由于系统间坐标基准的差异主要体现在卫星位置上，在多模GNSS数据处理时一般转换成相同的坐标系，可以忽略坐标基准差对ISB的影响。\n所以，ISB主要包括了时间基准之差和接收机设备时延。\n当多模GNSS联合网解时，整网内只估算一个基准钟差。但不同的信号频率和不同的卫星系统会产生不同的设备时延，而这些时延会被接收机钟差吸收，因此不同卫星系统的接收机钟差实际上存在差异，所以需要在不同系统中引入一个偏差参数。当ISB参数与其他轨道产品联合解算时会产生秩亏，因此需要引入一个额外的约束条件。\n目前有两种约束方法：一是选择一个特定的测站，令其ISB为零；二是令整个测站网的ISB和为零。\n三、精密单点定位数据预处理1.伪距观测值预处理伪距粗差检测通常与周跳探测同时进行，一般采用 MW 组合观测值或者 GF 组合观测值的电离层残差法。当组合观测值的检验量超过设定的阈值之后，则判定卫星的观测值存在粗差或者发生了周跳。之后继续对后续历元进行检验，如果检验量连续超出阈值，则存在粗差，否则将其标记为周跳。\n另一方面，因为上述方法在检测过程中对于发生异常的卫星并不区分伪距异常和载波相位异常，而在实际应用中一般都是伪距观测值上存在粗差，因此当某颗卫星被标记为存在粗差时它的伪距观测值和载波相位观测值都会被舍去，导致正常的载波相位观测值被浪费。此时可以采用码观测值差分法，即利用码观测值之间的差分来消除几何距离和对流层延迟等，对码观测值的粗差进行检测。式中：、、、分别为卫星端或接收机端的测距码偏差，这些数值在短时间内较为稳定，因此一般可被视为常数； 和  为不同码偏差之间的时变量； 为电离层延迟残余误差项； 和  对应组合观测值的多路径效应、观测噪声等误差。\n采用码观测值差分的组合消去了站星间几何距离、对流层延迟、接收机钟差、卫星钟差等，与卫星和接收机的运动状态无关，主要表现为卫星端和接收机端的伪距硬件延迟和伪距观测值的组合噪声。与此同时，卫星端的伪距硬件延迟一般较小且稳定，而接收机端的通道延迟对于所有卫星基本相同，因此码观测值差分的组合观测值较为稳定，适用于检测伪距观测值中的较大的粗差：正常且异常或式中： 与  为阈值，顾及电离层延迟残余误差项，。 通常取为 10m，  取为 30m，其他的小粗差可以在后续的参数估计时采用抗差估计的方法进行消除。\n2.相位平滑伪距伪距观测值的码元宽度为 293m，噪声水平约为码元宽度的 1%，即 3m 左右，载波相位观测值波长为 19cm，噪声水平为波长的 1/4，即 5cm 左右。相位平滑伪距是将两种观测值结合起来，得到一种既没有整周模糊度问题，噪声水平又相当小的观测值。随着观测历元数的增加，经过相位平滑之后的伪距观测值中的噪声和多路径效应将逐渐减弱，精度一般可以达到分米级。以平滑双频无电离层伪距组合观测值为例，设 L1 和 L2 载波上的测码伪距与相位观测值依次为 ，，，（单位为米，单位为周），采用双频组合观测值消除电离层延迟低阶项的影响后，得到无电离层组合测码伪距  和载波相位 为：相应的测码伪距和载波相位观测方程为：式中： 为站星几何距离；、 分别为消电离层组合相位观测值的波长和模糊度参数；、 和 、​ 分别为消电离层组合伪距与相位观测值中所有与频率无关的偏差项之和与观测噪声。将上述两式相减得到：在对一颗卫星连续跟踪且没有发生周跳的情况下，载波相位模糊度将保持不变，即 保持不变，因此对于多个历元的连续观测：$$\\begin{cases} P_{C}\n\\left(t_{1}\\right)=\\lambda_{C}\\varphi\\left(t_{1}\\right)+\\lambda_{C}N_{C}\\P_{C}\\left(t_{2}\\right)=\\lambda_{C}\\varphi\\left(t_{2}\\right)+\\lambda_{C}N_{C}\\qquad\\qquad\\qquad\\vdots\\P_{C}\\left(t_{i}\\right)=\\lambda_{C}\\varphi\\left(t_{i}\\right)+\\lambda_{C}N_{C} \n\\end{cases}对上述各历元取平均值可以得到模糊度的估值为：\\langle\\lambda_{c}N_{c}\\rangle_{i} = \\frac{1}{i}\\sum_{k=1}^{i} ( P_{c}( t_{k} ) - \\lambda_{c}\\varphi_{c}( t_{k} ) )由此可以得到相位平滑后的测码伪距观测值为：\\overline{P}{\\mathrm{C}}\\left(t{i}\\right)=\\lambda_{\\mathrm{C}}\\varphi_{\\mathrm{C}}\\left(t_{i}\\right)+\\left\\langle\\lambda_{\\mathrm{C}}n_{\\mathrm{C}}\\right\\rangle_{i}采用滤波方法，双频相位平滑伪距的逐历元递推计算公式为：\\overline{P}{\\mathrm{C}}(t{i})=\\frac{1}{i}P_{\\mathrm{C}}(t_{i})+\\left(1-\\frac{1}{i}\\right)\\left[\\overline{P}{\\mathrm{C}}(t{i-1})+\\lambda_{\\mathrm{C}}(\\varphi_{\\mathrm{C}}(t_{i})-\\varphi_{\\mathrm{C}}(t_{i-1}))\\right]\\\\overline{P}{\\mathrm{c}}(:t{1}:)=P_{\\mathrm{c}}(:t_{1}:)由误差传播定律知，相位平滑伪距观测值的方差为：\\delta_{P_{\\mathrm{C}}}^{2}=\\delta_{\\varphi_{\\mathrm{C}}}^{2}+\\frac{1}{i}(\\delta_{P_{\\mathrm{C}}}^{2}-\\delta_{\\varphi_{\\mathrm{C}}}^{2})\\approx\\frac{1}{i}\\delta_{P_{\\mathrm{C}}}^{2}$$\n3.载波相位周跳探测与修复由于某种原因，接收机中断了对卫星信号的跟踪，在此期间积分量为 0，在恢复信号跟踪后，其所获得的整数与正确整数之间存在着一个偏差，这个偏差被称为周跳，即计数器中断所丢的整周数。周跳产生的原因主要有以下几类：\n\n\n**由于障碍物的短时间遮挡。**如卫星号被树木、电线杆、建筑物或山丘等障碍物遮挡，无法到达接收机天线。因此，应用GNSS定位时，应尽可能地回避影响卫星信号传播的环境\n**接收机的快速运动。**接收机在锁定卫星信号时，需要预测接收机与卫星之间的多普勒频移量，接收机的运动将使得该过程的难度增加，甚至导致信号失锁。因此，在高动态情况下，要特别小心周跳的问题。\n**接收机接收到的卫星信号信噪比比较低。**当卫星高度角较低时，信号将在大气层中传播更远的距离，信号损耗加大，从而使到达接收机的卫星信号信噪比下降。另外，电离层的活动、其他射频信号的干扰以及多路径效应，也将导致信号的信噪比下降。当到达接收机的卫星信号信噪比过低时，将使得接收机无法正常锁定信号，从而引起周跳。因此，对于低高度角卫星、以及在电离层活跃时期，特别容易产生周跳。\n**接收机硬件的故障或者软件的不完善。**因此，知名品牌的接收机在抗周跳方面具有优势。\n**卫星的原因。**如果卫星的振荡器不能正常工作，导致所产生的信号不正确，也容易出现周跳现象\n\n\n周跳探测的方法主要分为三类：\n\n\n基于观测值随时间变化规律的方法，如高次差法、多项式拟合法，载波相位观测值随时间的变化主要受到站星几何距离的影响，而站星几何距离的时变则取决于接收机与卫星的运动状态。由于卫星的运动规律较强，而接收机的运动规律则较难确定，因此此类方法通常用于静态数据处理。另外，此类方法还需要考虑卫星钟差、接收机钟差、对流层折射以及电离层折射随时间的变化。\n基于不同观测值组合的方法，如单频/双频码相组合法、电离层残差法、多普勒积分法。此类方法是利用不同观测值之间的关系来进行周跳探测，通常都是一些与接收机-卫星间几何距离无关的组合。此类方法不受卫星钟差、接收机钟差以及接收机运动状态的影响。\n基于观测值估值残差的方法，此类方法根据参数估计得到的估计残差来确定周跳。\n\n\n（1）多项式拟合法多项式拟合法一般用于单频周跳探测，其基本思想是利用一个包含 m 个无周跳的载波相位观测值的序列进行多项式拟合，多项式的具体形式如下：式中： 为观测历元时刻；，，，， 为拟合系数，可通过最小二乘法求得。同时根据拟合残差  计算中误差：用拟合出的多项式推求下一历元的载波相位观测值 ，并与实际的观测值  进行比较，若则认为该观测值不存在周跳，并将其加入到用于拟合的观测值序列中，同时去掉原序列的首历元的观测值，利用新的  个无周跳的载波相位观测值的序列重新进行多项式拟合，并重复上述周跳检验过程。若则认为该观测值存在周跳，此时可用  的整数部分替代  的整数部分，而  的小数部分则保持不变，形成新的观测值 ，即式中：int 为取实数整数部分的函数；frac 为取实数小数部分的函数。多项式拟合法与高次差法是等价的。在相邻历元的观测值之间求一次差，实际上就相当于求一次导数。显然，当对一颗卫星的载波相位观测值序列求次差后，若该序列观测值中不存在周跳，则所得到的是一个微小量序列，否则，则说明观测值中存在周跳。\n多项式拟合法探测周跳的基础是假设观测值随时间的变化可以用一个高阶多项式来表示，这一假设很容易被接收机自身的运动所打破，因此该方法并不适用于动态定位中周跳的探测，另外卫星钟差和接收机钟差的突变也会打破该假设，导致该方法失效。\n（2）单频码相组合法对于单频用户，可构造单频码相组合观测值（检验量）LP为：式中： 为相位观测值； 载波波长； 为模糊度； 为伪距观测值； 为电离层延迟误差。该检验量消除了站星几何距离、对流层延迟、卫星钟差、接收机钟差等与频率无关的系统性误差的影响，等式右边只剩下载波相位模糊度和电离层延迟项。\n当电离层活动不剧烈时，电离层延迟项通常不会随时间发生较大的变化，因此可以逐历元计算单频码相组合 ，并进行历元间差分，得到  检验量。如果该检验量小于阈值，则说明没有发生周跳。\n由于伪距观测噪声较大，单频码相组合法的周跳探测能力比较有限。特别是在卫星高度角较低的时候，单频码相组合法的组合观测值的噪声水平达到了 5~10 周；也就是 10 周以内的小周跳被伪距观测噪声所淹没，无法准确探测出来。随着卫星高度角的增大，其探测周跳的能力有所提升，但其噪声仍然接近5周。不同采样间隔对应的单频码相组合观测值噪声水平基本相当，这主要是因为在静态/动态测站上电离层延迟误差变化相对缓慢，历元间差分后其残余误差较小，而起主导作用的仍然是测码伪距噪声。\n单频码相组合法的特点可总结为：\n\n单频相码组合不受接收机和卫星的几何位置影响,因而适用于动态、非差数据的周跳探测，也不会受到卫星钟差和接收机钟差的影响。\n由于与载波相位相比，测码伪距的噪声水平较高，因此单频码相组合法仅适用于对较大周跳进行探测。\n对于高采样率数据，由于历元间电离层折射延迟变化较小，更有利于周跳探测。另一方面，由于低轨卫星运动速度较快，两个相邻历元间的电离层折射延迟变化较大，因此该方法不适用于低轨卫星跟踪数据。\n\n（3）双频码相组合法双频码相组合法是指利用双频载波相位和测码伪距组合观测值来探测周跳这一类方法的总称。Melbourne-Wübbena（MW）组合是一种常用的双频码相线性组合，它广泛应用于 GNSS 载波相位的周跳探测。给定双频载波相位 、 和伪距观测值 、，可构造 MW 组合观测值：式中： 和  为载波频率； 和  为载波波长； 为载波相位的宽巷（WL）组合 ； 为宽巷模糊度 。\nMW 组合观测值消除了电离层、对流层、钟差和星地几何距离的影响，等式右侧仅剩宽巷模糊度。因此，在未发生周跳时 MW 检验量将在一常数  附近波动，而当有周跳发生时，该检验量将产生突变。因此，逐历元计算各卫星的 MW 组合观测值，并通过历元间差分方法获得周跳探测检验量 。若  大于设定的阈值则判定其发生周跳，否则认为无周跳发生。阈值的设置主要取决于伪距观测值的噪声水平。\n双频 MW 组合法具有如下特点：\n\nMW 组合不受接收机和卫星的几何位置、电离层折射以及卫星和接收机钟差影响，因而适用于动态、非差观测值的周跳探测。\n虽然与载波相位相比，伪距的噪声水平要高得多，但此方法构造的是宽巷观测值，波长较长，约为 0.86m，因此可以探测出小周跳。\n此方法无法独立地区分出发生周跳的频率，且当两个频率上发生的周跳数值相等或接近时，改检验量将无法检测出周跳。\n\n（4）双频电离层残差法电离层残差法，通常又称为无几何（GF）距离组合法，是指利用双频载波相位观测值的电离层残差来探测与修复周跳。电离层残差法探测周跳的基本思想是考察不同历元间电离层残差的变化。利用同一历元的双频载波相位观测值，构造 GF 组合观测值：式中： 和  为双频载波频率；、 和 、 为对应的载波波长和模糊度；​ 为第一个频点上的电离层延迟误差。若不考虑载波相位观测值的噪声和多路径效应，GF 组合观测值仅与双频模糊度和电离层延迟有关。\n和 MW 组合法类似，逐历元计算各卫星的 GF 组合观测值，并通过历元间求差获得周跳探测检验量 ，一般而言，相邻两历元间电离层残差非差小，任何异常的变化都可以表明在一个或者两个频率的相位观测值中发生了周跳。对于 30s 以内的采样间隔，阈值通常可以被设置为 0.05~0.10m。\n电离层残差法具有如下特点：\n\n组合观测值不受接收机和卫星的几何位置影响，因而该方法适用于动态、非差数据的周跳探测。\n该方法也不受卫星和接收机钟差的影响。\n由于仅由载波相位观测值构造周跳检验量，因而其精度较高，可以探测小周跳。\n\n（5）三频周跳探测与修复多频载波相位观测值可以提供更多长波长、弱电离层和低噪声的组合观测值，有利于周跳的探测与修复，因此出现了基于三频或者多频观测值的周跳探测与修复方法。\n4.接收机钟跳探测与修复由于测量型与导航型的 GNSS 接收机内部采用的一般都是价格低廉，稳定度差的石英钟，随着测量的进行接收机钟差会逐渐产生漂移，导致接收机内部时钟与 GPS 时同步误差不断累积，因此当接收机钟差漂移到某一阈值时，需要对其插入钟差跳跃来将接收机时钟的同步精度控制在允许的范围内。钟跳按照量级可以分为毫米级钟跳和微秒级钟跳，对于毫秒级钟跳，接收机通过周期性地插入整数毫秒的钟差对时钟进行修正。微秒级钟跳相比于毫秒级钟跳时钟修正频率高，同时修正数值较小，一般优于微秒量级。\n接收机发生钟跳之后，GNSS测量中的三个基本观测量，时标、伪距和相位观测量之间的一致性将会被破坏。\n\n\n第一类钟跳是在接收机钟差的允许范围内（钟差保持连续），通过持续地调整接收机时标，即触发时标阶跃来实现的，其实质相当于改变了数据的采样频率。第二类钟跳总是在第一类钟跳累积到一定程度时（钟差将超过接收机厂商设定的阈值），通过定期插入之间累积的时标阶跃与伪距阶跃来实现。第三类钟跳仅通过调整伪距来控制接收机的钟差。\n1.接收机钟跳的性质钟跳和周跳都会破坏数据的连续性，但是两者存在本质区别。首先在表现形式上，周跳仅是相位观测值发生阶跃，而钟跳具有多种不同的表现形式，它既可以是伪距发生阶跃，也可以是相位观测值发生阶跃，甚至是伪距和相位同时发生阶跃；其次，从产生的物理机制上来分析，周跳一般是部分卫星或所有卫星发生失锁，且不同卫星、不同频率上发生的周跳数值具有随机性，而钟跳是从时间同步的角度来控制接收机钟差的范围，且由其引起的观测值阶跃（距离单位）对于所有卫星和所有频率是相同的；此外，周跳一般具有整数特性，而钟跳却未必具有整数特性，如频繁的微秒级钟跳就不具备整数约束。钟跳对周跳探测极为不利，它将导致现有的部分周跳探测方法失效。以 MW 组合法为例，周跳探测的检验量可表示为：式中： 为历元差分算子； 为宽巷相位值； 为窄巷伪距值；、 为 、 观测值的模糊度； 为组合观测值的噪声。当发生第一类或第四类钟跳时，由于伪距与相位观测值具有一致性（同时连续或同时阶跃），MW 组合不受钟跳影响。当发生第二类或第三类钟跳，设由其引起的伪距阶跃项为 ，则上式变为：钟跳时刻该检验量将产生突变，远远超过周跳检验阈值，导致所有卫星均被标定为周跳。因此如果数据预处理阶段不将钟跳与周跳进行区分，将会导致模糊度参数的重置，即 PPP 的重新初始化。\n对于 GF 组合和电离层残差法，由于钟跳引起的伪距阶跃同相位阶跃的数值相等，钟跳在这两种方法中并不会影响周跳的探测：然而，这种不敏感性恰恰又限制了其对某些特殊周跳，如数值相近的小周跳或 、 比值为 ​ 的周跳组合的探测能力。因此，实际数据处理过程中往往需要多种方法结合进行周跳探测。\n2.接收机钟跳的探测与修复钟跳探测的方法大致有两种模式：一种是基于参数域，根据事先估计出的接收钟差参数，通过分析其历元间的变化量来判断是否存在钟跳，通过分析其历元间的变化量来判断是否存在钟跳；另一种是基于观测值域，通过分析观测值的连续性来探测其是否存在周跳。\n\n\n与周跳修复不同，钟跳修复并非将阶跃的观测值修复成连续的形式，因为这有悖于钟跳的出发点，而是将连续的相位观测值调整为阶跃形式，以便同伪距基准保持一致。由探测量计算得到的钟跳值一般为浮点数，而实际的接收机钟跳具有整数毫秒特性，因此需要采取类似模糊度固定的方法获得真实的钟跳数值。由于钟跳值的小数部分远小于 1ms，一般直接取整就可以满足要求。\n四、精密单点定位数学模型1.精密单点定位基本观测方程GNSS 定位的观测量主要有载波相位和测码伪距两种。非差载波相位  和测码伪距  基本观测方程可以表示为：$$\\begin{aligned}\\&amp;L_{i}^{k}=\\parallel\\left(\\boldsymbol{r}^{k}\\left(t-\\tau_{i}^{k}\\right)+\\delta\\boldsymbol{r}^{k}\\left(t-\\tau_{i}^{k}\\right)\\right)-\\left(\\boldsymbol{r}{i}\\left(t\\right)+\\delta\\boldsymbol{r}{i}\\left(t\\right)\\right)\\parallel-\\boldsymbol{I}{i}^{k}+T{i}^{k}+\\&amp;\\qquad\\qquad\\left[:\\mathrm{d}t_{i}\\left(t\\right)-\\mathrm{d}T^{k}\\left(t-\\tau_{i}^{k}\\right)\\right]+\\lambda\\left[:\\delta_{i}\\left(t\\right)+\\delta^{k}\\left(t-\\tau_{i}^{k}\\right)\\right]+\\lambda\\left[:\\phi_{i}\\left(t_{0}\\right)-\\phi^{k}\\left(t_{0}\\right)\\right]+\\lambda N_{i}^{k}+\\delta m_{i}^{k}+\\varepsilon_{i}^{k}\\\\&amp;P_{i}^{k}=\\parallel\\left(\\boldsymbol{r}^{k}\\left(t-\\tau_{i}^{k}\\right)+\\mathrm{d}\\boldsymbol{r}^{k}\\left(t-\\tau_{i}^{k}\\right)\\right)-\\left(\\boldsymbol{r}{i}\\left(t\\right)+\\mathrm{d}\\boldsymbol{r}{i}\\left(t\\right)\\right)\\parallel+I_{i}^{k}+T_{i}^{k}+\\&amp;\\qquad\\qquad\\left[:\\mathrm{d}t_{i}\\left(t\\right)-\\mathrm{d}T^{k}\\left(t-\\tau_{i}^{k}\\right)\\right]+\\left[:d_{i}\\left(t\\right)+d^{k}\\left(t-\\tau_{i}^{k}\\right)\\right]+\\mathrm{d}m_{i}^{k}+e_{i}^{k}\\end{aligned}$$\n\n\n 为载波波长； 为 信号接收时刻；  为卫星  到接收机  之间的信号传播时间；\n 为卫星端信号从产生到离开卫星发射天线之间的码延迟； 为接收机端信号从接收机天线到信号相关处理器之间的码延迟；\n 为卫星在卫星天线发射信号时刻的位置； 为接收机在接收机天线接收信号时刻的位置；\n 为卫星发射天线的伪距观测值偏心矢量； 为接收机天线的伪距观测值偏心矢量；\n 为卫星  到接收机  之间的电离层延迟； 为卫星  到接收机  之间的对流层延迟；\n 为接收机  在信号接收时刻的接收机钟差； 为卫星  在信号发射时刻的钟差；\n 为卫星  到接收机  之间伪距观测值的多路径误差； 为卫星  到接收机  之间的伪距测量误差；\n 为接收机  在参考时刻  的初始相位  为 卫星  在参考时刻  的初始相位；\n 为卫星  与接收机  之间的整周模糊度；\n 为卫星端信号从产生到离开卫星发射天线之间的载波相位延迟； 为接收机端信号从接收机天线到相关信号处理器之间的载波相位延迟；\n 为卫星发射天线的载波相位观测值偏心矢量； 为接收机天线的截波相位观测值偏心矢量；\n 为卫星  到接收机  之间的载波相位观测值多路径误差； 为卫星  到接收机 ​ 之间的载波相位观测值测量误差。\n\n\n除了上述列出的误差之外，相对论效应、天线相位缠绕、天线相位中心变化以及潮汐负荷等误差项可以直接使用现有的模型进行改正。\n由于接收机端或卫星端的初始相位和载波相位的硬件延迟不易分离，可将其合并为  和 ，统称为未校验的相位硬件延迟（UPD），且一般可以认为 UPD 在一个连续的跟踪弧段内是固定不变的。整周模糊度 N 以及相位偏差的小数部分 b 对于每颗卫星和接收机的组合都是不同的，一般与  和  合并在一起作为浮点模糊度 B 进行估计。群延迟 d 的变化缓慢，在一个连续的弧段内也可以作为常数估计。$$L_{i}^{k}=\\rho_{i}^{k}+\\mathrm{d}t_{i}-\\mathrm{d}T^{k}-I_{i}^{k}+m_{i}^{k}\\cdot\\mathrm{ZPD}{i}+\\lambda B{i}^{k}+\\delta m_{i}^{k}+e_{i}^{k}\\P_{i}^{k}=\\rho_{i}^{k}+\\mathrm{d}t_{i}-\\mathrm{d}T^{k}+I_{i}^{k}+m_{i}^{k}\\cdot\\mathrm{ZPD}{i}+d{i}-d^{k}+\\mathrm{d}m_{i}^{k}+e_{i}^{k}其中：B^k_i=N^k_i+b_i-b^k\\b_{i}=\\phi_{i}\\left(t_{0}\\right)+\\delta_{i}\\left(t\\right)\\b^{4}=\\phi^{k}\\left(t_{0}\\right)+\\delta^{k}\\left(t-\\tau_{i}^{k}\\right)$$\n2.精密单点定位函数模型（1）单频 PPP 函数模型在单频的情况下，由于观测方程数不足以估计出所有参数，因此需要使用模型或者产品对电离层延迟等进行改正，在应用相应的 DCB 产品后，顾及参数之间的相互吸收关系，忽略硬件延迟的影响，载波相位和伪距观测方程可以简化为：$$L_{1}=\\rho+\\mathrm{d}t-\\mathrm{d}T-I_{1}+m\\cdot\\mathrm{ZPD}+\\lambda_{1}\\cdot B_{1}+\\varepsilon_{1}\n\\P_{1}=\\rho+\\mathrm{d}t-\\mathrm{d}T+I_{1}+m\\cdot\\mathrm{ZPD}+e_{1}$$在单频精密单点定位中，电离层延迟是影响定位精度的最主要误差源之一。目前解决单频精密单点定位中电离层误差的方法主要有半和模型、Klobuchar  模型、格网电离层产品，以及 CORS 地基增强的卫星历元差分电离层（SEID）模型。\n1）单频半和（UofC）模型由于伪距观测值和相位观测值所受到的电离层延迟误差大小相等、符号相反，可借鉴 UofC 模型构建单频消电离层组合，即：该组合观测值通常又称为半和模型，它不仅消除了电离层的低阶项，同时也将伪距观测值的噪声降至原始伪距观测噪声的一半。单频 PPP 模型除了使用半和改正的观测方程式外，还要联立伪距观测方程进行求解，即：因为联立了伪距观测方程进行求解，模型中仍然包含电离层误差，此时可以使用 IGS 提供格网电离层产品进行改正，采用这种方法可以消除 70%~90%的电离层误差，残余的电离层误差可以通过随机模型来表达。卫星轨道和钟差误差一般采用精密星历和精密钟差产品消除；同时，为了保持与精密钟差产品的一致性，还需要顾及硬件延迟引起的差分码偏差，一般可通过广播星历中实时播发的群延迟参数或 IGS 事后解算得到的差分码偏差产品进行改正。接收机钟差和对流层延迟通过参数估计解算得到。\n因此该模型的待估参数包括接收机的 3 维坐标、接收机钟差、对流层延迟和载波相位模糊度。\n假设接收机近似坐标为 ，卫星坐标为 $\\left(X^,Y^,Z^*\\right)，对上述观测方程进行线性化，得到误差方程：用矢量形式可表示为：，，，，，式中：V为残差矢量；A为设计矩阵；l为常数矢量（观测值减去近似值（））；X为待估参数；\\Delta X、\\Delta Y和\\Delta Z为接收机三维维坐标的改正数；Q$​ 为观测值方差-协方差矩阵。由于待估参数的初始值一般取为近似值，线性化引起的误差较大，为了得到准确的定位结果， 通常需要进行迭代计算，直至参数收敛。\n以静态单频 GPS 为例，不考虑对流层梯度参数，当连续观测  颗卫星时，观测方程数为 2，待估参数个数为 ，自由度为 。\n2）单频 SIED 模型单频 SIED 模型的基本思想是利用电离层组合观测值的历元间变化信息，分别对每颗卫星构建区域电离层模型，然后反推出区域内单频站的虚拟观测值，进而可以采用双频无电离层组合的方式进行处理。\n（2）双频 PPP 函数模型双频情形下，假设两个频率上的载波相位和伪距观测值分别为 、 和 、，其观测方程可简化为：式中：，表示电离层频率因子；根据对电离层延迟误差处理策略的差异，可以构造双频无电离层组合模型、UofC 模型，以及非差非组合模型。\n1）双频无电离层模型双频无电离层组合模型是 PPP 中使用的最为广泛的模型，假设两个频率（ 和 ，）上的伪距和载波相位观测值分别为 ，， 和 ，对应的双频无电离层组合模型为：$$\\begin{aligned}P_{r,1F(i,j)}^{s}&amp;=\\frac{f_{i}^{2}}{f_{i}^{2}-f_{j}^{2}}P_{i}-\\frac{f_{j}^{2}}{f_{i}^{2}-f_{j}^{2}}P_{j}=\\&amp;\\rho_{r}^{s}-d\\hat{T}^{s}+d\\hat{t}{r}+m{r}^{s}\\cdot ZPD_{r}+e_{r,IF(i,j)}^{s}\\L_{r,IF(i,j)}^{s}&amp; =\\frac{f_{i}^{2}}{f_{i}^{2}-f_{j}^{2}}L_{i}-\\frac{f_{j}^{2}}{f_{i}^{2}-f_{j}^{2}}L_{j} \\&amp;=p_{r}^{s}-d\\hat{T}^{s}+d\\hat{t}{r}+\\Delta t{r}+B_{r}^{s}+m_{r}^{s}\\cdot ZPD_{r}+\\varepsilon_{r,IF\\left(i,j\\right)}^{s}\\end{aligned}其中：\\begin{gathered}d\\hat{T}^{s}=dT^{s}+\\left(\\frac{f_{i}^{2}}{f_{i}^{2}-f_{j}^{2}}\\cdot d_{i}^{s}-\\frac{f_{j}^{2}}{f_{i}^{2}-f_{j}^{2}}\\cdot d_{j}^{s}\\right) \\d\\hat{t}{r}=dt{r}+\\left(\\frac{f_{i}^{2}}{f_{i}^{2}-f_{j}^{2}}\\cdot d_{r,i}-\\frac{f_{j}^{2}}{f_{i}^{2}-f_{j}^{2}}\\cdot d_{r,j}\\right) \\\\Delta t_{r}=\\frac{f_{i}^{2}}{f_{i}^{2}-f_{j}^{2}}\\left(\\lambda_{i}b_{r,i}-d_{r,i}\\right)-\\frac{f_{j}^{2}}{f_{i}^{2}-f_{j}^{2}}\\left(\\lambda_{j}b_{r,j}-d_{r,j}\\right) \\B_{r}^{s}=\\frac{f_{i}^{2}}{f_{i}^{2}-f_{j}^{2}}\\lambda_{i}\\left(N_{i}-b_{i}^{s}\\right)-\\frac{f_{j}^{2}}{f_{i}^{2}-f_{j}^{2}}\\lambda_{j}\\left(N_{j}-b_{j}^{s}\\right)\\end{gathered}$$上述式中：\n\n 表示包含双频无电离层组合的卫星端伪距硬件延迟的卫星钟差，目前 IGS 提供的精密卫星钟差产品是由 L1/L2（GPS）、C1/C2（GLONASS）、B1/B2（Beidou）、E1/E5（Galileo）双频无电离层组合观测值估计得到的，其提供的钟差改正数中包括了对应无电离层组合的卫星端伪距硬件延迟。因此当采用的频率分别为 1 和 2 时，即如果在定位中使用和精密钟差估计相同的频率，伪距组合观测值对应的卫星端伪距硬件延迟可以被完全消除。\n 表示吸收了双频无电离层组合的接收机端伪距硬件延迟的接收机钟差。\n 表示接收机端的码相偏差，即接收机端伪距硬件延迟和接收机端载波相位硬件延迟之差，可以被视为常数项偏差，该项偏差会被模糊度吸收。因为接收机端的伪距硬件延迟被吸收进了接收机钟差中，而在方程组中伪距观测方程和载波相位观测方程共有同一个钟差，因此需要在载波相位观测方程中做出对应的补偿。\n​ 表示吸收了卫星端载波相位延迟的双频无电离层组合的模糊度。\n\n上述模型通过精密星历和钟差产品消除了卫星轨道和钟差误差，双频线性组合消除了电离层延迟误差，其余误差包括卫星天线相位中心偏差、地球自转、固体潮汐、相对论效应、对流层延迟干分量等误差可采用误差模型进行改正。\n因此，无电离层组合模型的基本参数包含接收机的三维坐标、接收机钟差、天顶对流层湿延迟、无电离层组合模糊度，即：$$X=\\left(\\begin{matrix}r_{r}，d\\hat{t}{r}，ZPD{r}，\\hat{B}_{r}^{s}\\end{matrix}\\right)^{\\mathrm{T}}$$对于多频多系统组合，还需要顾及系统间偏差和频间偏差的影响，通常可以引入额外的参数来消除或者补偿这些偏差项。\n以静态双频 GPS 为例，不考虑对流层梯度参数，当连续观测  颗卫星时，观测方程数为 2，待估参数个数为 ，自由度为 。\n2）双频非差非组合模型基于原始观测值的非差非组合模型可表示为：该模型大部分误差的处理方法同无电离层组合模型一致，但其电离层延迟需附近参数进行估计。硬件延迟引起的码偏差可利用 IGS 分析中心提供的差分码偏差（DCB）产品进行改正。硬件延迟引起的相位偏差和初始相位会被模糊度参数吸收。该模型的待估参数包括：接收机的三维坐标、天顶对流层湿延迟、L1 载波站星方向的电离层延迟、双频模糊度参数，将观测方程组用矢量形式表示为：式中：只有当卫星和接收机端的硬件延迟均被改正后，利用上述模型得到的  才是真实的电离层延迟，否则 ​ 中还会包括硬件延迟，因此当使用非差非组合模型获取电离层延迟信息时，必须要对卫星端和接收机端的硬件延迟进行改正。\n在没有电离层延迟信息的情况下，即不实施任何先验约束的情况下，非差非组合 PPP 模型等价于传统的无电离层组合 PPP 模型。除此之外，还可以采用外部的电离层信息对电离层延迟参数进行适当约束从而加快 PPP 的收敛速度，提供 PPP 的定位精度和可靠性。\n（3）三频 PPP 函数模型三频情形下，假设 3 个频率上的载波相位和伪距观测值分别为 、、 和 、、，其观测方程可简化为：在组合形式上，三频精密单点定位又可以分为无电离层组合模型和非差非组合模型。对于无电离层组合，三频精密单点定位的组合形式又可分为两种：一种是三频之间两两组合产生三个双频无电离层组合观测值，另一种是三频之间构造唯一一个噪声最小的无电离层组合观测值。对于非差非组合模型，则是直接处理三频原始观测值。\n1）三频 IF-1 模型根据三个频率上的载波相位观测值和伪距观测值，可以构造 3 个双频无电离层相位/伪距组合观测值，即：式中：以北斗系统为例，将北斗卫星的信号频率代入上式得到：需要说明的是，虽然三频之间能够构造出 3 个双频无电离层组合观测值，但是其中仅有两个是独立的，因为组合系数矩阵并不满秩。另一方面，由于第三个组合的噪声较大，达到了  倍，通常仅选用前两个无电离层组合进行定位。\n当同时利用两个无电离层组合时，由于不同组合的观测值对应的接收机硬件延迟也不同，因此不能再将接收机硬件延迟全部吸收到接收机钟差中，此时需要在两个组合观测值上各估计一个接收机钟差参数，或者估计一个钟差和一个频间偏差（IFB），或者称为接收机端的码间偏差。这个偏差的本质是两个组合观测值使用到了三个频率，相当于两个观测方程的频率不同，因此在接收机端和卫星端的硬件延迟也不相同，而在双频无电离层组合中，两个频率组合在一起，相当于只使用了一个频率，因此不需要处理频率间的偏差。\n在卫星端同样需要处理  和 ​ 的频间偏差的影响，这个偏差有时也被称为频间钟差偏差（IFCB），可以直接采用 IGS 提供的 DCB 产品进行改正，但是这种方法由于是提供的是两个基本频率的差值，只能消除常数项偏差，无法消除周期性变化。另一种方法则是通过两个双频无电离层组合相减，估计每颗卫星的 IFCB，并建立综合考虑常数偏差和周期性变化的偏差模型。\n2）三频 IF-2 模型与 IF-1 模型不同，IF-2 模型仅构造一个噪声最小的三频线性组合，即：式中： 为组合系数。与 IF-1 模型类似，可根据无电离层影响，几何距离不变这两个条件求解组合系数。但是只有这两个约束条件并不足以确定 3 个未知数，因此还需要引人其他的约束条件，例如噪声最小、整数特性、长波长等特性。从定位精度的角度出发可以选择噪声最小原则。因此，组合系数可按下式唯一确定：无电离层影响几何距离不变噪声最小解算得到的系数表达式为：$$\\begin{cases}w_1=\\frac{\\kappa_{2}^{2}+\\kappa_{3}^{2}-\\kappa_{2}-\\kappa_{3}}{2\\left(\\kappa_{2}^{2}+\\kappa_{3}^{2}-\\kappa_{2}\\kappa_{3}-\\kappa_{2}-\\kappa_{3}+1\\right)}\n\\w_2=\\frac{\\kappa_{3}^{2}-\\kappa_{2}\\kappa_{3}-\\kappa_{2}+1}{2\\left(\\kappa_{2}^{2}+\\kappa_{3}^{2}-\\kappa_{2}\\kappa_{3}-\\kappa_{2}-\\kappa_{3}+1\\right)}\n\\w_3=\\frac{\\kappa_{2}^{2}-\\kappa_{2}\\kappa_{3}-\\kappa_{3}+1}{2\\left(\\kappa_{2}^{2}+\\kappa_{3}^{2}-\\kappa_{2}\\kappa_{3}-\\kappa_{2}-\\kappa_{3}+1\\right)}\\end{cases}将北斗系统的信号频率代入上式得到三频组合系数为：w_{1}\\approx2.566,w_{2}\\approx-1.229,w_{3}\\approx-0.337由于该模型只有一个伪距组合观测值，接收机硬件延迟码偏差可以被接收机钟差参数完全吸收，而载波相位硬件延迟可以被浮点模糊度吸收，因此并不需要考虑频间偏差的影响。观测方程线性化后得到：V_{L_{IF(1,2,3)}}=\\rho_{0}\\mid_{(X_{0},Y_{0},Z_{0})}+\\alpha\\cdot\\Delta X+\\beta\\cdot\\Delta Y+\\gamma\\cdot\\Delta Z+\\mathrm{d}t+m\\cdot\\mathrm{ZPD}+B_{[IF(1,2,3)]}-L_{[IF(1,2,3)]}\\V_{P_{IF(1,2,3)}}=\\rho_{0}\\mid_{(X_{0},Y_{0},Z_{0})}+\\alpha\\cdot\\Delta X+\\beta\\cdot\\Delta Y+\\gamma\\cdot\\Delta Z+\\mathrm{d}t+m\\cdot\\mathrm{ZPD}-P_{IF(1,2,3)}待估参数为：X=^T$$\n3）三频 UC 模型基于原始观测值的非差非组合模型可视为组合模型的特例，即组合系数为单位阵。顾及卫星星历，卫星钟差，以及其他非色散性误差改正后，三频非差非组合模型的观测方程可以表示为：$$L_{1}=\\hat{\\rho}+dt+m\\cdot ZPD-\\left(I_{1}+\\eta_{12}\\cdot DCB_{12}\\right)+\\lambda_{1}B_{1}+\\varepsilon_{1} \\L_{2}=\\hat{\\rho}+dt+m\\cdot ZPD-\\kappa_{2}\\left(I_{1}+\\eta_{12}\\cdot DCB_{12}\\right)+\\lambda_{2}B_{2}+\\varepsilon_{2} \\L_{3}=\\hat{\\rho}+\\mathrm{d}t+m\\cdot\\mathrm{ZPD}-\\kappa_{3}\\left(I_{1}+\\eta_{12}\\cdot\\mathrm{DCB}{12}\\right)+\\lambda{3}B_{3}+\\varepsilon_{3} \\\\P_{1}=\\hat{\\rho}+\\mathrm{d}t+m\\cdot\\mathrm{ZPD}+\\left(I_{1}+\\eta_{12}\\cdot\\mathrm{DCB}{12}\\right)+e{1} \\P_{2}=\\hat{\\rho}+\\mathrm{d}t+m\\cdot\\mathrm{ZPD}+\\kappa_{2}\\left(I_{1}+\\eta_{12}\\cdot\\mathrm{DCB}{12}\\right)+e{2} \\P_{3}=\\hat{\\rho}+dt+m\\cdot ZPD+\\kappa_{3}\\left(I_{1}+\\eta_{12}\\cdot DCB_{12}\\right)+\\left(\\frac{\\eta_{12}}{\\eta_{13}}DCB_{12}-DCB_{13}\\right)+e_{3}式中：\\eta_{12}=-\\frac{f_{2}^{2}}{f_{1}^{2}-f_{2}^{2}},\\eta_{13}=-\\frac{f_{3}^{2}}{f_{1}^{2}-f_{3}^{2}}\\DCB_{12}=d_{1}-d_{2},DCB_{13}=d_{1}-d_{3}$$​ 为经过非色散性误差改正后的几何距离。\n由于采用非组合形式，该模型中的电离层延迟需要设置参数估计。但是，电离层延迟和频间码偏差，即硬件延迟，都和频率相关，难以区分。卫星端的码偏差可以采用 IGS 提供的差分码偏差产品进行改正，而接收机端的码偏差一方面可以事先标定，一方面可以将其和其他参数合并。对于双频 PPP，电离层延迟参数（）和码偏差参数（）是完全相关的，在系数矩阵中表现为线性相关，两者在没有外界约束的情况下无法分离。而对于三频 PPP，第三个频率上的伪距观测值的码偏差影响在数值上不同于前两个伪距观测值，电离层延迟参数不能完全吸收码偏差的影响，因此需要在第三个频率的观测值上引入一个额外的频间偏差参数，由于相位观测值上的偏差会被模糊度参数吸收，这里仅在伪距观测值上增加上一个频间偏差参数。\n因此三频非差非组合模型需要估计的参数包括：\n（4）多频多模 PPP 函数模型对于 GPS/GLONASS/BDS/Galileo 系统四系统组合，其观测模型可以表示为：$$\\left{\\begin{array}{l}p_{\\mathrm{r}, \\mathrm{j}}^{\\mathrm{G}}=-\\boldsymbol{u}{\\mathrm{r}}^{\\mathrm{G}} \\cdot \\boldsymbol{r}{\\mathrm{r}}-\\mathrm{d} T^{\\mathrm{G}}+\\mathrm{d} t_{\\mathrm{r}}+\\kappa_{\\mathrm{jG}} \\cdot I_{\\mathrm{r}, 1}^{\\mathrm{G}}+m_{\\mathrm{r}}^{\\mathrm{G}} \\cdot Z_{\\mathrm{r}}+\\left(d_{\\mathrm{rG}, \\mathrm{j}}-d_{\\mathrm{j}}^{\\mathrm{G}}\\right)+e_{\\mathrm{r}, \\mathrm{j}}^{\\mathrm{G}} \\p_{\\mathrm{r}, \\mathrm{j}}^{\\mathrm{R}k}=-\\boldsymbol{u}{\\mathrm{r}}^{\\mathrm{R}} \\cdot \\boldsymbol{r}r-\\mathrm{d} T^{\\mathrm{R}}+\\mathrm{d} t{\\mathrm{r}}+\\kappa_{\\mathrm{jR}} \\cdot I_{\\mathrm{r}, 1}^{\\mathrm{R}}+m_{\\mathrm{r}}^{\\mathrm{R}} \\cdot Z_{\\mathrm{r}}+\\left(d_{\\mathrm{rR}, \\mathrm{j}}-d_{\\mathrm{j}}^{\\mathrm{R}}\\right)+e_{\\mathrm{r}, \\mathrm{j}}^{\\mathrm{R}} \\p_{\\mathrm{r}, \\mathrm{j}}^{\\mathrm{E}}=-\\boldsymbol{u}{\\mathrm{r}}^{\\mathrm{E}} \\cdot \\boldsymbol{r}{\\mathrm{r}}-\\mathrm{d} T^{\\mathrm{E}}+\\mathrm{d} t_{\\mathrm{r}}+\\kappa_{\\mathrm{jE}} \\cdot I_{\\mathrm{r}, 1}^{\\mathrm{E}}+m_{\\mathrm{r}}^{\\mathrm{E}} \\cdot Z_{\\mathrm{r}}+\\left(d_{\\mathrm{rE}, \\mathrm{j}}-d_{\\mathrm{j}}^{\\mathrm{E}}\\right)+e_{\\mathrm{r}, \\mathrm{j}}^{\\mathrm{E}} \\p_{r, j}^{\\mathrm{C}}=-\\boldsymbol{u}{\\mathrm{r}}^{\\mathrm{C}} \\cdot \\boldsymbol{r}r-\\mathrm{d} T^{\\mathrm{C}}+\\mathrm{d} t{\\mathrm{r}}+\\kappa{\\mathrm{jC}} \\cdot I_{\\mathrm{r}, 1}^{\\mathrm{C}}+m_{\\mathrm{r}}^{\\mathrm{C}} \\cdot Z_{\\mathrm{r}}+\\left(d_{\\mathrm{rC}, \\mathrm{j}}-d_{\\mathrm{j}}^{\\mathrm{C}}\\right)+e_{\\mathrm{r}, \\mathrm{j}}^{\\mathrm{C}}\\end{array}\\right.$$\n$$l_{r,j}^{G}=-u_{r}^{G}\\cdot r_{r}-\\mathrm{d}T^{G}+\\mathrm{d}t_{r}-\\kappa_{jG}\\cdot I_{r,1}^{G}+m_{r}^{G}\\cdot Z_{r}+\\lambda_{jG}N_{r,j}^{G}+\\lambda_{jG}(b_{rG,j}-b_{j}^{G}-b_{v,j}^{G})+\\varepsilon_{r,j}^{G}\n\\l_{r,j}^{R_{k}}=-u_{r}^{R}\\cdot r_{r}-\\mathrm{d}T^{R}+\\mathrm{d}t_{r}-\\kappa_{jE}\\cdot I_{r,1}^{R}+m_{r}^{R}\\cdot Z_{r}+\\lambda_{jE}N_{r,j}^{R}+\\lambda_{jR_{k}}(b_{rR_{k,j}}-b_{j}^{R}-b_{v,j}^{R})+\\varepsilon_{r,j}^{R}\n\\l_{r,j}^{E}=-u_{r}^{E}\\cdot r_{r}-\\mathrm{d}T^{E}+\\mathrm{d}t_{r}-\\kappa_{jE}\\cdot I_{r,1}^{E}+m_{r}^{E}\\cdot Z_{r}+\\lambda_{jE}N_{r,j}^{E}+\\lambda_{jE}(b_{rE,j}-b_{j}^{E}-b_{r,j}^{E})+\\varepsilon_{r,j}^{E}\n\\l_{r,j}^{C}=-u_{r}^{C}\\cdot r_{r}-\\mathrm{d}T^{C}+\\mathrm{d}t_{r}-\\kappa_{jC}\\cdot I_{r,1}^{C}+m_{r}^{C}\\cdot Z_{r}+\\lambda_{jC}N_{r,j}^{C}+\\lambda_{jC}(b_{rC,j}-b_{j}^{C}-b_{r,j}^{C})+\\varepsilon_{r,j}^{C}$$\n式中：上标 G、R、E、C 分别代表 GPS、GLONASS、Galileo 系统和 BDS 卫星； 表示 GLONASS 卫星的频率因子（频分多址）；、、 和  为接收机端各卫星系统的通道时延（码偏差）。\n不同系统之间的码偏差数值大小并不相等，其差值称为系统间偏差（ISB），此外，GLONASS 系统内部不同频率的卫星对应的接收机码偏差也不一致，其差值一般称为频间偏差，同理，在相位观测值中也存在类似的系统间和频率间相位延迟偏差。因此在多频多系统组合中需要额外引入 ISB 和 IFB 参数。\n3.PPP 随机模型利用 GNSS 观测值进行 PPP 解算之前，除了确定其函数模型之外，还需要确定平差系统的随机模型，随机模型涉及观测值本身的精度水平和参数的随即特性。GNSS 观测值的精度通常可以量化为与卫星高度角、信噪比相关的函数形式。参数随机模型主要用来描述各类参数的初始精度以及过程噪声。\n随机模型可以分为先验随机模型、验后随机模型和参数随机模型。先验随机模型又包括高度角函数法和信噪比函数法。\n4.浮点解 PPP 数据处理流程\n\n\n\n","categories":["阅读笔记"],"tags":["GNSS"]},{"title":"Windows实用指令","url":"/2022/03/11/Windows%E5%AE%9E%E7%94%A8%E6%8C%87%E4%BB%A4/","content":"CMD命令行指令：cmd命令大全／cmd命令提示符大全 - 简书 (jianshu.com)\n Win10 CMD命令大全与超好用的快捷键__Charge的博客-CSDN博客_cmd指令大全指令    \n1.可能相对常用的指令：d: \t\t\t\t#进入D盘eventvwr\t\t#事件查看器explorer\t\t#资源管理器control\t\t\t#控制面板mem.exe\t\t\t#显示内存使用情况mplayer2\t\t#简易widnows media playermsinfo32\t\t#系统信息.ncpa.cpl\t\t#网络连接.Nslookup\t\t#打开记事本Dccw\t\t\t#显示颜色较准desk.cpl\t\t#屏幕分辨率diskmgmt.msc\t#磁盘管理main.cpl\t\t#鼠标属性magnify\t\t\t#放大镜实用程序\n\n\n\n2.长见识的Windows快捷键：\n\n\n打开“任务管理器”\nCtrl+Shift+Esc\n\n\n\n显示桌面\nWin + D\n\n\n召唤Windows截图\nWin + Shift + S\n\n\n贴靠窗口\nWin +左/右&gt; Win +上/下&gt;\n\n\n\n\n\n\n打开文件资源管理器（计算机）\nWin + E\n\n\n打开设置\nWin + I\n\n\n打开运行窗口\nWin + R\n\n\n锁屏\nWin + L\n\n\n左下角开始图标上面点击右键\nWin + X\n\n\n\n\n\n\n打开打印视图\nCtrl + P\n\n\n撤销\nCtrl + Z\n\n\n查找\nCtrl + F\n\n\n\n\n\n\n打开“投影”\nWin +P\n\n\n全选中链接\nWin + F4键\n\n\n打开搜索“小娜”\nWin + S\n\n\n打开「连接」设备\nWin+K\n\n\n\n\n\n\n3.真正常用的CMD快捷键：\ncmd中输入 命令名 /? ，就可查看其对应的帮助文件。**1、ping 命令：**用来验证与远程计算机的连接。\nping 是Windows自带的一个DOS命令。利用它可以检查网络是否能够连通和分析网络速度，用好它可以很好地帮助我们分析判定网络故障。\n语法：ping  [选项]  [主机名称或IP地址]\n\nD:\\&gt;ping 127.0.0.1正在 Ping 127.0.0.1 具有 32 字节的数据:来自 127.0.0.1 的回复: 字节=32 时间&lt;1ms TTL=64来自 127.0.0.1 的回复: 字节=32 时间&lt;1ms TTL=64来自 127.0.0.1 的回复: 字节=32 时间&lt;1ms TTL=64来自 127.0.0.1 的回复: 字节=32 时间&lt;1ms TTL=64127.0.0.1 的 Ping 统计信息: 数据包: 已发送 = 4，已接收 = 4，丢失 = 0 (0% 丢失)，往返行程的估计时间(以毫秒为单位): 最短 = 0ms，最长 = 0ms，平均 = 0ms\n\n2、用命令查看和终止进程\n语法：netstat   [选项] \n\n常用参数：\n-a            显示所有连接和侦听端口。-n            以数字形式显示地址和端口号。-o            显示拥有的与每个连接关联的进程 ID。\n比如：\n查看监听端口以及监听对应的进程（PID）&gt;netstat -ano | findstr 端口号D:\\&gt;netstat -ano | findstr 8000TCP    0.0.0.0:8000           0.0.0.0:0              LISTENING       29296TCP    [::]:8000              [::]:0                 LISTENING       29296\n\n3.taskkill命令：按照进程 ID (PID) 或映像名称终止任务。语法：taskkill   [选项] \n\n常用参数：\n/F           \t\t\t\t\t\t\t\t\t 指定要强行终止 。/T            \t\t\t\t\t\t\t\t    Tree kill: 终止指定的进程和任何由此启动的子进程。/IM       image name  \t\t\t   指定要终止的进程的名称。/PID  process id       \t\t\t\t 指定要终止的进程的PID。\n比如：\n终止 idea进程及子进程&gt; \t\t taskkill /f /im  idea64.exe  /t\n根据pid强制终止进程&gt; \t\t\ttaskkill  /pid  进程的PID  /f \nD:\\&gt;taskkill /pid 29296 /f成功: 已终止 PID 为 29296 的进程。\n4.超实用的mklink命令\t\t可以用来清C盘(我猜的)Win10 mklink 命令怎么用，mklink 命令使用教程-逍遥峡谷 (icoa.cn)\n\n创建文件符号链接：什么参数都不带，就是默认创建文件符号链接。使用这个命令是需要管理员权限的，方法就是在【命令提示符】上点右键，选择【以管理员身份运行】。\n\nmklink d:\\Simple\\Link\\text.txt d:\\Simple\\Target\\text.txt\n\n\n带 /D 参数，创建目录符号链接。同样这个也是需要管理员权限的命令行，输入下面命令：\n\nmklink /d d:\\Simple\\Link\\Target d:\\Simple\\Target\n\n\n带 /J 参数，创建目录联接。之所以第三个介绍这个，是因为这个命令跟第2个命令非常相似，而且这个命令无需管理员权限。\n\nmklink /j d:\\simple\\link\\new d:\\simple\\target\n\n\n带 /H 参数，创建硬链接。经过测试硬链接依然不需要管理员权限：\n\nmklink /h d:\\Simple\\Link\\new.txt d:\\Simple\\Target\\text.txt\n\n","categories":["阅读笔记"],"tags":["基础操作"]},{"title":"RTKLIB函数解析","url":"/2024/10/09/RTKLIB%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/","content":"RTKLIB函数解析一、pntpos.c0、伪距单点定位流程：\n\n1、pntposa.函数功能说明：主函数\t使用伪距和多普勒观测值测量接收机的位置、速度、钟差\nextern int pntpos(const obsd_t *obs, int n, const nav_t *nav,                  const prcopt_t *opt, sol_t *sol, double *azel, ssat_t *ssat,                  char *msg)\n\nb.函数参数说明：const obsd_t *obs\t\t\tOBS观测文件，以卫星为单位int n\t\t\t\t\t\t观测文件的数量const nav_t *nav\t\t\t导航电文const prcopt_t *opt\t\t\t处理过程选项sol_t *sol\t\t\t\t\t输出double *azel\t\t\t\t方位角和俯仰角ssat_t *ssat\t\t\t\t卫星健康标志char *msg\t\t\t\t\t异常信息\n\nc.具体功能实现：（1）将 sol-&gt;stat 置零，即将存储输出结果的数据结构初始化；\n（2）检查 n 是否大于 0 ，即检测有没有 OBS 输入，如果没有，将异常信息传入 msg； \n（3）将 sol-time 设置为第一个 OBS 的时间；\n（4）创建用于存储结果的矩阵，包括 rs（卫星在ECEF下的位置和速度），dts（卫星钟差），var（卫星位置和钟差的协方差），azel（高度角和俯仰角），resp（伪距残差）；\n（5）当处理选项 opt中的模式不是SPP单点模式时，电离层校正采用 Klobuchar模型，对流层校正采用 Saastamoinen模型；\n（6）调用satposs函数按照观测到的卫星顺序计算出每颗卫星的位置、速度、钟差和钟漂；\n（7）调用estpos函数通过伪距计算出接收机的位置和钟差，以及实现定位后每颗卫星的方位角、俯仰角、定位有效性和伪距残差；\n（8）如果观测卫星数大于等于6，调用raim_fde函数对定位结果进行接收机自主正直性检测（RAIM）；\n（9）调用estvel函数依据多普勒观测值计算接收机的速度；\n（10）首先将 ssat_t结构体数组的 vs (定位时有效性)、azel（方位角、仰角）、resp (伪距残差)、resc (载波相位残差)和 snr (信号强度)都置为 0，然后再将实现定位后的 azel、snr 赋予 ssat_t结构体数组，而 vs、resp 只赋值给那些对定位有贡献的卫星，没有参与定位的卫星，这两个属性值为 0。\n2、satpossa.函数功能说明：按照所观测到的卫星顺序计算出每颗卫星的位置、速度、{钟差、频漂}\nextern void satposs(gtime_t teph, const obsd_t *obs, int n, const nav_t *nav,                    int ephopt, double *rs, double *dts, double *var, int *svh)\n\nb.函数参数说明：gtime_t teph\t\t\t\t用于选择星历的参考时刻const obsd_t *obs\t\t\t观测文件\t\t\tint n\t\t\t\t\t\t观测文件的数量const nav_t *nav\t\t\t导航电文int ephopt\t\t\t\t\t广播星历的选项double *rs\t\t\t\t\t卫星的位置和速度(ecef)\t\t\t\t\t\t\t\t\trs [(0:2)+i*6]= obs[i] sat position {x,y,z} (m)\t\t\t\t\t\t\t\trs [(3:5)+i*6]= obs[i] sat velocity {vx,vy,vz} (m/s)double *dts\t\t\t\t\t卫星钟差和钟漂\t\t\t\t\t\t\t\tdts[(0:1)+i*2]= obs[i] sat clock {bias,drift} (s|s/s)double *var\t\t\t\t\t卫星速度和钟差的协方差\t\t\t\t\t\t\t\tvar[i]        = obs[i] sat position and clock error variance (m^2)int *svh\t\t\t\t\t卫星健康标志\n\nc.具体功能实现：（1）按照观测顺序，依次对每颗卫星进行一轮计算，这里计算得到的卫星位置是天线相位中心；\n（2）首先将 rs 和 dts ，var 和 svh 初始化为0；\n（3）检测每颗卫星对应的OBS观测数据中每个频率上是否存在伪距观测值，如果没有，则跳过该次数据；\n（4）调用timeadd函数将 OBS 观测数据的时间减去伪距除以光速，得到卫星信号的发射时刻 t（可能包含接收机钟差，以及电离层，对流层等误差）；\n（5）调用ephclk函数，根据广播星历得到卫星钟差（t_0 = a_0+a_1*t+a_2*t^2），此时的钟差是没有考虑相对论效应和TGD的；\n（6）再次调用timeadd函数将 time 减去（5）中得到的卫星钟差，得到修正过的卫星信号发射时间 t_1，可能是GPS时间下的卫星信号发射时刻；\n（7）调用satpos函数，使用 t_1 ，teph，sat（卫星的数量）等参数计算得到 t_1 时刻卫星的位置、速度和钟差，并且此时的卫星钟差经过相对论效应的修正，但是没有考虑 TGD；\n（8）检测 dts 是否为零，即satpos函数计算出来的钟差是否为零，如果是，则直接将广播星历中的钟差赋给 dts，同时将dts中的钟漂设置为0，将var设置为SQR(STD_BRDCCLK)；\nc.问题\n  for (i=0;i&lt;n&amp;&amp;i&lt;2MAXOBS;i++) {         //这里为什么会是2？\n  为什么要检查dts是否为0，一种解释是：如果没有精密钟差，则ephclk()用广播星历的钟差替代，猜测可能是选了EPHOPT_PREC精密星历，但精密钟差计算出问题，用广播星历重新计算钟差\n  为什么要将钟漂设为0？\n  if (dts[i2]==0.0) {\t\t\t\t//这里的dts的长度为观测数2，因为其中包含钟差和钟漂\n\n3、estpos\n\na.函数功能说明：根据伪距观测值计算接收机的位置\nstatic int ephpos(gtime_t time, gtime_t teph, int sat, const nav_t *nav,                  int iode, double *rs, double *dts, double *var, int *svh)\n\nb.函数参数说明：gtime_t time\t\t\t\t卫星信号发射时刻gtime_t teph\t\t\t\t用于选择星历的参考时刻int sat\t\t\t\t\t\t卫星编号const nav_t *nav\t\t\t导航电文double *rs\t\t\t\t\t卫星的位置和速度(ecef)const double *dts\t\t\t卫星钟差double *var\t\t\t\t\t卫星位置和钟差的协方差const int *svh\t\t\t\t卫星健康标志\n\nc.具体功能实现：（1）声明并初始化参数，包括 x，dx ，Q ，v，H，var，sig等，依次表示三维位置和钟差，迭代得到的改正值，伪距残差，几何矩阵，伪距残差的方差，其中x，dx和Q的大小为NX=8，除了三维位置和钟差外还包括其他导航系统与GPS参考时间的差值，其中伪距残差和伪距残差的方差的大小为 n+4，n 为观测数据的数量，即观测到的卫星数，+4 是为了保证计算有效，防止出现秩亏，几何矩阵的大小为 (4+4)*(n+4)，每一列分别为卫星的三维位置、钟差，以及其他系统相对于 GPS 的时钟偏差；\n（2）将 sol-&gt;rr的前 3项，即上一次定位的结果，赋值给 x 数组，方便快速结算；\n（3）开始迭代计算，最大迭代次数为 10；\n（4）调用rescode函数计算当前迭代的伪距残差 v、几何矩阵 H，使用的参数包括 var（伪距残差的方差），azel（所有观测卫星的方位角和仰角），vsat（定位时有效性），resp（定位后伪距残差），ns（参与定位的卫星个数），nv（方程个数）；\n（5）迭代方程：\n（6）判断方程的数目是否大于设定的未知参数数目NX=8，如果小于，则返回异常信息；\n（7）以伪距残差的标准差的倒数作为权重，对 H 和 v 分别左乘权重对角阵，得到加权之后的 H 和 v；\n（8）调用 lsq 函数,得到当前未知参数 x 的修改量 dx 和定位误差协方差矩阵中的权系数阵 Q；\n（9）将 lsq 中求得的 dx 加入到当前 x 值中，得到更新之后的 x 值，方程为：$dx=({(H*H^T)}^{-1})Hv$，H已经事先加权处理；\n（10）判断 dx 是否小于设定的阈值，如果小于，则将迭代的结果输入到 sol 中，否则，进行下一次循环；\n（11）调用 valsol函数确认当前解是否符合要求（伪距残余小于某个值和 GDOP小于某个门限值），最后返回stat。\n（12）如果i大于最大迭代次数，函数返回0。\nc.问题\nfor (k=0;k&lt;NX;k++) H[k+j*NX]/=sig;\t\t为什么H和v要加权处理？\n\n4、raim_fdea.函数功能说明：对计算得到的定位结果进行接收机自主正直性检测，找出定位结果均方误差最小的组合\nstatic int raim_fde(const obsd_t *obs, int n, const double *rs,                    const double *dts, const double *vare, const int *svh,                    const nav_t *nav, const prcopt_t *opt, sol_t *sol,                    double *azel, int *vsat, double *resp, char *msg)\n\nb.函数参数说明：const obsd_t *obs\t\t\t观测文件\t\t\tint n\t\t\t\t\t\t观测文件的数量const double *rs\t\t\t卫星的位置和速度(ecef)const double *dts\t\t\t卫星钟差const double *vare\t\t\t卫星位置和时钟的协方差const int *svh\t\t\t\t卫星健康标志const nav_t *nav\t\t\t导航电文const prcopt_t *opt\t\t\t处理选项sol_t *sol\t\t\t\t\t输出double *azel\t\t\t\t方位角和俯仰角int *vsat\t\t\t\t\t表征卫星在定位时是否有效double *resp\t\t\t\t定位后的位居残差 (P-(r+c*dtr-c*dts+I+T))char *msg\t\t\t\t\t异常信息\n\nc.具体功能实现：（1）首先为 obs_e 分配内存，如果失败，则返回 0，然后初始化各个矩阵和向量；\n（2）在 for 循环中依次舍弃一颗卫星的数据，并将剩下的卫星的数据复制到用于评测的数据中；\n（3）调用estpos函数，使用去掉一颗卫星后的数据计算接收机的位置，并累加使用剩余卫星定位后的伪距残差平方和；\n（4）如果 nvsat&lt;5，则说明当前卫星数目过少，无法进行 RAIM_FDE操作，发送异常信息；\n（5）计算伪距残差平方和的标准平均值，如果小于rms（初始值为100），则说明当前定位结果更合理，将 stat置为 1，重新更新 sol、azel、vsat(当前被舍弃的卫星，此值置为0)、resp等值，并将当前的 rms_e更新到 rms中。\n（6）如果 stat 不为 0，则说明在弃用卫星的前提下有更好的解出现，输出信息，指出弃用了哪颗卫星。\n（7）使用 free函数回收相应内存。\n5、estvela.函数功能说明：依靠多普勒频移测量值计算接收机的速度\nstatic void estvel(const obsd_t *obs, int n, const double *rs, const double *dts,                   const nav_t *nav, const prcopt_t *opt, sol_t *sol,                   const double *azel, const int *vsat)\n\nb.函数参数说明：const obsd_t *obs\t\t\t观测文件\t\t\tint n\t\t\t\t\t\t观测文件的数量const double *rs\t\t\t卫星的位置和速度(ecef)const double *dts\t\t\t卫星钟差const nav_t *nav\t\t\t导航电文const prcopt_t *opt\t\t\t处理选项sol_t *sol\t\t\t\t\t输出double *azel\t\t\t\t方位角和俯仰角int *vsat\t\t\t\t\t表征卫星在定位时是否有效\n\nc.具体功能实现：（1）创建数组 x[4]，dx[4]，Q[16] ，v 和 H，并将 opt-&gt;err 赋给 err（Doppler error (Hz)）；\n（2）开始迭代，首先使用resdop函数计算定速方程组左边的几何矩阵和右端的速度残余，并返回定速时所使用的卫星数目；\n（3）然后调用lsq函数解得速度和频漂的步长，并累加到 x中；\n（4）检查当前计算出的步长的绝对值是否小于 1E-6。是，则说明当前解已经很接近真实值了，将接收机三个方向上的速度存入到 sol_t.rr中；否，则进行下一次循环。\nsol-&gt;qv[0]=(float)Q[0];  /* xx */sol-&gt;qv[1]=(float)Q[5];  /* yy */sol-&gt;qv[2]=(float)Q[10]; /* zz */sol-&gt;qv[3]=(float)Q[1];  /* xy */sol-&gt;qv[4]=(float)Q[6];  /* yz */sol-&gt;qv[5]=(float)Q[2];  /* zx */\n\n\n\n6、ephclka.函数功能说明：根据广播星历得到卫星钟差的修正，此时的钟差没有考虑相对论效应和 TGD\nstatic int ephclk(gtime_t time, gtime_t teph, int sat, const nav_t *nav,                  double *dts)\n\nb.函数参数说明：gtime_t time\t\t\t\t卫星信号发射的时间gtime_t teph\t\t\t\t用于选择星历的时刻int sat\t\t\t\t\t\t卫星编号const nav_t *nav\t\t\t导航电文const double *dts\t\t\t卫星钟差\n\nc.具体功能实现：（1）调用satsys函数根据卫星编号确定该卫星所属的导航系统和该卫星在该系统中的 PRN编号；\n（2）根据所属的系统来选择对应的星历数据提取函数，如果是GPS或者Beidou，调用seleph函数来选择 toe值与星历选择时间标准 teph最近的那个星历；\n（3）调用eph2clk函数，通过广播星历和信号发射时间计算出卫星钟差。\n7、satposa.函数功能说明：计算卫星信号发射时刻卫星的 P(ecef,m)、V(ecef,m/s)、C((s|s/s))\nextern int satpos(gtime_t time, gtime_t teph, int sat, int ephopt,                  const nav_t *nav, double *rs, double *dts, double *var,                  int *svh)\n\nb.函数参数说明：gtime_t time\t\t\t\t卫星信号发射的时刻gtime_t teph\t\t\t\t用于选择星历的时刻int sat\t\t\t\t\t\t卫星编号int ephopt\t\t\t\t\t星历选项const nav_t *nav\t\t\t导航电文double *rs\t\t\t\t\t卫星的位置和速度(ecef)const double *dts\t\t\t卫星钟差和钟漂double *var\t\t\t\t\t卫星位置和钟差的协方差const int *svh\t\t\t\t卫星健康标志\n\nc.具体功能实现：（1）根据星历选项的值选择对应的卫星位置计算函数，如果是 EPHOPT_BRDC，调用ephpos函数，根据广播星历计算出算卫星信号发射时刻卫星的 P、V、C；如果是EPHOPT_PREC，调用peph2pos函数根据精密星历计算位置；\n8、satsysa.函数功能说明：根据卫星编号确定该卫星所属的导航系统和该卫星在该系统中的 PRN编号\nextern int satsys(int sat, int *prn)\n\nb.函数参数说明：int sat\t\t\t\t\t\t卫星编号int *prn\t\t\t\t\t卫星PRN编号\n\nc.具体功能实现：（1）首先将函数返回值sys赋值SYS_NONE，保证函数正常运行；\n（2）之后根据rtklib的命名规则，得到卫星所属的系统和PRN编号；\n（3）命名规则是按照GPS、GLO、GAL、QZS、CMP、TIRN、LEO、SBS的顺序，如果卫星属于GPS，那么它的编号为GPS的卫星数目加上它在GLO系统中的编号。\nc.问题\nNSATGP和NSATGLO分别为32和27，而NSATGAL和NSATCMP却为0\n\n9、selepha.函数功能说明：从导航电文中选择星历，返回的是星历数据\nstatic eph_t *seleph(gtime_t time, int sat, int iode, const nav_t *nav)\n\nb.函数参数说明：gtime_t time\t\t\t\t选择星历的时间int sat\t\t\t\t\t\t卫星编号int iode\t\t\t\t\t星历数据期号const nav_t *nav\t\t\t导航电文\n\nc.具体功能实现：（1）根据该卫星所属的导航系统给与星历参考时间的最大时间间隔 tmax赋予相应的值；\n（2）根据nav数据中的导航电文数目遍历导航数据，首先确保所查找星历的卫星号是否相同，接着确保星历期号是否相同，如果出现不同，跳过该次数据，；\n（3）如果星历星历选择时间大于待查找星历的参考时间，则跳过该次数据，接着确保星历选择时间与待查找星历的参考时间的间隔是否小于 tmax；\n（4）对于通过了 2-3验证的星历，如果此时对于输入的期号，有 iode&gt;=0，则该星历就是所要寻找的星历；否则，验证 3中的 t是否满足 t&lt;=tmin。满足的话，就令 tmin=t，该星历目前是距离参考时间最近的；\n（5）循环 2-4步操作，遍历完导航数据中的所有星历。如果都不符合条件，输出异常信息并返回 NULL；否则，返回所查找到的星历。\nc.问题\ntmax=MAXDTOE+1.0 \t\t//这里为什么要+1？，还有为什么tmin=tmax+1.0？\n为什么eph_sel的返回值全是0？\n\nd.注意（1）对于 GPS 系统，星历数据期号每 2h 更新一次，所以 RTKLIB 中对 MAXDTOE的定义为 7200，同理 GALILEO 系统的星历数据期号每 4h 更新一次，所以 RTKLIB 中对 MAXDTOE的定义为 14400。默认星历数据期号为 7200。\n10、eph2clka.函数功能说明：根据信号发射时间和广播星历，计算卫星钟差，返回的是卫星钟差\nextern double eph2clk(gtime_t time, const eph_t *eph)\n\nb.函数参数说明：gtime_t time\t\t\t\t用于选择星历的时刻const eph_t *eph\t\t\t广播星历\n\nc.具体功能实现：（1）计算与星历参考时间的偏差 dt = t-toc；\n（2）连续使用两次t=ts-(eph-&gt;f0+eph-&gt;f1*t+eph-&gt;f2*t*t)；\n（3）利用二项式校正计算出卫星钟差，从 dt 中减去这部分，然后再进行一次上述操作，得到最终的 dt；\nc.问题\nt=ts=timediff(time,eph-&gt;toc);       //这一步是什么意思\ntoc指的是卫星钟的参考时刻。toe是完全用于轨道拟合的时间参数，而toc是完全用于卫星钟差的时间参数，两者是完全独立的时间参考系。但是为了便于计算，将卫星钟差参考时刻toc和轨道外推时刻toe设定为同步。便于编辑广播星历。\n返回的卫星钟差没有考虑相对论效应和TGD，使用了三次二阶项公式修正\n\n11、ephposa.函数功能说明：根据广播星历计算出算信号发射时刻某颗卫星的 P、V、C\nstatic int ephpos(gtime_t time, gtime_t teph, int sat, const nav_t *nav,                  int iode, double *rs, double *dts, double *var, int *svh)\n\nb.函数参数说明：gtime_t time\t\t\t\t卫星信号发射时刻gtime_t teph\t\t\t\t用于选择星历的参考时刻int sat\t\t\t\t\t\t卫星编号const nav_t *nav\t\t\t导航电文int iode\t\t\t\t\t星历数据期号double *rs\t\t\t\t\t卫星位置和速度double *dts\t\t\t\t\t卫星钟差和钟漂double *var\t\t\t\t\t卫星位置和时钟的协方差int *svh\t\t\t\t\t卫星健康标志\n\nc.具体功能实现：（1）调用satsys函数，确定该卫星所属的导航系统；\n（2）如果卫星GPS，GAL，QZS，CMP，IRN，则调用seleph函数选择对应的广播星历，然后调用eph2pos函数计算信号发射时刻卫星的 位置、钟差和相应结果的误差；\n（3）在信号发射时刻的基础上给定一个微小的时间间隔，再次计算新时刻的 P、V、C，之后通过两次时刻测得的卫星位置，除以给定的时间间隔来得到卫星的速度和频漂，即通过扰动法计算出卫星的速度和频漂；\nc.问题12、eph2posa.函数功能说明：根据广播星历计算出算信号发射时刻某颗卫星的位置和钟差，适用于GPS等系统\nextern void eph2pos(gtime_t time, const eph_t *eph, double *rs, double *dts,                    double *var)\n\nb.函数参数说明：gtime_t time\t\t\t\t卫星信号发射时刻const eph_t *eph\t\t\t广播星历int sat\t\t\t\t\t\t卫星编号double *rs\t\t\t\t\t卫星位置和速度double *dts\t\t\t\t\t卫星钟差和钟漂double *var\t\t\t\t\t卫星位置和时钟的协方差\n\nc.具体功能实现：（1）如果星历文件中半长轴A等于0，直接退出函数；\n（2）计算GPS卫星时和星历参考时刻的差值，根据卫星所属的系统选择对应的常量，然后计算当前时刻的平近点角M；\n（3）采用牛顿迭代法计算偏近点角E，当前后两次差距小于RTOL_KEPLER（1E-13）时停止迭代；\n（4）计算升交点赤经U、卫星矢径R、卫星轨道倾角I等，得到卫星在轨道平面中的位置；\n（5）计算卫星的真近点角O，得到坐标转换矩阵，将卫星的轨道平面坐标转换到惯性坐标系中，如果是北斗的地球同步轨道卫星，需要采用另一种计算方法；\n（6）用GPS时减去星历参考时间，得到tk，对tk使用二阶项修正和相对论效应修正，得到修正过的卫星钟差，最后根据星历文件中的用户测距精度URA得到卫星位置和钟差的协方差；\nc.问题\n时间关系，tk代表什么，以及星历参考时间， 观测数据中的时间减去伪距除以光速得到的时间，这些都是什么含义？\n\n13、rescodea.函数功能说明：计算在当前接收机位置和钟差值，定位方程的右端部分伪距残差 v(nv*1)、几何矩阵 H(NX*nv)、此时所得的伪距残余的方差 var、所有观测卫星的 azel{方位角、仰角}、定位时有效性 vsat、定位后伪距残差 resp、参与定位的卫星个数 ns和方程个数 nv\nstatic int rescode(int iter, const obsd_t *obs, int n, const double *rs,                   const double *dts, const double *vare, const int *svh,                   const nav_t *nav, const double *x, const prcopt_t *opt,                   double *v, double *H, double *var, double *azel, int *vsat,                   double *resp, int *ns)\n\nb.函数参数说明：int iter\t\t\t\t\t当前迭代次数const obsd_t *obs\t\t\tOBS观测文件int n\t\t\t\t\t\t观测文件的数目const double *rs\t\t\t卫星位置和速度const double *dts\t\t\t卫星钟差和钟漂const double *vare\t\t\t卫星位置和时钟的协方差const int *svh\t\t\t\t卫星健康标志const nav_t *nav\t\t\t导航电文const double *x\t\t\t\t本次迭代开始之前的定位值const prcopt_t *opt\t\t\t处理选项double *v\t\t\t\t\t定位方程的右端部分，伪距残余double *H\t\t\t\t\t定位方程中的几何矩阵double *var\t\t\t\t\t参与定位的伪距残余方差double *azel\t\t\t\t对于当前定位值，每一颗观测卫星的{方位角、高度角}int *vsat\t\t\t\t\t每一颗观测卫星在当前定位时是否有效double *resp\t\t\t\t每一颗观测卫星的伪距残余，(P-(r+c*dtr-c*dts+I+T))int *ns\t\t\t\t\t\t参与定位的卫星的个数\n\nc.具体功能实现：（1）将之前得到的定位解信息赋值给rr和dtr数组，以进行关于当前解的伪距残余的相关计算；\n（2）调用ecef2pos函数，将上一步中得到的位置信息由 ECEF转化为大地坐标系（WGS84），并将x[3]接收机钟差赋给dts；\n（3）将vsat、azel和resp数组置0，因为在前后两次定位结果中，每颗卫星的上述信息都会发生变化；\n（4）将时间设置为当前观测文件的时间，调用satsys函数，验证卫星编号是否合理及其所属的导航系统，如果否则跳过这次数据；\n（5）检测当前观测卫星编号是否和下一个相邻数据的卫星编号重复，如果重复则跳过，继续去处理下一条，节省计算资源；\n（6）调用satexclude函数处理选项中事先指定定位时要排除的导航系统或卫星\n（7）调用geodist函数计算卫星和当前接收机位置之间的几何距离r和接收机到卫星方向的观测矢量e，然后检验r是否大于0，其中卫星矢径e即伪距方程中的 ；\n（8）调用satazel函数计算在接收机位置处的站心坐标系中卫星的方位角和仰角，若仰角低于截断值，不处理此数据；\n（9）调用snrmask函数根据接收机高度角和信号频率来检测该信号是否可用；\n（10）调用ionocorr函数计算电离层延时I, 所得的电离层延时是建立在L1信号上的，当使用其它频率信号时，依据所用信号频组中第一个频率的波长与L1波长的大小关系，对上一步得到的电离层延时进行修正（相除后取平方）；\n（11）调用tropcorr函数计算对流层延时T，\n（12）调用prange 函数，得到经过DCB校正后的伪距值P，\n（13）计算伪距残差：v[nv]=P-(r+dtr-CLIGHT*dts[i*2]+dion+dtrp)；\n（14）组装几何矩阵，前3行为6卫星矢径的反向，第4行为1，其它行为0，原因是后面四用于其他系统的钟偏修正；\n（15）如果卫星所属系统不是GPS，还需要在伪距残差中根据所属系统加入对应的时钟偏差修正，并将几何矩阵中的对应行设为1；\n（16）将参与定位的卫星的定位有效性标志设为1，给当前卫星的伪距残余赋值，参与定位的卫星个数ns加1；\n（17）调用varerr函数，计算此时的导航系统误差，然后累加计算用户测距误差URE；\n（18）为了防止不满秩的情况，人为地把矩阵H补满秩了，如果某个非GPS系统无观测值，将除了系统对应的钟差外的其他参数的系数都设为0，方差设为0.01；\n以下为设计矩阵H，其中第四行为GLONASS系统，第nv-1行为没有观测值的CMP系统，为了补秩，将钟差设为1：\nd.问题\n为什么要先把坐标从地心地固坐标系转换为大地坐标系，这样有什么影响\n电离层延迟改正，为什么可以直接利用比例关系得到其他频率的电离层延迟\n这里是（i&lt;n&amp;&amp;i&lt;MAXOBS），而之前的是（i&lt;n&amp;&amp;i&lt;MAXOBS*2）\n\ne.注意\n在组装几何矩阵时把卫星矢径向量反向的原因是在geodist函数中，e表示，与方程反向\n\n14、valsola.函数功能说明：确认当前解是否符合要求，即伪距残差小于某个卡方分布值和GDOP小于某个门限值\nstatic int valsol(const double *azel, const int *vsat, int n,                  const prcopt_t *opt, const double *v, int nv, int nx,                  char *msg)\n\nb.函数参数说明：const double *azel\t\t\t对于当前定位值，每一颗观测卫星的{方位角、高度角}const int *vsat\t\t\t\t表征卫星在定位时是否有效int n\t\t\t\t\t\t观测文件的数量const prcopt_t *opt\t\t\t处理选项const double *v\t\t\t\t定位后的伪距残差：(P-(r+c*dtr-c*dts+I+T))int nv\t\t\t\t\t\t定位方程组的方程个数int nx\t\t\t\t\t\t未知数的个数char *msg\t\t\t\t\t异常信息\n\nc.具体功能实现：（1）首先计算定位后伪距残差的平方和vv；\n（2）检查是否满足nv大于nx，以及vv大于，如果大于，则说明定位结算误差过大，返回0；\n（3）复制那些定位结果有效的卫星的azels，并统计定位有效的卫星的个数；\n（4）调用dops函数，计算各种精度因子DOP，并检查是否有精度因子小于0或者大于maxGDOP；\n15、resdopa.函数功能说明：计算定速方程组左边的几何矩阵和右端的速度残余，返回定速时所使用的卫星数目\nstatic int resdop(const obsd_t *obs, int n, const double *rs, const double *dts,                  const nav_t *nav, const double *rr, const double *x,                  const double *azel, const int *vsat, double err, double *v,                  double *H)\n\nb.函数参数说明：const obsd_t *obs\t\t\t观测文件int n\t\t\t\t\t\t观测文件的数量const double *rs\t\t\t卫星位置和速度const double *dts\t\t\t卫星钟差和钟漂const nav_t *nav\t\t\t导航电文const double *rr\t\t\t接收机位置和速度const double *x\t\t\t\t本次迭代开始之前的速度const double *azel\t\t\t对于当前定位值，每一颗观测卫星的{方位角、高度角}const int *vsat\t\t\t\t表征卫星在定位时是否有效double err\t\t\t\t\t观测文件中的多普勒观测值double *v\t\t\t\t\t定速方程中的速度残差double *H\t\t\t\t\t定速方程中的几何矩阵\n\nc.具体功能实现：（1）首先将接收机的坐标从ECEF转换到大地坐标系，然后调用xyz2enu函数得到从ECEF到当地坐标系的坐标转换矩阵；\n（2）调用sat2freq函数根据卫星和观测文件得到载波频率；\n（3）如果卫星的多普勒观测值，频率，有效性和坐标的二范数不满足条件，跳过该次数据；\n（4）计算当前接收机位置下ENU中的视向量，然后转换得到ECEF中视向量的值；\n（5）计算ECEF中卫星相对于接收机的速度，然后再计算出考虑了地球自转的用户和卫星之间的几何距离变化率；\n（6）根据公式计算出方程右端项的多普勒残余，然后再构建左端项的几何矩阵，最后将观测方程数增1；\n16、ecef2posa.函数功能说明：将接收机位置由ECEF-XYZ 转换为大地坐标系pos\nextern void ecef2pos(const double *r, double *pos)\n\nb.函数参数说明：const double *r\t\t\t\t在ECEF坐标系中的坐标（x,y,z）{m}double *pos\t\t\t\t\t大地坐标系（lat,lon,h）{rad,m}\n\nc.具体功能实现：（1）根据WGS84坐标系中的常数定义转换系数；\n（2）具体转换公式如下：\nc.问题\nfor (z=r[2],zk=0.0;fabs(z-zk)&gt;=1E-4;) {\t\t\t//for循环的目的是什么？\n\n17、satexcludea.函数功能说明：处理选项中事先指定定位时排除哪些导航系统或卫星\nextern int satexclude(int sat, double var, int svh, const prcopt_t *opt)\n\nb.函数参数说明：int sat\t\t\t\t\t\t卫星编号double var\t\t\t\t\t星历下方差{m^2}int svh\t\t\t\t\t\t卫星健康标志const prcopt_t *opt\t\t\t处理选项const double *r\t\t\t\t在ECEF坐标系中的坐标（x,y,z）{m}double *pos\t\t\t\t\t大地坐标系（lat,lon,h）{rad,m}\n\nc.具体功能实现：（1）调用satsys函数获得卫星所属的系统，并没有获得卫星的PRN编号；\n（2）如果卫星健康标志小于0，则说明这个星历不可用，返回1；\n（3）如果处理选项不为空，则检测处理选项中对该卫星的排除标志的值(1:excluded,2:included)，之后再检测该卫星所属的导航系统与处理选项中预先设定的是否一致：否-返回1，是-进入下一步（这里的代码顺序可能有点问题）；\n（4）如果svh大于0，或者var大于MAX_VAR_EPH，则发出异常信息，返回1（这里验证的顺序可能应该放在前面）；\n18、geodista.函数功能说明：计算卫星和当前接收机位置之间的几何距离和接收机到卫星方向的观测矢量 e\nextern double geodist(const double *rs, const double *rr, double *e)\n\nb.函数参数说明：const double *rs\t\t\t卫星位置和速度const double *rr\t\t\t接收机位置和速度double *e\t\t\t\t\t接收机到卫星方向的观测矢量\n\nc.具体功能实现：（1）检查卫星到WGS84坐标系原点的距离是否大于基准椭球体的长半径；\n（2）计算接收机到卫星方向的观测矢量e（）；\n（3）考虑到地球自转，信号发射时刻卫星的位置与信号接收时刻卫星的位置在WGS84坐标系中不一致，需要用Sagnac效应进行改正；\n19、satazela.函数功能说明：计算在接收机位置处的站心坐标系中卫星的方位角和俯仰角，若低于截断值，不处理此数据\nextern double satazel(const double *pos, const double *e, double *azel)\n\nb.函数参数说明：const double *pos\t\t\t大地坐标系{E,N.U}{lat,lon,h}double *azel\t\t\t\t方位角和俯仰角{az,el}{rad}const double *e\t\t\t\t接收机到卫星方向的观测矢量\n\nc.具体功能实现：（1）检查接收机的高度是否大于地球长半轴RE_WGS84；\n（2）调用ecef2enu函数，将接收机的观测向量e转换到以接收机为原点的站心坐标系中；\n（3）计算方位角和俯仰角，如果az小于0，加上2*pi；（4）如果azel不为空，分别将az和el赋值给azel[0]和azel[1]；\n20、snrmaska.函数功能说明：根据接收机高度角和信号频率来检测该信号是否可用\nstatic int snrmask(const obsd_t *obs, const double *azel, const prcopt_t *opt)\n\nb.函数参数说明：const obsd_t *obs\t\t\t观测文件const double *azel\t\t\t方位角和俯仰角{az,el}{rad}const prcopt_t *opt\t\t\t处理选项\n\nc.具体功能实现：（1）调用testsnr函数，根据高度角和观测文件中的信噪比等判断信噪比是否小于设定的最小值；\n（2）如果opt中选择了双频组合消无电离层模型，需要将testsnr函数的参数变更一下；\n21、ionocorra.函数功能说明：使用klobuchar模型计算给定电离层选项时的电离层延迟I(m)\nextern int ionocorr(gtime_t time, const nav_t *nav, int sat, const double *pos,                    const double *azel, int ionoopt, double *ion, double *var)\n\nb.函数参数说明：gtime_t time\t\t\t\tconst nav_t *nav\t\t\t导航电文int sat\t\t\t\t\t\t卫星数目const double *pos\t\t\t接收机坐标{lat,lon,h}{rad,m}const double *azel\t\t\t方位角和俯仰角{az,el}{rad}int ionoopt\t\t\t\t\t电离层改正选项double *ion\t\t\t\t\t电离层延迟{L1}{m}double *var\t\t\t\t\t电离层延迟方差{l1}{m^2}\n\nc.具体功能实现：（1）根据电离层改正选项ionoopt选择对应的模型，如果是GPS，调用ionmodel函数，根据输入的时间、导航电文中的数据、接收机的位置，方位角和俯仰角，将结果赋值给ion，并返回1；\n（2）如果没有符合的选项，将ion设为0，var设为SQR(ERR_ION)，并返回1；\nd.注意（1）当 ionoopt==IONOOPT_BRDC时，调用ionmodel函数，计算 Klobuchar模型时的电离层延时(L1，m)；\n（2）当 ionoopt==IONOOPT_TEC时，调用iontec函数，计算 TEC网格模型时的电离层延时(L1，m)；\n（3）当 ionoopt==IONOOPT_IFLC时，此时通过此函数计算得到的延迟和方差都为0。对于 IFLC模型，其延时值在prange函数中计算伪距时已经包括在里面了，而方差是在varerr函数中计算的，并且会作为导航系统误差的一部分给出。\n22、tropcorra.函数功能说明：使用标准大气和Saastamoinen模型计算给定对流层选项时的对流层延迟 T(m)\nextern int tropcorr(gtime_t time, const nav_t *nav, const double *pos,                    const double *azel, int tropopt, double *trp, double *var)\n\nb.函数参数说明：gtime_t time\t\t\t\tconst nav_t *nav\t\t\t导航电文int sat\t\t\t\t\t\t卫星数目const double *pos\t\t\t接收机坐标{lat,lon,h}{rad,m}const double *azel\t\t\t方位角和俯仰角{az,el}{rad}int tropopt\t\t\t\t\t电离层改正选项double *trp\t\t\t\t\t电离层延迟{L1}{m}double *var\t\t\t\t\t电离层延迟方差{l1}{m^2}\n\nc.具体功能实现：（1）根据对流层改正选项tropopt选择模型，如果是TROPOPT_SAAS、TROPOPT_EST和TROPOPT_ESTG，调用tropmodel函数，根据输入的时间、接收机的位置，方位角和俯仰角，湿度（humidity），将结果赋值给trp，并返回1；\n（2）如果没有符合的选项，将ion和var设为0，并返回1；\nd.问题\n对流层改正模型中虽然传入了时间t，但是好像没有用到\n\n23、prangea.函数功能说明：返回经过DCB校正后的伪距P\nstatic double prange(const obsd_t *obs, const nav_t *nav, const prcopt_t *opt,                     double *var)\n\nb.函数参数说明：const obsd_t *obs\t\t\t观测文件const nav_t *nav\t\t\t导航电文const prcopt_t *opt\t\t\t处理选项double *var\t\t\t\t\t伪距测量的码偏移误差\n\nc.具体功能实现：（1）如果伪距P1等于0，或者opt中选择了双频消电离层组合并且P2等于0，返回0，P1和P2代表伪距观测值；\n（2）根据卫星所属系统选择对应的解决方案，如果选择了GPS或者GLO，首先判断观测文件中的CODE是否匹配，然后将导航电文中的卫星差分码偏差DCB（cbias）加入P1和P2中；\n（3）接下来判断是否选择了双频消电离层组合，如果没有，设置var等于SQR(ERR_CBIAS)，根据导航电文得到卫星的群时延TGD，然后返回将群时延引入到P1后的改正值，如果是GPS，直接返回P1-b1；\n（4）如果选择了双频消电离层组合，利用P1和P2的关系返回包含P1和P2的无电离层组合值；\nc.问题\n这个函数在对伪距进行哪部分的计算？计算进行C/A码修正后的伪距值？\n为什么双频消电离层组合要这样处理？\n\nd.注意DCB差分码偏差，针对伪距，是由不同类型的GNSS信号在卫星和接收机不同通道产生的时间延迟（硬件延迟／码偏差）差异 。由于卫星播发的测距码类型很多， C1、 P1、 P2 等 ，不同的测距信号虽然 在同一台卫星钟的驱动下生成的，因而花费的时间也不同。我们把卫星钟脉冲驱动下开始生成测距信号至信号生成并最终离开卫星发射天线相位中心之间所花费的时间称为信号在卫星内部的时延。DCB体现的就是不同码信号时延的差。分为：\n频内偏差：相同频率不同码之间存在的偏差（如P1-C1、P2-C2等）频间偏差：不同频率之间存在的偏差（如P1-P2）\n24、varrera.函数功能说明：计算导航系统伪距测量值的误差\nstatic double varerr(const prcopt_t *opt, double el, int sys)\n\nb.函数参数说明：const prcopt_t *opt\t\t\t处理选项double el\t\t\t\t\t高度角int sys\t\t\t\t\t\t卫星所属系统\n\nc.具体功能实现：（1）首先根据卫星所属的系统确定误差因子fact；\n（2）如果高度角小于最小高度角，令高度角等于最小高度角；\n（3）根据观测文件中记录的伪距观测误差和高度角计算varr；\n（4）如果选择了双频消电离层组合，IFLC模型的方差也会添加到最终计算得到的方差中；\nc.问题\n为什么观测文件中会有伪距观测误差？\n\n二、rtkpos.crtklib代码详解——rtkpos.c - 博客园—哆啦A梦 - 博客园 (cnblogs.com)\n1、rtkposa.函数功能说明：主函数，计算导航系统伪距测量值的误差\nextern int rtkpos(rtk_t *rtk, const obsd_t *obs, int n, const nav_t *nav)\n\nb.函数参数说明：* args   : rtk_t *rtk       IO  RTK control/result struct\t\t\t\t\t\t\trtk控制结构体*            rtk-&gt;sol       IO  solution\t\t\t\t\t\t\t\t\t\t\t结果结构体*                .time      O   solution time\t\t\t\t\t\t\t\t\t\t结果的时间*                .rr[]      IO  rover position/velocity\t\t\t\t\t\t\t\t流动站的位置和速度*                               (I:fixed mode,O:single mode)*                .dtr[0]    O   receiver clock bias (s)\t\t\t\t\t\t\t\t接收机钟差*                .dtr[1-5]  O   receiver GLO/GAL/BDS/IRN/QZS-GPS time offset (s)\t其他系统的钟差*                .Qr[]      O   rover position covarinace\t\t\t\t\t\t\t流动站位置协方差*                .stat      O   solution status (SOLQ_???)\t\t\t\t\t\t\t结果的状态*                .ns        O   number of valid satellites\t\t\t\t\t\t\t有效卫星数*                .age       O   age of differential (s)\t\t\t\t\t\t\t\t\t*                .ratio     O   ratio factor for ambiguity validation*            rtk-&gt;rb[]      IO  base station position/velocity\t\t\t\t\t\t基准站位置和速度*                               (I:relative mode,O:moving-base mode)\t*            rtk-&gt;nx        I   number of all states\t\t\t\t\t\t\t\t状态量总数*            rtk-&gt;na        I   number of integer states\t\t\t\t\t\t\t整数状态量总数*            rtk-&gt;ns        O   number of valid satellites in use\t\t\t\t\t使用中的有效卫星数*            rtk-&gt;tt        O   time difference between current and previous (s)\t历元时间差*            rtk-&gt;x[]       IO  float states pre-filter and post-filter\t\t\t\t浮点解*            rtk-&gt;P[]       IO  float covariance pre-filter and post-filter\t\t\t浮点解的协方差*            rtk-&gt;xa[]      O   fixed states after AR\t\t\t\t\t\t\t\t固定解*            rtk-&gt;Pa[]      O   fixed covariance after AR\t\t\t\t\t\t\t固定解的协方差*            rtk-&gt;ssat[s]   IO  satellite {s+1} status\t\t\t\t\t\t\t\t卫星状态控制结构体数组*                .sys       O   system (SYS_???)\t\t\t\t\t\t\t\t\t卫星导航系统*                .az   [r]  O   azimuth angle   (rad) (r=0:rover,1:base)\t\t\t方位角*                .el   [r]  O   elevation angle (rad) (r=0:rover,1:base)\t\t\t高度角*                .vs   [r]  O   data valid single     (r=0:rover,1:base)\t\t\t有效卫星单一标志*                .resp [f]  O   freq(f+1) pseudorange residual (m)\t\t\t\t\t伪距残差*                .resc [f]  O   freq(f+1) carrier-phase residual (m)\t\t\t\t载波相位残差*                .vsat [f]  O   freq(f+1) data vaild (0:invalid,1:valid)\t\t\t有效卫星标志*                .fix  [f]  O   freq(f+1) ambiguity flag\t\t\t\t\t\t\t模糊度状态标志*                               (0:nodata,1:float,2:fix,3:hold)*                .slip [f]  O   freq(f+1) cycle slip flag\t\t\t\t\t\t*                               (bit8-7:rcv1 LLI, bit6-5:rcv2 LLI,*                                bit2:parity unknown, bit1:slip)*                .lock [f]  IO  freq(f+1) carrier lock count*                .outc [f]  IO  freq(f+1) carrier outage count\t\t\t\t\t\t载波中断计数*                .slipc[f]  IO  freq(f+1) cycle slip count*                .rejc [f]  IO  freq(f+1) data reject count*                .gf        IO  geometry-free phase (L1-L2) (m)*                .gf2       IO  geometry-free phase (L1-L5) (m)*            rtk-&gt;nfix      IO  number of continuous fixes of ambiguity\t\t\t\t\t*            rtk-&gt;neb       IO  bytes of error message buffer\t\t\t\t\t\t错误信息的缓冲区长度*            rtk-&gt;errbuf    IO  error message buffer\t\t\t\t\t\t\t\t错误信息缓冲区*            rtk-&gt;tstr      O   time string for debug*            rtk-&gt;opt       I   processing options\t\t\t\t\t\t\t\t\t处理选项*          obsd_t *obs      I   observation data for an epoch\t\t\t\t\t\t一个历元的观测文件*                               obs[i].rcv=1:rover,2:reference*                               sorted by receiver and satellte*          int    n         I   number of observation data\t\t\t\t\t\t\t观测文件的数目*          nav_t  *nav      I   navigation messages\t\t\t\t\t\t\t\t\t导航电文\n\nsol_t：结果结构体gtime_t time;       //GPST时间double rr[6];       /* 位置、速度结果 (m|m/s) */                        /* {x,y,z,vx,vy,vz} or {e,n,u,ve,vn,vu} */float  qr[6];       /* 位置估计协方差阵 (m^2) */                        /* {c_xx,c_yy,c_zz,c_xy,c_yz,c_zx} or */                        /* {c_ee,c_nn,c_uu,c_en,c_nu,c_ue} */float  qv[6];       /* 速度估计协方差阵 (m^2/s^2) */double dtr[6];      /* receiver clock bias to time systems (s) */uint8_t type;       /* type (0:xyz-ecef,1:enu-baseline) */uint8_t stat;       /* solution status (SOLQ_???) */uint8_t ns;         //有效卫星数float age;          //差分龄期float ratio;        //模糊度固定Ratio值float thres;        //模糊度固定的Ratio阈值\n\nc.具体功能实现：（1）根据opt中的选项和opt中的数据设置基准站坐标，rtk-&gt;rb[i]=i&lt;3?opt-&gt;rb[i]:0.0，速度设置为0；\n（2）统计基准站的个数nr和流动站的个数nu，可用于后面判断是否满足差分条件；\n（3）给time赋值先前历元的时间rtk-&gt;sol.time；\n（4）调用pntpos函数计算流动站的坐标，作为后续kalman滤波的近似坐标，如果单点定位无解，并且!rtk-&gt;opt.dynamics，则调用outsolstat函数输出结果，然后直接返回0，不进行rtk运算；\n（5）如果time.time!=0，计算当前历元和上一历元的时间差，并赋值给rtk-&gt;tt；\n（6）如果是单点定位模式PMODE_SINGLE，直接调用outsolstat函数输出（4）中的结果，然后返回1；\n（7）如果不是单点模式，抑制单点解的输出，rtk-&gt;sol.stat=SOLQ_NONE；\n（8）如果是精密单点定位模式PMODE_PPP_KINEMA，调用pppos函数，输入rtk结构体、obs文件、流动站数目nu、导航电文nav，然后调用outsolstat函数输出结果，并返回1；\n（9）之后检查基准站的数目nr，如果等于0，则输出错误信息，调用outsolstat函数输出结果，并返回1；\n（10）如果是动基线模式PMODE_MOVEB，首先调用pntpos函数估计基准站的位置和速度，然后计算差分龄期rtk-&gt;sol.age，如果差分龄期大于TTOL_MOVEB，则输出错误信息并返回0，否则，将solb.rr赋值给rtk-&gt;rb，即之前调用pntpos函数得到的估计值，然后进行时间同步，三维位置的坐标加上对应方向的速度乘以差分龄期rtk-&gt;rb[i]+=rtk-&gt;rb[i+3]*rtk-&gt;sol.age；\n（12）如果是非动基线模式，则令差分龄期rtk-&gt;sol.age 等于第一个流动站的观测值时间obs[0].time减去第一个基准站的观测值时间obs[nu].time，然后判断rtk-&gt;sol.age的绝对值是否大于opt-&gt;maxtdiff，如果大于则输出错误信息并返回1；\n（13）调用relpos函数进行相对定位，然后调用outsolstat()输出解算结果，最后返回1；\nc.问题\n这些参数的具体意义，比如差分龄期，以及动基线模式和非动基线模式的区别，动基线模式中基准站的坐标会变化，因此需要计算它的位置\n\n2、outsolstata.函数功能说明：输出rtk_t结构体中的结果\nstatic void outsolstat(rtk_t *rtk)\n\nb.函数参数说明：* args   : rtk_t *rtk       IO  RTK control/result struct\t\t\t\t\t\t\t\trtk控制结构体\n\nc.具体功能实现：（1）判断几个全局变量的状态，如果不符合条件则返回0；\n（2）调用swapsolstat函数根据时间划分结果文件；\n（3）调用rtkoutstat函数将结果写入buff中；\n（4）调用time2gpst函数并将结果赋给tow，然后写入残差和状态；\n3.sbosa.函数功能说明返回单差观测值pi==0.0||pj==0.0?0.0:pi-pj\nstatic double sdobs(const obsd_t *obs, int i, int j, int k)\n\n\n\n4.gfobsa.函数功能说明返回无几何观测值L1*CLIGHT/freq1-L2*CLIGHT/freq2\nstatic double gfobs(const obsd_t *obs, int i, int j, int k, const nav_t *nav)\n\n\n\n5.baselinea.函数功能说明返回基站和流动站之间的基线距离dr[i]=ru[i]-rb[i]\nstatic double baseline(const double *ru, const double *rb, double *dr)\n\n\n\n6.selsata.函数功能说明选择参考站和流动站之间的共视星\nstatic int selsat(const obsd_t *obs, double *azel, int nu, int nr,                  const prcopt_t *opt, int *sat, int *iu, int *ir)\n\nb.函数参数说明：args: \t\t I     const obsd_t *obs\t\t\t参考站和流动站的观测文件\t\t\t\t I     double *azel\t\t\t I     int nu \t\t\t\t\t\t I     int nr              I     const prcopt_t *opt            IO    int *sat\t\t\t\t\t\t记录共视星的卫星编号            IO    int *iu\t\t\t\t\t\t流动站的观测历元            IO    int *ir\t\t\t\t\t\t基准站的观测历元\n\nc.具体功能实现：（1）比较参考站和流动站观测文件中是否存在编号相同的卫星，并且检验卫星高度角是否大于设定的阈值，如果大于，则将卫星编号和参考站、流动站的观测历元分别记录在sat、iu和ir中，注意参考站和流动站的观测文件似乎都是存储在obs中的，其中流动站的序号为obs[0:nu]，参考站的序号为[nu:nu+nr]。\n7.udposa.函数功能说明更新接收机位置等状态量和协方差矩阵，使用前一历元记录的位置作为当前历元的估计值\nstatic void udpos(rtk_t *rtk, double tt)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I     double tt\t\t\t\t\t\n\nc.具体功能实现：（1）初始化F、P、FP和x、xp等；\n（2）如果是PMODE_FIXED模式，直接用流动站的位置初始化状态向量；\n（3）如果位置没有初始化，即norm(rtk-&gt;x,3)&lt;=0.0，则用前一历元的位置来初始化，如果选择了动态模式，还需要利用前一历元的速度和加速度1E-6来初始化当前历元的速度和加速度；\n（4）此时如果是PMODE_STATIC模式，位置已经初始化过，即rtk中存储的是前一历元的结果，可以直接退出函数；\n（5）如果不是PMODE_STATIC模式且不是动态模式，则用前一历元的位置来作为当前历元的估计值；\n（6）检查方差，如果超过阈值，将前一历元的位置，速度和加速度作为当前历元的估计值；\n（7）生成有效状态的索引ix，然后组装状态转移矩阵F、协方差矩阵P、状态向量x和，再通过矩阵运算得到先验估计状态向量xp=F*x和先验估计协方差矩阵P=F*P*F+Q，然后将xp和P分别赋值到rtk中对应的x和P；\n（8）最后计算加速度的过程噪声矩阵Q，并将Q加入先验协方差矩阵P中；\nd.注意事项\n区分一下每种模式具体都需要那些状态，以及是怎么初始化的\n\n8.udiona.函数功能说明更新电离层参数和协方差矩阵\nstatic void udion(rtk_t *rtk, double tt, double bl, const int *sat, int ns)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I     double tt            I     double bl\t\t\t\t基线距离            IO    const int *sat            IO    ns\n\nc.具体功能实现：（1）如果rtk中存储的电离层参数不为0，而且两个频率的相位中断计数都大于重置电离层参数的阈值，将rtk中的电离层参数重置为0；\n（2）依次为观测值中每颗卫星赋值，如果电离层参数等于0，赋值为1E-6，方差为SQR(rtk-&gt;opt.std[1]*bl/1E4，如果不等于0，电离层参数保持不变，在协方差矩阵P中加入和高度角有关的过程噪声SQR(rtk-&gt;opt.prn[1]*bl/1E4*fact)*fabs(tt)；\nd.注意事项\n比ppp中的简单很多\n可见电离层噪声和基线距离、高度角有关\n\n9.udtropa.函数功能说明更新接收机位置等状态量和协方差矩阵，使用前一历元记录的位置作为当前历元的估计值\nstatic void udtrop(rtk_t *rtk, double tt, double bl)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I     double tt            I     double bl\t\t基线距离\n\nc.具体功能实现：（1）如果rtk中存储的对流层参数等于0，将其初始化为INIT_ZWD=0.15m，如果选择了TROPOPT_ESTG模式，继续将水平方向上的两个梯度初始化为1E-6；\n（2）如果rtk中存储的对流层参数不等于0，则在协方差矩阵P中加入过程噪声，其中梯度增加的过程噪声为准天顶延迟ZTD增加的过程噪声SQR(rtk-&gt;opt.prn[2]*0.3)*fabs(tt)再乘以0.3；\nd.注意事项\n比ppp中的简单很多，没有用到各种模型\n对流层的状态量数量和估计模型相关，和卫星数量无关\n\n10.udrcvbiasa.函数功能说明更新接收机的硬件偏差\nstatic void udrcvbias(rtk_t *rtk, double tt)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I     double tt\n\nc.具体功能实现：（1）如果rtk中存储的对应参数等于0，将其初始化为1E-6；\n（2）如果rtk-&gt;nfix&gt;=rtk-&gt;opt.minfix&amp;&amp;rtk-&gt;sol.ratio&gt;rtk-&gt;opt.thresar[0]，即模糊度固定次数大于门限值且模糊度解算比例因子大于门限值，则P阵不会变，将对应参数赋值为固定状态量xa；\n（3）否则，在协方差矩阵中加入SQR(PRN_HWBIAS)*fabs(tt)；\nd.注意事项\n这个得去看看整周模糊度固定了\n为什么和GLONASS系统有关？\n\n11.detslp_lla.函数功能说明通过LLI检测周跳\nstatic void detslp_ll(rtk_t *rtk, const obsd_t *obs, int i, int rcv)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I     const obsd_t *obs\t\t\t I\t   int i\t\t\t I     int rcv\n\nc.具体功能实现：（1）对每个频率进行循环，如果当前频率对应的载波观测值等于0，或者观测时间和前一次载波相位时间之差小于阈值，跳过当前频率；\n（2）首先将上一历元该卫星的周跳标志取出存在LLI变量中，该变量之后会用来判断上一历元和当前历元中半周跳标志（LLI&amp;2，即LLI的bit 1位）是否发生变化；\n（3）将当前历元原始观测量中的LLI赋值给slip变量；\n（4）如果上一历元和当前历元的半周跳标志不同，则认为有周跳，将slip的第0位置1；\n（5）存放当前历元的LLI和周跳；\nd.注意事项\n没学过\nLLI描述主要由前端GNSS芯片决定，RTKLIB在前向处理和后向处理上对利用LLI进行周跳探测上有所不同\n\n12.detslp_gfa.函数功能说明通过几何无关组合GF`检测周跳\nstatic void detslp_gf(rtk_t *rtk, const obsd_t *obs, int i, int j,                      const nav_t *nav)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I     const obsd_t *obs\t\t\t I\t   int i\t\t\t\t当前观测历元序号\t\t\t I     int j\t\t\t I\t   const nav_t *nav\n\nc.具体功能实现：（1）对每个频率调用gfobs函数将无几何距离观测值返回给g1，将前一历元的无几何距离观测值赋给临时变量g0，并将g1赋给前一历元的无几何距离观测值，如果g1和g0的差值大于阈值，则认为有周跳，将周跳标志设为1；\nd.注意事项\nrtk-&gt;ssat[sat-1].gf[k-1]=g1;\n这里为什么要把给给gf[k-1]而不是gf[k]？\n\n13.udbiasa.函数功能说明更新整周模糊度\nstatic void udbias(rtk_t *rtk, double tt, const obsd_t *obs, const int *sat,                   const int *iu, const int *ir, int ns, const nav_t *nav)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I\t   double tt\t\t\t I     const obsd_t *obs\t\t\t I\t   const int *sat\t\t\t I\t   const int *iu\t\t\t I     const int *ir\t\t\t I\t   int ns\t\t\t I\t   const nav_t *nav\n\nc.具体功能实现：（1）循环检测各个卫星观测量相位周跳情况，依次调用detslp_ll，detslp_dop和detslp_gf检测周跳，更新半周跳有效标志位；\n（2）根据整周模糊度的解算方式、 周跳检测标志、码相位等信息更新整周状态量等，遍历不同频点的不同观测值，如果固定模式为ARMODE_INST，并且模糊度不等于0，重置载波相位模糊度，如果相位中断计数器超过阈值，并且模糊度不等于0，重置载波相位模糊度和相位中断计数器；\n（4）总结起来就是，如果状态量等于0或者相位中断计数器没有超过阈值，不做处理；\n（5）对每个观测值进行循环，将协方差矩阵加上过程噪声，如果选择无电离层组合，并且两个频率上出现了周跳，将模糊度重置为0；\n（6）如果不是无电离层组合，将载波相位模糊度设为cp-pr*freqi/CLIGHT；\n（7）否则，利用载波相位的无电离层组合观测值减去码伪距的无电离层组合观测值，得到近似的载波相位模糊度，同时记录两次历元的模糊度之差，并将每颗卫星的模糊度加上所有卫星的历元间模糊度偏差的平均值；\n（8）再次对当前频率的所有观测值进行遍历，如果状态向量中的模糊度参数等于0并且bias不等于0，将rtk中对应状态向量赋值为bias；\nd.注意事项\n\n模糊度固定模式\n#define ARMODE_OFF  0                 /* AR mode: off   不固定模糊度*/#define ARMODE_CONT 1                 /* AR mode: continuous      模糊度连续固定*/#define ARMODE_INST 2                 /* AR mode: instantaneous   瞬时模糊度固定*/#define ARMODE_FIXHOLD 3              /* AR mode: fix and hold   模糊度固定并保持*/#define ARMODE_WLNL 4                 /* AR mode: wide lane/narrow lane */#define ARMODE_TCAR 5                 /* AR mode: triple carrier ar */\n\n\n\nrtk-&gt;ssat[i-1].lock[k]=-rtk-&gt;opt.minlock;是什么意思？\n\n\n\n14.udstatea.函数功能说明状态向量时间更新\nstatic void udstate(rtk_t *rtk, const obsd_t *obs, const int *sat,                    const int *iu, const int *ir, int ns, const nav_t *nav)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I     const obsd_t *obs\t\t\t I\t   const int *sat\t\t\t I\t   const int *iu\t\t\t I     const int *ir\t\t\t I\t   int ns\t\t\t I\t   const nav_t *nav\n\nc.具体功能实现：（1）更新位置速度以及加速度；\n（2）如果将电离层设为待估参数，计算基线距离，然后更新电离层参数；\n（4）如果将对流层设为待估参数，计算基线距离，然后更新对流层参数；\n（5）如果是GLONASS，更新接收机硬件延迟；\n（6）如果是PMODE_DGPS，更新模糊度参数；\n15.zdres_sata.函数功能说明为卫星计算非差载波相位和伪距残差\nstatic void zdres_sat(int base, double r, const obsd_t *obs, const nav_t *nav,                      const double *azel, const double *dant,                      const prcopt_t *opt, double *y, double *freq)\n\nb.函数参数说明：args: \t\t I     int base\t\t\t I     const obsd_t *obs\t\t\t I\t   const nav_t *nav\t\t\t I\t   const double *azel\t\t\t I\t   const double *dant\t\t\t\t天线改正\t\t\t I\t   const prcopt_t *opt\t\t\t IO    double *y\t\t\t\t\t\t残差\t\t\t IO\t   double *freq\n\nc.具体功能实现：（1）如果在配置中选择了无电离层线性组合：调用testsnr函数检查L1，L2观测量的载噪比是否大于配置中SNR Mask的最低要求，计算无电离层线性组合系数C1、C2, 并采用该系数计算无电离层组合的天线偏移量，计算无电离层线性组合载波相位和伪距残差：\n残差 = 观测量 - 卫地距 - 天线偏移量if (obs-&gt;L[0]!=0.0&amp;&amp;obs-&gt;L[1]!=0.0) {    y[0]=C1*obs-&gt;L[0]*CLIGHT/freq1+C2*obs-&gt;L[1]*CLIGHT/freq2-r-dant_if;}if (obs-&gt;P[0]!=0.0&amp;&amp;obs-&gt;P[1]!=0.0) {    y[1]=C1*obs-&gt;P[0]+C2*obs-&gt;P[1]-r-dant_if;}\n\n（2）如果不是无电离层组合：调用testsnr函数，检查每个频段的载噪比是否大于配置中SNR Mask的要求，计算每个频段下的载波相位、伪距残差：\n残差 = 观测量 - 卫地距 - 天线偏移量if (obs-&gt;L[i]!=0.0) y[i   ]=obs-&gt;L[i]*CLIGHT/freq[i]-r-dant[i];if (obs-&gt;P[i]!=0.0) y[i+nf]=obs-&gt;P[i]               -r-dant[i];\n\nd.问题\nUD (undifferenced) phase/code residual for satellite 指的是用于计算卫星的位置吗？\n明显不是\n\n16.zdresa.函数功能说明计算非差载波相位和伪距残差\nstatic int zdres(int base, const obsd_t *obs, int n, const double *rs,                 const double *dts, const double *var, const int *svh,                 const nav_t *nav, const double *rr, const prcopt_t *opt,                 int index, double *y, double *e, double *azel, double *freq)\n\nb.函数参数说明：args: \t\t I     int base\t\t\t I     const obsd_t *obs\t\t\t I     n\t\t\t\t\t\t\t观测文件数/观测卫星数\t\t\t I     const double *rs\t\t\t\t卫星位置、速度\t\t\t I     const double *dts\t\t\t卫星钟差、钟漂\t\t\t I\t   const double *var\t\t\t卫星位置和钟差协方差\t\t\t I\t   const int *svh\t\t\t\t卫星健康标志\t\t\t I\t   const nav_t *nav\t\t\t\t导航电文\t\t\t I\t   const double *rr\t\t\t\t接收机位置\t\t\t I\t   const prcopt_t *opt\t\t\t\t\t\t I\t   int index\t\t\t\t\t\t\t\t IO    double *y\t\t\t\t\t残差\t\t\t I\t   double *e\t\t\t\t\t方向余弦\t\t\t I\t   const double *azel\t\t\t IO\t   double *freq\n\nc.具体功能实现：（1）进行地球潮汐改正，将改正数加入先验状态估计rr_，将坐标转换到大地坐标系中；\n（2）依次遍历每个观测值，计算几何距离和高度角、俯仰角，排除指定卫星，在几何距离r中加入卫星钟差改正-CLIGHT*dts[i*2]；\n（3）调用tropmodel计算对流层准天顶延迟ztd，再调用tropmapf计算投影函数，将对流层延迟改正加入r中；\n（4）调用antmodel计算接收机天线相位中心改正，调用zdres_sat利用经过上述改正的r和其他改正数计算非差相位和码残差；\n17.ddresa.函数功能说明计算非差载波相位和伪距残差\nstatic int ddres(rtk_t *rtk, const nav_t *nav, double dt, const double *x,                 const double *P, const int *sat, double *y, double *e,                 double *azel, double *freq, const int *iu, const int *ir,                 int ns, double *v, double *H, double *R, int *vflg)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\t I     const obsd_t *obs\t\t\t I     double dt\t\t\t            I\t   const double *x            I\t   const double *P            I \t   const int *sat            IO    double *y\t\t\t\t\t残差\t\t\t I\t   double *e\t\t\t\t\t方向余弦\t\t\t I\t   const double *azel\t\t\t IO\t   double *freq\t\t\t IO\t   const int *iu\t\t\t IO    const int *ir\t\t\t I\t   int ns            IO\t   double *v            IO    double *H            IO    double *R            IO    int *vflg\n\nc.具体功能实现：三、ppp.c1、ppposa.函数功能说明：主函数，精密单点定位\nextern void pppos(rtk_t *rtk, const obsd_t *obs, int n, const nav_t *nav)\n\nb.函数参数说明： args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\t\t\t I     const obsd_t *obs\t观测文件\t\t\t I     int n \t\t\t\t流动站观测值数目\t\t\t I     const nav_t *nav     导航电文 typedef struct {        /* RTK control/result type */    sol_t  sol;         /* RTK solution */    double rb[6];       /* base position/velocity (ecef) (m|m/s) */    int nx,na;          /* number of float states/fixed states */    double tt;          /* time difference between current and previous (s) */    double *x, *P;      /* float states and their covariance */    double *xa,*Pa;     /* fixed states and their covariance */    int nfix;           /* number of continuous fixes of ambiguity */    ambc_t ambc[MAXSAT]; /* ambibuity control */    ssat_t ssat[MAXSAT]; /* satellite status */    int neb;            /* bytes in error message buffer */    char errbuf[MAXERRMSG]; /* error message buffer */    prcopt_t opt;       /* processing options */} rtk_t; typedef struct {        /* observation data record */    gtime_t time;       /* receiver sampling time (GPST) */    uint8_t sat,rcv;    /* satellite/receiver number */    uint16_t SNR[NFREQ+NEXOBS]; /* signal strength (0.001 dBHz) */    uint8_t  LLI[NFREQ+NEXOBS]; /* loss of lock indicator */    uint8_t code[NFREQ+NEXOBS]; /* code indicator (CODE_???) */    double L[NFREQ+NEXOBS]; /* observation data carrier-phase (cycle) */    double P[NFREQ+NEXOBS]; /* observation data pseudorange (m) */    float  D[NFREQ+NEXOBS]; /* observation data doppler frequency (Hz) */} obsd_t;\n\nc.具体功能实现：（1）初始化一系列数据rs（卫星位置和速度），dts（卫星钟差），var（协方差），v（残差），H（几何矩阵），R（噪声），azel（方位角和高度角），xp，Pp，dr[3]，std[3]，然后令所有卫星的所有频率的rtk-&gt;ssat[i].fix[j]（整周模糊度标志）=0；\n（2）调用udstate_ppp函数进行卡尔曼滤波的预测过程，获得各种状态量的预测值；\n（3）调用satposs函数计算卫星位置和速度；\n（4）如果在配置中选择排除block IIA卫星，调用testeclipse函数，将这些卫星的位置、速度置0；\n（5）如果在配置中选择潮汐修正，调用tidedisp函数进行潮汐修正；\n（6）令nv等于n*rtk-&gt;opt.nf*2+MAXSAT+3，即观测值数量*频率数(1:L1,2:L1+L2,3:L1+L2+L5)*2+全部卫星数+3；\n（7）开始迭代，最大迭代次数为8，首先调用matcpy函数将rtk-&gt;x和rtk-&gt;P中的数据分别复制到xp和Pp中；\n（8）调用ppp_res函数计算预测值与量测值之间的残差；\n（9）调用filter函数进行卡尔曼滤波的量测更新；\n（10）调用ppp_res函数计算量测更新后的残差，然后调用matcpy函数将xp和Pp中的数据分别复制到rtk-&gt;x和rtk-&gt;P中，之后令stat等于SOLQ_PPP，结束迭代；\n（11）如果迭代次数超过最大值，报错；\n（12）如果stat=SOLQ_PPP，调用ppp_ar函数进行整周模糊度计算，并调用ppp_res函数进行残差计算，然后调用matcpy函数将xp和Pp中的数据分别复制到rtk-&gt;xa和rtk-&gt;Pa中，之后令std[i]=sqrt(Pp[i+i*rtk-&gt;nx])，得到均方差，如果小于MAX_STD_FIX，继续令stat=SOLQ_FIX，否则令rtk-&gt;nfix=0；\n（13）调用update_stat函数更新输入、输出参数rtk solution的状态；\nd.问题\nppp_ar是个空函数，返回值为0，因此实际后面的ppp_res函数也不会被调用，实际PPP的解为通过卡尔曼滤波得到的浮点解。\n\n2、udstate_pppa.函数功能说明：进行卡尔曼滤波的一步预测，更新rtk-&gt;x状态量值\nstatic void udstate_ppp(rtk_t *rtk, const obsd_t *obs, int n, const nav_t *nav)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\t\t I     const obsd_t *obs\t观测文件\t\t I     int n \t\t\t\t流动站观测值数目\t\t I     const nav_t *nav     导航电文\n\nc.具体功能实现：（1）调用udpos_ppp函数获得暂时的位置、速度、加速度状态量并赋值给rtk-&gt;x以及相应协方差阵rtk-&gt;P的更新；\n（2）调用udclk_ppp函数，获得暂时的钟差状态量（以GPST为基准）；\n（3）如果在配置中选择的对流层模型为ZTD estimation或者ZTD+grad estimation，调用udtrop_ppp函数进行对流层参数更新；\n（4）如果在配置中选择电离层模型为estimation函数，调用udiono_ppp函数进行电离层参数更新；\n（5）如果选择的频率数(1:L1,2:L1+L2,3:L1+L2+L5)大于等于3，则调用uddcb_ppp函数进行L5接收机 dcb状态量的更新；\n（6）调用udbias_ppp函数进行相位bias状态量的更新；\nc.问题\n参考 RTKLIB Manual 可知状态量包括位置、速度、接收机钟差、对流层参数、卫星相位偏差，PPP算法中使用的量测量为“无电离层组合的伪距、载波相位”；\n\n3、udpos_pppa.函数功能说明：使用rtk中记录的上一次的状态量对位置，速度和加速度等状态量进行时间更新（），如果是STATIC模式，状态量不变，只更新P阵\nstatic void udpos_ppp(rtk_t *rtk)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\n\nc.具体功能实现：（1）如果定位模式是PMODE_FIXED，即流动站固定的定位模式，则调用initx函数，将观测文件中流动站的位置rtk-&gt;opt.ru[i]依次赋值给rtk-&gt;x[i]，即对流动站的位置进行初始化，并将状态协方差阵P的方差（i=j）设置为一个较小的值（1E-8），然后退出函数；\n（2）如果norm(rtk-&gt;x,3)&lt;=0.0，即还没有对位置进行初始化，调用initx函数将流动站的位置rtk-&gt;x[0:2]初始化为rtk结构体中保存的位置rtk-&gt;sol.rr[1:2]，并将协方差矩阵对角线上的方差设置为VAR_POS（60）。如果在opt中将dynamics设置为1，则将速度状态量rtk-&gt;x[3:5]初始化为rtk结构体中保存的速度，将协方差矩阵对角线上的方差设置为VAR_VEL（10），然后将加速度状态量rtk-&gt;x[6:8]初始化1E-6，方差设置为VAR_ACC（10），协方差矩阵中的其他元素全被设置为0；（3）如果选择PMODE_PPP_STATIC模式（静止PPP），将协方差矩阵P中的[i*(1+rtk-&gt;nx)]项加上位置噪声rtk-&gt;opt.prn[5]的平方乘以当前历元和上一时刻的时间差rtk-&gt;tt的绝对值，然后退出函数；\n（4）如果配置中没有将dynamics设置为1，即采用无动力学的实时定位模式，则直接将位置状态量rtk-&gt;x[0:2]设置为rtk结构体中保存的位置rtk-&gt;sol.rr[i]，并且将方差设置为VAR_POS（60），然后退出函数；\n（5）创建一个矩阵ix用来保存有效状态的编号，有效状态应满足rtk-&gt;x[i]!=0.0&amp;&amp;rtk-&gt;P[i+i*rtk-&gt;nx]&gt;0.0，即状态向量的每个元素都不为0且协方差阵P的对角线上的元素不为0，并用nx记录有效状态的数量，有效状态共有9个，依次为：（6）如果有效状态的数量nx小于9，释放内存，然后退出函数；\n（7）定义状态转移矩阵F，过程噪声矩阵P，并将对角线上速度和加速度对应的元素分别设置为时间差和时间差的平方除2，用于后续的卡尔曼滤波，注意rtklib中矩阵是以数组的形式保存的；（8）矩阵运算，完成卡尔曼滤波中的时间更新，$xp=Fx，P=FP*F^T+Q$，即卡尔曼滤波中的先验状态估计值和先验状态误差估计值，并将对应值赋给rtk结构体中的rtk-&gt;x[0:8]和rtk-&gt;P[i+i*rtk-&gt;nx]；\n（9）该过程噪声建模为随机游走噪声。由于在配置中所设的Receiver Accel horiz/vertical是东北天坐标系下的，因此还需要将该过程噪声的方差阵由东北天坐标系转到地球固定坐标系。由此，便完成了位置、速度、加速度状态量以及协方差矩阵的时间更新；\nd.注意事项\n卡尔曼滤波用于不是PMODE_PPP_FIXED，也不是PMODE_PPP_STATIC，并且rtk-&gt;opt.dynamics不等于1的情况；\n rtk-&gt;P[i+jrtk-&gt;nx]=rtk-&gt;P[j+irtk-&gt;nx]=i==j?var:0.0;\n\n4、udclk_pppa.函数功能说明：使用rtk中记录的上一次的状态量对钟差状态量进行时间更新（）\nstatic void udclk_ppp(rtk_t *rtk)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\n\nc.具体功能实现：（1）利用for循环为每个系统的时钟参数进行更新；\n（2）如果opt中选择了EPHOPT_PREC精密星历模式，令dtr=rtk-&gt;sol.dtr[0]，此时忽略了系统间偏差，否则，如果不是GPS系统，还需要加上保存在rtk-&gt;sol中的每个系统各自对应的rtk-&gt;sol.dtr[i]；\n（3）调用initx函数使用CLIGHT*dtr和VAR_CLK对rtk进行初始化；\n（4）此时的状态向量和协方差矩阵为：\n\nIC(i,&amp;rtk-&gt;opt)即(&amp;rtk-&gt;opt)-&gt;dynamics?9:3)+(i))，会判断dynamics是否等于1，即是否采用动态定位，如果不采用，则i=1，此时的状态变量为x[0:3]，如果采用，则状态变量为x[0:8]。\n\nd.注意事项：\n1.这里的时钟参数具体指哪些？\n2.rtx-&gt;x里包括很多状态量，每次调用initx时的索引i都不一样（索引 i 用来给状态变量赋值）\n\n5、udtrop_pppa.函数功能说明：对对流层状态量进行时间更新，如果状态量不如0，直接加入与过程噪声和时间有关的噪声\nstatic void udtrop_ppp(rtk_t *rtk)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\n\nc.具体功能实现：（1）判断对流层对应的状态量是否为0，如果是，调用ecefpos函数将rtk-&gt;sol.rr转换到大地坐标系，然后调用sbstropcorr函数计算星基增强系统提供的对流层延迟改正数，再调用initx函数给rtk-&gt;x中对应的状态量赋值；\n（2）如果opt中选择了TROPOPT_ESTG模式，即ZTD+Grad模型，再调用initx函数给rtk-&gt;x中对应的水平方向上的梯度状态量均赋值为1E-6和rtk-P对角线上的方法赋值VAR_GRA；\n（3）如果对流层对应的状态量不为0，则在将其协方差阵加上过程噪声，如果opt中选择了TROPOPT_ESTG模式，继续加上噪声；\nd.注意事项\n\n在整个对流层状态量的时间更新中，实际上仅对对流层状态量为0的那些卫星进行了初始化，其余卫星的对流层状态量并没有变化，仅仅是在协方差阵中加入了过程噪声；\n协方差矩阵P的具体结构是什么样的，（3）中前后两次加上噪声的位置不一样（其实是一样的）；\ntrop_model_prec函数调用Saasamoinen模型计算天顶对流层干延迟，调用NMF模型计算干湿投影系数，根据方位角高度角计算湿延迟及东、北水平方向上的梯度系数dtdx，根据干延迟投影函数、新计算的湿延迟投影函数及干延迟计算信号传播路径上的对流层延迟。\n\n\n6、udiono_pppa.函数功能说明：对电离层状态量进行时间更新，如果没用初始化，根据伪距观测值比例关系得到电离层延迟，否则直接加入噪声\nstatic void udiono_ppp(rtk_t *rtk, const obsd_t *obs, int n, const nav_t *nav)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\t\t I     const obsd_t *obs\t观测文件\t\t I     int n \t\t\t\t观测文件中数目\t\t I     const nav_t *nav     导航电文\n\nc.具体功能实现：（1）在基线比较长的情况下（&gt;10km）,由于电离层的误差，通过双差不能完全消除，因此需要考虑电离层的影响。在长基线的情况，可以在配置中选择电离层修正方式为IONOOPT_EST。这种配置下，会将垂直方向的单差电离层延迟添加到卡尔曼滤波状态量中；\n（2）如果状态向量中的电离层参数rtk-&gt;x[j]!=0.0并且L1，L2载波相位的中断次数大于GAP_RESION（120），则将该卫星的单差电离层延迟状态量0；\n（3）依次所有观测到的卫星进行处理，如果该卫星的电离层状态量为0，调用sat2freq函数根据卫星编号，code indicator和导航电文获得对应的频率，如果伪距观测值和相位观测值为0，则跳过这颗卫星，之后通过将P1码和P2码的差值除以对应两个频率与基准频率FREQ1的比值的平方差，即(obs[i].P[0]-obs[i].P[1])/(SQR(FREQ1/freq1)-SQR(FREQ1/freq2))，得到消去了其他误差的ion；\n（4）调用ecef2pos函数将rtk-&gt;sol.rr中存储的接收机的位置转换到大地坐标系中，之后调用ionmapf函数，根据高度角，俯仰角以及接收机坐标获得电离层投影系数。该函数首先判断接收机距离地心的高度是否大于电离层的最高范围HION，如果超过，则返回1，否则返回1.0/cos(asin((RE_WGS84+pos[2])/(RE_WGS84+HION)*sin(PI/2.0-azel[1])))；\n（5）将当前这颗卫星的经过投影系数修正的电离层延迟加入到rtk结构体中，并将协方差矩阵中对应的方差设为VAR_IONO（60*60）；\n（6）如果电离层状态量不为0，则在它的协方差阵上添加过程噪声，过程噪声大小由高度角，基线长度，过程噪声参数决定；\nd.问题\n\n在整个电离层状态量的时间更新中，实际上仅对电离层状态量为0的那些卫星进行了初始化，其余卫星的电离层状态量并没有变化，仅仅是在协方差阵中加入了过程噪声。\n\n和对流层更新相比，电流层输入的参数更多，而且除了rtk结构体中的观测文件外，还有一个opt输入。\n\nII(i+1,&amp;rtk-&gt;opt)表示为：\n((&amp;rtk-&gt;opt)-&gt;dynamics?9:3)+(1+1+0+0+0+0+0))+(&amp;rtk-&gt;opt)-&gt;tropopt&lt;3?0(&amp;rtk-&gt;opt)&gt;tropopt==3?1:3))+(+1)-1)\n根据opt中的选项确定电离层参数在状态向量中的位置j\n\ncode indicator是什么？\n\n注意在（3）中不管卫星有没有被观测到，结构体中对应位置的参数都会被初始化，如果当前卫星没有观测值，则被设置为0；\n\n\n\ne.注意\n这里的电离层参数的数目等于观测文件的数目，即等于N，随观测量增加而增加；\n\n7、uddcb_ppp如果使用了L5频率，则对DCB状态量进行时间更新，如果没有初始化，直接设为1E-6\na.函数功能说明：static void uddcb_ppp(rtk_t *rtk)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\n\nc.具体功能实现（1）利用宏定义ID以及opt中的选项得到DCB在rtk中对应的序号；\n（2）如果DCB对应的状态量等于0，即未进行初始化，将1E-6和VAR_DCB（30*30）赋给DCB对应的状态量；\nd.问题\n有一点简单（简单好啊）\n后面迭代可能会继续修正，这里只是作为自定义的初始值使用\n\n8、udbias_pppa.函数功能说明：对整周模糊度状态量进行时间更新，首先检测周跳，如果相位中断计数大于阈值，将所有模糊度设为0，利用无电离层组合或者载波减去伪距的组合得到bias，计算当前历元所有观测值的bias的平均值，然后加入对应状态量，并在P阵中加入噪声。如果出现周跳，将对应状态量直接设为bias。\nstatic void udbias_ppp(rtk_t *rtk, const obsd_t *obs, int n, const nav_t *nav)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\t\t I     const obsd_t *obs\t观测文件\t\t I     int n \t\t\t\t流动站观测值数目\t\t I     const nav_t *nav     导航电文\n\nc.具体功能实现：（1）如果在opt中选择了day-boundary clock jump日界时钟跳变，调用time2gpst函数将第一个观测文件的时间转换到GPST，然后检测时间乘10除以86400的余数是否等于0，即clk_jump等于1或者0；\n（2）将所有系统的所有卫星的周跳标志ssat[i].slip[j]置为0；\n（3）调用detslp_ll函数对所有共视星进行周跳检测：根据LLI（失锁显示器）来判断是否存在周跳，输入为rtk，obs，n；\n（4）调用detslp_gf函数，利用GF无几何组合进行周跳检测，输入为rtk，obs，n，nav；\n（5）调用detslp_mw函数，利用MW组合进行周跳检测，输入为rtk，obs，n，nav，rtk，obs，n，nav；\n（6）调用ecef2pos函数将坐标转换到大地坐标系；\n（7）对所有卫星进行循环，判断是否需要重置单差相位偏移状态量。如果所配置的AR的模式为ARMODE_INST，或者卫星载波相位的中断次数大于配置中所设置的最大次数，或者存在日界时钟跳变，将单差相位偏移状态量重置为0；\n（8）对每一组观测数据，调用corr_meas函数对伪距、载波相位进行修正，包括L1和L2频率的DCB改正，以及对流层，电离层和相位缠绕，然后计算无电离层组合的伪距、载波相位量测量Pc和Lc；\n（9）将每颗卫星的载波相位偏差bias初始化为0；\n（10）如果电离层模型为IONOOPT_IFLC双频消电离层组合模型，则直接计算载波相位模糊度bias[i]=Lc-Pc，并且在两个频率上发生周跳时均将周跳标志slip[]设置为真；\n（10）否则如果有L[f]!=0.0&amp;&amp;P[f]!=0.0，依次调用sat2freq函数根据obs[i].code[]得到freq1和freq2，然后得到电离层延迟ion=(obs[i].P[0]-obs[i].P[l])/(1.0-SQR(freq1/freq2))，即通过将伪距和载波相减，然后加上双倍的电离层延迟来消去几何距离、对流层、电离层、接收机钟差、卫星钟差等误差，得到近似的载波相位模糊度bias；\n（11）由于L1和L2伪距相减后，只剩下L1和L2的电离层误差之差，以及伪距噪声项，所以由此计算出来的电离层误差实际包含了伪距噪声，误差相对较大。利用修正电离层误差后的伪距和载波相位之差计算bias[i]=L[f]-P[f]+2.0*ion*SQR(lam[f]/lam[0]);\n（12）计算每颗卫星的bias与载波相位偏差状态量rtk-&gt;x之间的偏差offset之和；\n（13）如果有效观测数大于2，并且模糊度平均偏差小于阈值，在原有的载波相位偏差状态量上加上offset的平均值，以此来作为载波相位偏差一步预测值rtk-&gt;x[j]+=offset/k；\n（14）对每颗卫星循环，更新一步预测协方差阵，对有周跳的卫星、或者没有初始化载波相位偏差状态量的卫星，用之前计算的bias值重新初始化载波相位偏差状态量。\nd.问题\n\n理论知识跟不上了，有一点复杂（确实）\n\nIB(i+1,f,&amp;rtk-&gt;opt)d 详细内容：\n((((&amp;rtk-&gt;opt)-&gt;dynamics?9:3)+((1+1+0+0+0+0+0))+((&amp;rtk-&gt;opt)-&gt;tropopt)&lt;3?0:((&amp;rtk-&gt;opt)-&gt;tropopt==3?1:3))+\n((&amp;rtk-&gt;opt-&gt;)-&gt;ionopt==4?((32-1+1)+(27-1+1)+0+0+0+0+0+(158-120+1)+0):0)+((&amp;rtk-&gt;opt)-&gt;nf&gt;=3?1:0))+((32-1+1)+(27-1+1)+0+0+0+0+0+(158-120+1)+0)*(f)+(i+1)-1)\n首先根据定位模式确定接收机的空间状态量的数目，如果选择静态模式，则为 4 ，如果选择动态模式，则为9；\n然后再加上钟差和对流层的准天顶延迟，即(1+1+0+0+0+0+0)，再判断对流层改正是否选择了梯度改正，如果没有，则+0，如果选择了，再根据选择的梯度改正的模式+3或者+1，即计算3个方向上的梯度或者1个方向上的梯度；\n接着根据电离层改正的选项(&amp;rtk-&gt;opt-&gt;)-&gt;ionopt是否等于4来选择电离层的参数，如果等于4，则为每颗卫星，不管有没有观测到，都设置一个电离层延迟参数，(27-1+1)+0+0+0+0+0+(158-120+1)+0)表示所有系统的卫星数，并且每颗卫星都有一个自己的编号，如果不等于4，则不需要设置电离层参数；\n然后根据((&amp;rtk-&gt;opt)-&gt;nf&gt;=3?1:0))判断是否选择了三频L5观测，如果是，则为L5增加一个DCB参数，可能是因为L1和L2的DCB可以通过导航电文改正；\n最后再根据 ((&amp;rtk-&gt;opt-&gt;)-&gt;ionopt==3?1:(&amp;rtk-&gt;opt)-&gt;nf) 确定 f 的值，乘以卫星数，可能对应于整周模糊度，即每颗卫星的每个频率都需要一个模糊度参数；\n\n\n\n9、testeclipsea.函数功能说明：如果在配置中选择排除block IIA卫星，将这些卫星的位置、速度置0\nstatic void testeclipse(const obsd_t *obs, int n, const nav_t *nav, double *rs)\n\nb.函数参数说明：args: \t\t I     const obsd_t *obs\t观测文件\t\t\t I     int n \t\t\t\t流动站观测值数目\t\t\t I     const nav_t *nav     导航电文\t\t\t IO\t   double *rs\t\t\t卫星位置和速度\n\nc.具体功能实现（1）调用sunmoonpos函数获得太阳和月亮在ECEF中的坐标，并分别赋给rsun和erpv；\n（2）调用normv3函数获得标准化的太阳坐标；\n（3）如果卫星与地心的距离小于0，跳过当前观测值；\n（4）计算卫星和太阳之间的余弦值cosa，并为其设置上界和下界1和-1，然后调用反三角函数计算夹角ang；\n（5）如果角度小于90或者卫星到原点的距离乘以ang的正弦值大于地球半径，跳过这颗卫星，即不排除；\n（6）否则将卫星的三维坐标全设置为0；\nd.问题\n理论知识跟不上了（现在可以了）\n\n10、tidedispa.函数功能说明：计算地球潮汐改正，包括固体潮汐、海洋潮汐、极潮\nstatic void testeclipse(const obsd_t *obs, int n, const nav_t *nav, double *rs)\n\na.函数参数说明：* displacements by earth tides* args   : gtime_t tutc     I   time in utc*          double *rr       I   site position (ecef) (m)*          int    opt       I   options (or of the followings)*                                 1: solid earth tide*                                 2: ocean tide loading*                                 4: pole tide*                                 8: elimate permanent deformation*          double *erp      I   earth rotation parameters (NULL: not used)*          double *odisp    I   ocean loading parameters  (NULL: not used)*                                 odisp[0+i*6]: consituent i amplitude radial(m)*                                 odisp[1+i*6]: consituent i amplitude west  (m)*                                 odisp[2+i*6]: consituent i amplitude south (m)*                                 odisp[3+i*6]: consituent i phase radial  (deg)*                                 odisp[4+i*6]: consituent i phase west    (deg)*                                 odisp[5+i*6]: consituent i phase south   (deg)*                                (i=0:M2,1:S2,2:N2,3:K2,4:K1,5:O1,6:P1,7:Q1,*                                   8:Mf,9:Mm,10:Ssa)*          double *dr       O   displacement by earth tides (ecef) (m)* return : none* notes  : see ref [1], [2] chap 7*          see ref [4] 5.2.1, 5.2.2, 5.2.3*          ver.2.4.0 does not use ocean loading and pole tide corrections\n\nb.具体功能实现：（1）\nc.问题\n理论知识又跟不上了\n\n11、ppp_res计算载波相位和伪距残差，组装观测矩阵，记录最大残差\na.函数功能说明：nv=ppp_res(0,obs,n,rs,dts,var,svh,dr,exc,nav,xp,rtk,v,H,R,azel\n\nppp_res(i+1,obs,n,rs,dts,var,svh,dr,exc,nav,xp,rtk,v,H,R,azel)\n\nstatic int ppp_res(int post, const obsd_t *obs, int n, const double *rs,                   const double *dts, const double *var_rs, const int *svh,                   const double *dr, int *exc, const nav_t *nav,                   const double *x, rtk_t *rtk, double *v, double *H, double *R,                   double *azel)\n\nb.函数参数说明：args: \t\t I     int post             是否是修正后残差计算标志\t\t\t I     const obsd_t *obs\t当前历元观测值\t\t\t I     int n                当前流动站观测值数目            I     const double *dts    卫星钟差            I     const double *var_rs 卫星位置和钟差的协方差            I     const int *svh       卫星健康标志            I     const double *dr     地球潮汐位移            IO    int *exc             卫星排除标志\t\t I     const nav_t *nav \t星历\t\t IO    const double *x      状态量\t\t IO    rtk_t *rtk\t\t\trtk solution structure\t\t IO    double *v            载波相位和伪距残差\t\t IO    double *H            卡尔曼滤波中的观测矩阵\t\t IO    double *R            测量误差的协方差矩阵\t\t I     const double *azel \t方位角和俯仰角 (rad) \n\nc.具体功能实现（1）定义一系列变量，观测值y，站星距离r，接收机钟差cdtr，载波相位模糊度bias，电离层系数C，接收机坐标rr{x,y,z}[3]，接收机坐标pos[3]{lat,lon,h}，站星向量e[3]，对流层梯度系数dtdx，L[NFREQ]，P[NFREQ]，无电离层组合载波观测值Lc，无电离层组合伪距观测值Pc，观测值对应的噪声var[MAXOBS*2]，对流层改正dtrp，电离层改正dion，对流层噪声vart，电离层噪声vari，L5频点dcb，freq，dantr[NFREQ]，dants[NFREQ]，ve[MAXOBS*2*NFREQ]，存储最大残差vmax；\n（2）把所有卫星的有效标志位均置0，rtk-&gt;ssat[i].vsat[j]=0；\n（3）将地球潮汐改正dr加入先验预测值x[i]；\n（4）将先验坐标rr[0:3]{x,y,z}转换到大地坐标系中pos[0:3]{lat,lon,h}；\n（5）按照卫星编号依次处理所有观测数据，首先获得当前观测值的卫星编号sat=obs[i].sat，然后调用geodist函数计算经过地球自转改正的站星距离，调用satazel函数计算高度角和方位角，如果站星距离小于0或者高度角低于设定的阈值，则将卫星排除标准exc设置为1，然后跳过这颗卫星；\n（6）调用satsys函数获取卫星的系统，判断单点定位卫星有效标志，然后调用satexclude函数检测卫星是否在设置中被排除，如果不符合以上情况，则将卫星排除标准exc设置为1，然后跳过这颗卫星；\n（7）调用model_trop函数根据观测时间obs[i].time，接收机位置pos，高度角和方位角azel，以及x和dtdx计算对流层误差dtrp，其中x和dtdx会在ZTD Estimation和ZTD Grad Estimation中使用，如果返回结果为0，跳过这颗卫星；\n（8）调用model_iono函数根据观测时间obs[i].time，接收机位置pos，高度角和方位角azel，以及x和dtdx计算电离层误差dion，其中x会在IONOOPT_EST模式中使用，如果返回结果为0，跳过这颗卫星；\n（9）调用satantpcv函数根据卫星位置rs，接收机位置rr和导航电文nav-&gt;pcvs计算卫星天线相位中心变化dants，调用antmodel函数计算根据接收机天线的相位中心参数计算接收机天线偏移量dantr；\n（10）调用model_phw函数计算相位缠绕改正值rtk-&gt;ssat[sat-1].phw；\n（11）调用corr_meas函数对伪距和载波相位观测值进行修正，包括天线相位中心改正，相位缠绕改正以及使用双频组合消除电离层延迟的Lc和Pc，如果是GPS或者GLONASS，还包括码差分偏差改正；\n（12）依次处理每种频率上的伪距观测值和载波观测值，首先将dcb和bias设置为0，如果选择双频无电离层组合，判断((y=j%2==0?Lc:Pc)==0.0)，即j为奇数时y=Pc，j为偶数时y=Lc，如果伪距或者载波组合观测值等于0，跳过这个频率，如果不是双频无电离层组合，检验伪距观测值和载波观测值是否等于0，如果是，跳过这个频率，之后根据位置pos，高度角和方位角azel得到电离层系数C=SQR(FREQ1/freq)*ionmapf(pos,azel+i*2)*(j%2==0?-1.0:1.0)，如果是载波，则*-1；\n（13）for (k=0;k&lt;nx;k++) H[k+nx*nv]=k&lt;3?-e[k]:0.0;，组装设计矩阵H，nx表示状态量的数目，nv表示观测值总数，即nx表示行数，nv表示列数，其中设计矩阵前三行为-e[k]，表示坐标向量，其余为0，\n（14）根据系统类型选择k的值，并根据k设置接收机钟差参数，将H中对应位置设为1；\n（15）如果选择了TROPOPT_EST或者TROPOPT_ESTG，将dtdx放入H中对应位置，这里使用IT(opt)找到对流层参数的位置，可能是由于rtklib中只实现了gps和glonass，这里的钟差参数为1+1+0+0+0+0，后面的0表示其他系统的钟差参数，因为没有使用，所有默认为0；\n（16）如果选择了IONOOPT_EST，即将电离层延迟作为参数估计，首先判断x中电离层参数是否等于0，然后将前面计算的电离层参数赋给H中对应的位置；\n（17）如果选择了L5频率，则令dcb等于x中对应位置的参数。并将H中对应位置设置为1;\n（18）在载波观测值中，如果x中对应的bias参数不等于0，将H中对应位置设为1；\n（19）计算当前频率对应的观测值，y中存储的是伪距观测值或者载波观测值，cdtr是乘以光速的钟差，dts是卫星钟差，v[nv]=y-(r+cdtr-CLIGHT*dts[i*2]+dtrp+C*dion+dcb+bias)；\n（20）根据j的奇偶性选择将v[nv]存储在 rtk-&gt;ssat[sat-1]中的位置；\n（21）调用varerr函数计算伪距（载波相位）噪声，以及对流层噪声vart、电离层噪声vari、卫星位置钟差噪声var_rs后，作为量测噪声赋给var[nv]，如果是GLONASS卫星，还要加上IFB噪声（频间差噪声）；\n（22）如果是pre-fit，即!post的情况，如果某颗卫星的残差值大于阈值，设置exc[i]=1，即将这颗星排除，并在rtk-&gt;ssat[sat-1].rejc[j%2]++中将reject counter加1；\n（23）如果设置post==1，并且残差v[nv]的绝对值大于阈值，在obsi中记录这颗卫星的编号，在freq中记录当前频率编号，在ve中记录当前的残差；\n（24）如果当前是载波相位观测值，将rtk-&gt;ssat[sat-1].vsat[j/2]的有效标志设为1；\n（25）每颗卫星的循环都进行比较，将残差最大值赋给vmax，并将对应的观测值和频率赋给maxobs和maxfrq，以及rej；\n（26）对观测噪声矩阵R赋值，依次将对角线上的元素设为var[i]，其他元素为0；\n（27）根据post标志选择返回函数状态stat或者当前观测值序号nv；\nd.问题\n理论知识跟不上了（跟上了也没用）\n\ne.涉及到的矩阵的详细内容以下H矩阵为使用\n\nf.注意：可以看到在观测矩阵H中接收机钟差、dcb、bias是线性相关的。\n\n104-RTKLIB中PPP设计_ppp-kine-CSDN博客\n\n并不是模糊度有164个，而是为82颗卫星的模糊度预留了位置，实际估计的就是圈出来的模糊度参数。\n\n12.update_stata.函数功能说明：更新状态，将rtk中的结果更新到rtk-&gt;sol中\nstatic void update_stat(rtk_t *rtk, const obsd_t *obs, int n, int stat)\n\nb.函数参数说明：args: \t\t IO    rtk_t *rtk\t\t\trtk solution structure\t\t\t I     const obsd_t *obs\t当前历元观测值\t\t\t I     int n                当前流动站观测值数目            I     int stat             解算模式\n\nc.具体功能实现13.一些宏定义和注意事项1.状态矩阵参数量和序号频率数，选择无电离层组合为则为1个频率，nf:(1:L1,2:L1+L2,3:L1+L2+L5)#define NF(opt)     ((opt)-&gt;ionoopt==IONOOPT_IFLC?1:(opt)-&gt;nf)静态（3）或者动态（9）模式#define NP(opt)     ((opt)-&gt;dynamics?9:3)所有系统的卫星数量#define NC(opt)     (NSYS)对流层待估参数，不将对流层延迟作为待估参数则为0，之后根据是否选择梯度分量设置1或者3个参数#define NT(opt)     ((opt)-&gt;tropopt&lt;TROPOPT_EST?0:((opt)-&gt;tropopt==TROPOPT_EST?1:3))电离层待估参数，如果选择估计，则为每颗卫星设置一个待估参数，同一颗卫星的其他频率可以通过比例系数得到该频率对应的电离层延迟#define NI(opt)     ((opt)-&gt;ionoopt==IONOOPT_EST?MAXSAT:0)DCB待估参数，如果选择L5频率，由于导航电文中没用L5频率的DCB产品，需要设置1个待估参数#define ND(opt)     ((opt)-&gt;nf&gt;=3?1:0)#define NR(opt)     (NP(opt)+NC(opt)+NT(opt)+NI(opt)+ND(opt))#define NB(opt)     (NF(opt)*MAXSAT)#define NX(opt)     (NR(opt)+NB(opt))钟差和钟漂在状态矩阵中的位置#define IC(s,opt)   (NP(opt)+(s))对流层待估参数在状态矩阵中的位置#define IT(opt)     (NP(opt)+NC(opt))电离层待估参数在状态矩阵中的位置#define II(s,opt)   (NP(opt)+NC(opt)+NT(opt)+(s)-1)DCB待估参数在状态矩阵中的位置#define ID(opt)     (NP(opt)+NC(opt)+NT(opt)+NI(opt))频间偏差待估参数在状态矩阵中的位置#define IB(s,f,opt) (NR(opt)+MAXSAT*(f)+(s)-1)\n\n2.IB的详细内容：\n按照参数最多的情况，即动态定位、ZTD+Grad Estimation模式估计对流层延迟、将电离层延迟作为待估参数、使用L5频率时的DCB，每个频率的整周模糊度，则具体的参数为：.\n观测矩阵H的行数为nv，在ppp_res中每个观测值的每个频率中都会执行一次nv++，观测矩阵H的列数为状态量的数目，按前面介绍的照参数最多的情况，依次为9+2+3+MAXOBS+1+MAXOBS*f，由于不是每颗卫星都能被观测到，矩阵中存在很多全为0的空行。\n3.一些疑惑的地方\n\n在观测矩阵中，接收机钟差和L5频率的DCB的系数均为1，存在线性相关，而且电离层比例系数C是根据接收机的位置和高度角计算的，前后两次历元的比例系数可能相差很小。\n\n注意上面的dcb不是一整列都是1，因为有的频率没有用到L5频率，所以又不是相关的。\n\n这里的相关指的更多的还是伪距硬件延迟和载波硬件延迟被吸收到接收机钟差，卫星钟差和载波相位模糊度等里面。\n\n\n\n4.注意事项\n\nppp定位流程主要为，首先调用update_ppp对状态量进行时间更新，根据上一时刻的状态量进行先验估计，然后调用ppp_res根据当前观测值计算残差矩阵和观测矩阵，再调用filter对状态量进行状态更新，之后再调用ppp_res，根据修正过的状态量计算新的残差矩阵。\n\n\n四、参考博客RTKlib PPP代码解析-CSDN博客\n[我的rtklib不完全学习笔记之十] 精密单点定位ppp.c - 知乎\nrtklib代码详解——ppp.c - 博客园—哆啦A梦 - 博客园\n（二）使用RTKLIB进行PPP数据解算的流程_rtklib解算结果空文档-CSDN博客\n用RTKLIB中的rtkpost进行ppp和spp定位（附用CUI rnx2rtkp 编译）_rtklib能够实现spp吗-CSDN博客\nRTKLIB学习（一）–spp代码分析-CSDN博客\nRTKLIB学习（二）–2、PPP代码分析_rtklib ppp-CSDN博客\n","categories":["GNSS"],"tags":["GNSS","Rtklib"]},{"title":"Hello World","url":"/2022/10/02/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new \"My New Post\"\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","categories":["阅读笔记"],"tags":["新玩具"]},{"title":"Pytorch 非平衡数据集的处理","url":"/2022/04/05/pytorch%E9%9D%9E%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%84%E7%90%86/","content":"非平衡数据集“非平衡数据集指的是数据集中各个类别的样本数目相差巨大。非平衡数据集会导致训练的模型有偏差。”\n非平衡数据集的常见处理方法：\n\n图片来自:独家 | 一文教你如何处理不平衡数据集 - 知乎 (zhihu.com)\n关于每种采样的具体介绍请自行搜索，因为我也不清楚。\n我遇到的情况如下图所示，每种类别的数据的数量从 20 左右一直到 200，相差了将近 10 倍。同时，因为这里的数据集涉及到混合数据，所以不能简单地通过 RandomOverSample 等函数进行过采样或者欠采样之类的操作。当然，我觉得更大的可能是因为我并不熟悉这些函数的操作，所以最后只能自己通过 collections 包中 Count 函数对数据进行一种“类过采样”。\n\n具体的代码如下图所示。因为需要计数的数据是浮点数，而且跨度很大，所以我首先对数据进行整除使得可以直接使用Count进行计数。\n\n\n之后根据 count 得到的每个区间的数据的数量大小，对数据相应的数据进行“过采样”。\ntest_latency = np.array(latency).copy()//16000count = Counter(test_latency)count = sorted(count.items(),key=lambda d: d[0], reverse=False)print(count)total_params = sorted(total_params,key=lambda cus:cus[2],reverse=False)#过采样数据for i in range(len(total_params)):    for j in range(len(count)):        if count[8][0] * 16000 &lt; total_params[i][2] &lt; (count[9][0]) * 16000:            continue        if count[13][0] * 16000 &lt; total_params[i][2] &lt; (count[14][0]) * 16000:            continue        if count[j][0]*16000 &lt;total_params[i][2] &lt; (count[j][0]+1)*16000:            for t in range(int(300//count[j][1])):                total_params.append(total_params[i])\n\n最后得到的数据分布如下，其实最终的结果仍然不太理想。\n\n","categories":["Debug"],"tags":["Pytorch","深度学习"]},{"title":"《动手学Pytorch》阅读笔记","url":"/2022/10/02/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6Pytorch%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"动手学Pytorch阅读笔记4.1 nn.Module\nModule能够自动检测到自己的parameter，并将其作为学习参数。除了parameter，Module还包含子Module，主Module能够递归查找子Module中的parameter。\n\nclass Perceptron(nn.Module):    def __init__(self,in_features,hidden_features,out_features):        nn.Module.__init__(self)        self.layer1 = Linear(in_features,hidden_features)    # 此处的Linear是前面自定义的全连接层        self.layer2 = Linear(hidden_features,out_features)            def forward(self,x):        x = self.layer1(x)        x = t.sigmoid(x)        x = self.layer2(x)        return x\n\nperceptron = Perceptron(3,4,1)for name,param in perceptron.named_parameters():    print(name,param.size())\n\nlayer1.w torch.Size([3, 4])layer1.b torch.Size([4])layer2.w torch.Size([4, 1])layer2.b torch.Size([1])\n\n\n构造函数 __init__ 中，可利用前面自定义的Linear层（Module）作为当前 Module 对象的一个子 Module ，它的可学习参数，也会成为当前Module的可学习参数。在前向传播函数中，我们有意识地将输出变量都命名为 x ，是为了能让 Python 回收一些中间层的输出，从而节省内存。但并不是所有的中间结果都会被回收，有些 variable 虽然名字被覆盖，但其在反向传播时仍需要用到，此时 Python 的内存回收模块将通过检查引用计数，不会回收这一部分内存。\n\n自定义层 Linear 必须继承 nn.Module ，并且在其构造函数中需调用 nn.Module 的构造函数，即 super(Linear,self).init() 或nn.Module.init(self)。\n\n在构造函数 __init__ 中必须自己定义可学习的参数，并封装成 Parameter，Parameter 是一种特殊的Variable，但其默认需要求导（requires_grad=True），感兴趣的读者可以通过 nn.Parameter?? 查看 Parameter 类的源代码。\n\nforward函数实现前向传播过程，其输入可以是一个或多个variable，对x的任何操作也必须是variable支持的操作。\n\n无须写反向传播函数，因其前向传播都是对variable进行操作，nn.Module能够利用autograd自动实现反向传播，这一点比Function简单许多。\n\n使用时，直观上可将 net 看成数学概念中的函数，调用 net(x) 即可得到 x 对应的结果。它等价于 net.call(x) ，在 __call__ 函数中，主要调用的是 net.forward(x) ，另外还对钩子做了一些处理。所以在实际使用中应尽量使用 net(x) 而不是使用net.forward(x)。\n\nModule 中的可学习参数可以通过 named_parameters() 或者 parameters() 返回迭代器，前者会给每个 parameter 附上名字，使其更具有辨识度。\n\n阅读 nn.Module 的其他 layers 的文档时应主要关注以下几点：\n构造函数的参数，如 nn.Linear(in_features,out_features,bias)，需关注这三个参数的作用。属性、可学习参数和子 Module。如 nn.Linear 中有 weight 和 bias 两个可学习参数，不包含子 Module。输入输出的形状，如 nn.Linear 的输入形状是（N，input_features），输出形状为（N，output_features），N是batch_size。\n\n这些自定义 layer 对输入形状都有假设：输入的不是单个数据，而是一个 batch 。若想输入一个数据，必须调用 unsqueeze(0) 函数将数据伪装成 batch_size=1的batch。\n\n\n4.2 常用的神经网络层\nReLU 函数有个 inplace 参数，如果设为 True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算 ReLU 的反向传播时，只需根据输出就能推算出反向传播的梯度。但是只有少数的 autograd 操作支持 inplace操作（如variable.sigmoid_()），除非你明确地知道自己在做什么，否则一般不要使用 inplace 操作。\n\n# Sequential的三种写法\nnet1 = nn.Sequential()\nnet1.add_module('conv', nn.Conv2d(3, 3, 3))\nnet1.add_module('batchnorm', nn.BatchNorm2d(3))\nnet1.add_module('activation_layer', nn.ReLU())\n\nnet2 = nn.Sequential(\n        nn.Conv2d(3, 3, 3),\n        nn.BatchNorm2d(3),\n        nn.ReLU()\n        )\n\nfrom collections import OrderedDict\nnet3= nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(3, 3, 3)),\n          ('bn1', nn.BatchNorm2d(3)),\n          ('relu1', nn.ReLU())\n        ]))\nprint('net1:', net1)\nprint('net2:', net2)\nprint('net3:', net3)\n\nnet1: Sequential(\n  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (activation_layer): ReLU()\n)\nnet2: Sequential(\n  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU()\n)\nnet3: Sequential(\n  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu1): ReLU()\n)\n- ModuleList是 Module 的子类，当在 Module 中使用它时，就能自动识别为子 Module，如果使用 list保存网络，就不能被识别到。### 4.3 优化器为不同子网络设置不同的学习率，在finetune中经常用到如果对某个参数不指定学习率，就使用最外层的默认学习率\n\n\n\n首先定义一个LeNet网络class Net(nn.Module):    def init(self):        super(Net, self).init()        self.features = nn.Sequential(                    nn.Conv2d(3, 6, 5),                    nn.ReLU(),                    nn.MaxPool2d(2,2),                    nn.Conv2d(6, 16, 5),                    nn.ReLU(),                    nn.MaxPool2d(2,2)        )        self.classifier = nn.Sequential(            nn.Linear(16 * 5 * 5, 120),            nn.ReLU(),            nn.Linear(120, 84),            nn.ReLU(),            nn.Linear(84, 10)        )\ndef forward(self, x):\n    x = self.features(x)\n    x = x.view(-1, 16 * 5 * 5)\n    x = self.classifier(x)\n    return x\n\nnet = Net()\n\noptimizer =optim.SGD([                {‘params’: net.features.parameters()}, # 学习率为1e-5                {‘params’: net.classifier.parameters(), ‘lr’: 1e-2}            ], lr=1e-5)optimizer\n\nSGD (Parameter Group 0    dampening: 0    lr: 1e-05    momentum: 0    nesterov: False    weight_decay: 0\nParameter Group 1    dampening: 0    lr: 0.01    momentum: 0    nesterov: False    weight_decay: 0)\n### 4.4 nn.functionalnn中还有一个很常用的模块：nn.functional。nn中的大多数layer在functional中都有一个与之对应的函数。nn.functional中的函数和nn.Module主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class Layer(nn.Module)定义，会自动提取科学系参数；而nn.functional中的函数更像是纯函数，由def function(input)定义。### 4.5 nn.Module深入分析\ndef init(self):    self._parameters = OrderedDict()    self._modules = OrderedDict()    self._buffers = OrderedDict()    self._backward_hooks = OrderedDict()    self._forward_hooks = OrderedDict()    self.training = True\n其中每个属性的解释如下：- \\_parameters：字典，保存用户直接设置的 parameter，self.param1 = nn.Parameter(t.randn(3, 3)) 会被检测到，在字典中加入一个 key 为 ’param’，value 为对应 parameter 的 item。而 self.submodule = nn.Linear(3, 4) 中的 parameter 则不会存于此。- \\_modules：子 module，通过 self.submodel = nn.Linear(3, 4) 指定的子 module 会保存于此。- \\_buffers：缓存，如 batchnorm 使用 momentum 机制，每次前向传播需用到上一次前向传播的结果。- \\_backward_hooks 与 \\_forward_hooks：钩子技术，用来提取中间变量，类似 variable 的 hook。- training：BatchNorm 与 Dropout 层在训练阶段和测试阶段中采取的策略不同，通过判断 training 值来决定前向传播策略。- 上述几个属性中，\\_parameters、\\_modules 和 \\_buffers 这三个字典中的键值，都可以通过 self.key 方式获得，效果等价于self._parameters[‘key’].nn.Module 在实际使用中可能层层嵌套，一个 module 包含若干个子 module，每一个子 module 又包含了更多的子 module。为方便用户访问各个子 module，nn.Module 实现了很多方法，如函数 children 可以查看直接子 module，函数 module 可以查看所有的子module（包括当前module）。与之相对应的还有函数 named_childen 和 named_modules，其能够在返回 module 列表的同时返回它们的名字。对于batchnorm、dropout、instancenorm等在训练和测试阶段行为差距巨大的层，如果在测试时不将其training值设为True，则可能会有很大影响，这在实际使用中要千万注意。虽然可通过直接设置training属性，来将子module设为train和eval模式，但这种方式较为繁琐，因如果一个模型具有多个dropout层，就需要为每个dropout层指定training属性。更为推荐的做法是调用model.train()函数，它会将当前module及其子module中的所有training属性都设为True，相应的，model.eval()函数会把training属性都设为False。在PyTorch中保存模型十分简单，所有的Module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用model.load_state_dict()函数将状态加载进来。优化器（optimizer）也有类似的机制，不过一般并不需要保存优化器的运行状态。### 3.1 Tensor- 需要注意的是，t.Tensor(*sizes ) 创建 tensor，系统不会马上分配空间，只会计算剩余的内存是否足够使用，使用到 tenso r时才会分配，而其他操作都是在创建完 tensor 后马上进行空间分配。- 通过 tensor.view 方法可以调整 tensor 的形状，但必须保证调整前后元素总数一致。view 不会修改自身的数据，返回的新 tensor 与源 tensor 共享内存，即更改其中一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时 squeeze 和unsqueeze 两个函数就派上了用场。- resize 是另一种可用来调整 size 的方法，但与 view 不同，它可以修改 tensor 的尺寸。如果新尺寸超过了原尺寸，会自动分配新的内存空间，而如果新尺寸小于原尺寸，则之前的数据依旧会被保存。- ```  In :  a = t.randn(3,4)        a  Out :  tensor([[-0.3973,  0.0700, -0.5916, -0.4848],                 [ 0.5232,  0.8094,  2.4726,  1.4351],                 [-0.0845,  2.0454, -0.0363, -0.8606]])    In :  a[0]    # 第0行（下标从0开始）  Out :  tensor([-0.3973,  0.0700, -0.5916, -0.4848])    In :  a[:,0]    # 第0列  Out :  tensor([-0.3973,  0.5232, -0.0845])    In :  a[0][2]    # 第0行第2个元素，等价于a[0,2]  Out :  tensor(-0.5916)    In :  a[0,-1]    # 第0行最后一个元素  Out :  tensor(-0.4848)    In :  a[:2]    # 前两行  Out :  tensor([[-0.3973,  0.0700, -0.5916, -0.4848],                 [ 0.5232,  0.8094,  2.4726,  1.4351]])    In :  a[:2,0:2]    # 前两行，前两列  Out :  tensor([[-0.3973,  0.0700],                 [ 0.5232,  0.8094]])    In :  print(a[0:1,:2])    # 第0行，前两列        print(a[0,:2])    # 注意两者的区别，形状不同  Out :  tensor([[-0.3973,  0.0700]])         tensor([-0.3973,  0.0700])    In :  a &gt; 1  Out :  tensor([[False, False, False, False],                 [False, False,  True,  True],                 [False,  True, False, False]])    In :  a[a&gt;1]    # 等价于a.maked_select(a&gt;1)，选择结果与原tensor不共享内存空间  Out :  tensor([2.4726, 1.4351, 2.0454])    In :  a[t.LongTensor([0,1])]    # 第0行和第1行  Out :  tensor([[-0.3973,  0.0700, -0.5916, -0.4848],                 [ 0.5232,  0.8094,  2.4726,  1.4351]])\n\n\nTensor有不同的数据类型，如下表所示，每种类型分别对应有CPU和GPU版本（HalfTensor除外）。默认的Tensor是FloatTensor，可通过t.set_default_tensor_type修改默认Tensor的类型（如果默认为GPU Tensor，则所有操作都将在GPU上进行）。\n\n归并操作\n此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法sum，既可以计算整个Tensor的和，也可以计算Tensor中 每一行或每一列的和。常用的归并操作如下所示。\n\n假设输入的形状是（m,n,k）：\n如果指定dim=0，输出的形状就是（1,n,k）或者（n,k）如果指定dim=1，输出的形状就是（m,1,k）或者（m,k）如果指定dim=2，输出的形状就是（m,n,1）或者（m,n）size中是否有“1”，取决于参数keepdim，keepdim=True会保留维度1，从PyTorch 0.2版本起，keepdim默认为False。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如cumsum。\n\n线性代数\nPyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如下所示。\n\n\n\n具体使用说明请参考官方文档，需要注意的是，矩阵的转置会导致存储空间不连续，需要调用它的.contiguous方法将其转为连续。\nIn :  b = a.t()      b.is_contiguous()Out :  FalseIn :  b.contiguous()Out :  tensor([[ 0.,  9.],               [ 3., 12.],               [ 6., 15.]])\n\n\nTensor的数据结构如下图所示。Tensor分为头信息区（Tensor）和存储区（Storage），信息区主要保存着Tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用取决于Tensor中元素的数目，即存储区的大小。\n\n向量化计算是一种特殊的并行计算方式，一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批命令，或者说把指令应用于一个数组或向量上。向量化可极大地提高科学运算的效率，Python本身是一门高级语言，使用很方便，但许多操作很低效，尤其是for循环。在科学计算程序中应当极力避免使用Python原生的for循环，尽量使用向量化的数值计算。\n\n3.2 autogradPyTorch在autograd模块中实现了计算图的相关功能，autograd中的核心数据结构是Variable。Variable封装了tensor，并记录对tensor的操作记录用来构建计算图。Variable的数据结构如下图所示，主要包括三个属性。\ndata：保存variable所包含的tensor。grad：保存data对应的梯度，grad也是variable，而非tensor，它与data形状一致。grad_fn：指向一个Function，记录variable的操作历史，即它是什么操作的输出，用来构建计算图。如果某一个变量是由用户创建的，则它为叶子节点，对应的grad_fn等于None。\n\n\n在PyTorch中计算图的特点可总结如下：\n\nautograd根据用户对variable的操作构建其计算图。对变量的操作抽象为Function。\n\n对于那些不是任何函数(Function)的输出，由用户创建的节点称为叶子节点，叶子节点的grad_fn为None。叶子节点中需要求导的variable，具有AccumulateGrad标识，因其梯度是累加的。\n\nvariable默认是不需要求导的，即requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。\n\nvariable的volatile属性已经废除了。\n\n多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。\n\n非叶子节点的梯度计算完之后即被清空，可以使用autograd.grad或hook技术获取非叶子节点的值。\n\nvariable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。\n\n反向传播函数backward的参数grad_variables可以看成链式求导的中间结果，如果是标量，可以省略，默认为1。\n\nPyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。\n\n\n3.3 扩展autogradIn :  from torch.autograd import Function      class Mul(Function):          @staticmethod          def forward(ctx, w, x, b, x_requires_grad = True):              ctx.x_requires_grad = x_requires_grad              ctx.save_for_backward(w,x)              output = w * x + b              return output           @staticmethod           def backward(ctx, grad_output):               w,x = ctx.saved_tensors               grad_w = grad_output * x               if ctx.x_requires_grad:                   grad_x = grad_output * w               else:                   grad_x = None               grad_b = grad_output * 1               return grad_w, grad_x, grad_b, None\n\n对以上代码的分析如下：\n\n自定义的Function需要继承autograd.Function，没有构造函数__init__，forward和backward函数都是静态方法。\n\nforward函数的输入和输出都是tensor，backward函数的输入和输出都是variable。\n\nbackward函数的输出和forward函数的输入对应，backward函数的输入和forward函数的输出对应。\n\nbackward函数的grad_output参数即torch.autograd.backward中的grad_variables。\n\n如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可。\n\n反向传播可能需要利用前向传播的某些中间结果，在前向传播过程中，需要保存中间结果，否则前向传播结束后这些对象即被释放。\n\n\n5.1数据处理（1）数据加载\n在PyTorch中，数据加载可通过自定义的数据集对象实现。数据集对象被抽象为Dataset，实现自定义的数据集需要继承Dataset，并实现两个Python魔法方法。\n\ngetitem：返回一条数据或一个样本。obj[index]等价于obj.getitem(index)。\n\nlen：返回样本的数量。len(obj)等价于obj.len()。\n\n\nimport torch as tfrom torch.utils import dataimport osfrom PIL import  Imageimport numpy as npclass DogCat(data.Dataset):    def __init__(self, root):        imgs = os.listdir(root)        # 所有图片的绝对路径        # 这里不实际加载图片，只是指定路径，当调用__getitem__时才会真正读图片        self.imgs = [os.path.join(root, img) for img in imgs]            def __getitem__(self, index):        img_path = self.imgs[index]        # dog-&gt;1， cat-&gt;0        label = 1 if 'dog' in img_path.split('/')[-1] else 0        pil_img = Image.open(img_path)        array = np.asarray(pil_img)        data = t.from_numpy(array)        return data, label        def __len__(self):        return len(self.imgs)dataset = DogCat('./data/dogcat/')img, label = dataset[0] # 相当于调用dataset.__getitem__(0)for img, label in dataset:    print(img.size(), img.float().mean(), label)\n\n（2）数据预处理\ntorchvision 是一个视觉工具包，提供了很多视觉图像处理的工具，其中transforms模块提供了对PIL Image对象和Tensor对象的常用操作。\n对PIL Image的操作包括：\n\nScale：调整图片尺寸，长宽比保持不变\nCenterCrop、RandomCrop、RandomResizedCrop： 裁剪图片\nPad：填充\nToTensor：将 PIL Image 对象转成 Tensor，会自动将 [0, 255] 归一化至 [0, 1]\n\n对Tensor的操作包括：\n\nNormalize：标准化，即减均值，除以标准差\nToPILImage：将 Tensor 转为 PIL Image 对象\n\n如果要对图片进行多个操作，可通过Compose函数将这些操作拼接起来，类似于nn.Sequential。注意，这些操作定义后是以函数的形式存在，真正使用时需调用它的__call__方法，这点类似于nn.Module。\n（3）数据加载\nDataset只负责数据的抽象，一次调用__getitem__只返回一个样本。前面提到过，在训练神经网络时，最好是对一个batch的数据进行操作，同时还需要对数据进行shuffle和并行加速等。对此，PyTorch提供了DataLoader实现这些功能。Dataloader 是一个可迭代的对象。\nDataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False)dataiter = iter(dataloader)batch_datas, batch_labesl = next(dataiter)\n\n\ndataset：加载的数据集(Dataset对象)\n\nbatch_size：batch size\n\nshuffle:：是否将数据打乱\n\nsampler： 样本抽样\n\nnum_workers：使用多进程加载的进程数，0 代表不使用多进程\n\ncollate_fn： 如何将多个样本数据拼接成一个 batch，一般使用默认的拼接方式即可\n\npin_memory：是否将数据保存在 pin memory 区，pin memory 中的数据转到 GPU 会快一些\n\ndrop_last：dataset 中的数据个数可能不是 batch_size 的整数倍，drop_last 为 True 会将多出来不足一个batch的数据丢弃\n\n\n在数据处理中，有时会出现某个样本无法读取等问题，比如某张图片损坏。这时在__getitem__函数中将出现异常，此时最好的解决方案即是将出错的样本剔除。如果实在是遇到这种情况无法处理，则可以返回 None 对象，然后在 Dataloader 中实现自定义的 collate_fn，将空对象过滤掉。但要注意，在这种情况下 dataloader 返回的 batch 数目会少于 batch_size。（4）数据采样\nPyTorch中还单独提供了一个 sampler 模块，用来对数据进行采样。常用的有随机采样器：RandomSampler，当dataloader的shuffle参数为True时，系统会自动调用这个采样器，实现打乱数据。默认的是采用SequentialSampler，它会按顺序一个一个进行采样。这里介绍另外一个很有用的采样方法： WeightedRandomSampler，它会根据每个样本的权重选取数据，在样本比例不均衡的问题中，可用它来进行重采样。\n构建WeightedRandomSampler时需提供两个参数：每个样本的权重weights、共选取的样本总数num_samples，以及一个可选参数replacement。权重越大的样本被选中的概率越大，待选取的样本数目一般小于全部的样本数目。replacement用于指定是否可以重复选取某一个样本，默认为True，即允许在一个epoch中重复采样某一个数据。如果设为False，则当某一类的样本被全部选取完，但其样本数目仍未达到num_samples时，sampler将不会再从该类中选择数据，此时可能导致weights参数失效。\n","categories":["阅读笔记"],"tags":["Pytorch","深度学习"]},{"title":"《深度学习从算法到实战》阅读笔记","url":"/2022/05/16/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8E%E7%AE%97%E6%B3%95%E5%88%B0%E5%AE%9E%E6%88%98%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"总结：读这本的过程比预期的要快很多，大部分，也可以说几乎是全部，其实只用了一个下午的时间就看完了。因为书里面的内容之前就已经或多或少地了解过，而且书本身的内容也不算特别复杂。网络的基本结构，卷积，梯度都已经比较熟悉了。时间基本都放在了矩阵分解和后面介绍几种经典的网络结构。总的来说，这本书的内容并不复杂，因为我之前就已经了解过相关的知识了，之所以再来回顾一遍，是为了系统性地把这些知识全部都再过一遍，让之前那些拼凑起来的知识可以连成一个整体。不过书里面并没有介绍数据处理的方法，我觉得在学习和使用神经网络的时候，如何处理数据的问题经常会比网络本身更复杂一些。\n因为偷懒，所以只简单地记录了一些我觉得值得关注的内容，而且大部分内容都可以在网上找到。书里面的内容更多地还是偏向普及一些常识和用代码实现一些经典网络，所以想要更深入地了解还是需要在网上查找资料。\n一、绪论二、深度学习5-4-6速成法1.深度学习编程难点：深度学习的理论\n深度学习的理论如何与编程工具结合\n2.计算图\t计算图 - 深度学习™ (yiibai.com)计算图模型是一种运行深度学习的特殊的计算机专用语言\n3.深度学习编程主要框架8种主流深度学习框架介绍 - 知乎 (zhihu.com)4.五步法构造基本模型构造模型基本步骤：\n构造网络模型- &gt; 编译模型 -&gt; 训练模型 -&gt; 评估模型 -&gt; 使用模型进行预测\n\n网络模型基本元素：\n（1）网络层结构：由6种基本层模型构成。全连接层Dense；卷积层，主要用于图像数据处理；循环层，主要用于处理语言和文本等序列数据；防止过拟合层，如Dropout层，池化层等；层间结合层，用于数据结构的转换；辅助层。\n（2）激活函数：如sigmoid，relu，softmax等，可以赋予网络非线性关系。\n（3）损失函数：主要有四类，针对多分类的categorical_crossentropy对数损失，针对二分类的binary_crossentropy对数损失，还有用于回归的mean_squared_error平均方差损失和mean_absolute_error平均绝对值损失。\n（4）优化器：如SGD随机梯度下降，RMSProp，Adagrad，Adam，Adadelta等\n\n三、张量与计算图四、向量与矩阵五、高级矩阵编程（线性代数）1.范数及其实现欧几里得距离和Frobenius是范数的特例，范数的定义如下：其中p属于R，p&gt;=1。\n范数本质上是将向量映射到非负值的函数。当p=2时，L2范数称为欧几里得范数。\n更严格地说，范数是满足下列性质的任意函数：\n\n属于，\n2.迹运算迹运算的定义为对角线元素之和。矩阵转置后迹不变，可以通过迹来计算矩阵的范数。\n3.矩阵分解对于多数方阵，可以直接使用特征值分解。对于非方阵，可以通过奇异向量和奇异值对非方阵进行奇异值分解(,singular value decomposition ,SVD)。将矩阵分解为个矩阵的乘积：，其中和都定义为正交矩阵，是对角矩阵，但不一定是方阵。D在对角线上的值称为奇异值，U称为左奇异向量，V称为右奇异向量。\n4.Moore-Penrose广义逆( Moore-Penrose伪逆（Moore-Penrose广义逆）使用逆矩阵可以非常方便的求解线性方程组，但是在机器学习中很多方程组都是欠定的，即并不存在逆矩阵，不能使用常规的方法求解逆矩阵。\nMoore-Penrose广义逆的定义为：或者\n即奇异值分解，其中的计算方法是将中所有非值取倒数然后将矩阵转置。\n六、优化方法 梯度下降算法及其改进方法详解\n1.梯度下降原理2.优化方法进阶\n（1）批量梯度下降（BGD）\n（2）随机梯度下降（SGD）\n（3）动量优化\n（4）自适应方法\n（5）自适应与动量结合\n\n","categories":["阅读笔记"],"tags":["深度学习"]},{"title":"低轨卫星轨道误差、卫星钟差灯对地面定位的影响","url":"/2024/06/09/%E4%BD%8E%E8%BD%A8%E5%8D%AB%E6%98%9F%E8%BD%A8%E9%81%93%E8%AF%AF%E5%B7%AE%E3%80%81%E6%98%9F%E9%92%9F%E8%AF%AF%E5%B7%AE%E7%AD%89%E5%AF%B9%E5%9C%B0%E9%9D%A2%E5%AE%9A%E4%BD%8D%E7%9A%84%E5%BD%B1%E5%93%8D/","content":"一、轨道与钟差对低轨导航增强 PPP 性能影响分析(2023)该篇文章以适用于低轨卫星的空间信号测距误差 SISRE 为基础，通过控制卫星轨道误差和卫星钟差来改变 SISRE，并验证不同的 SISRE 对低轨卫星增强 PPP 定位性能的影响。文中将仿真低轨星座设置为含有近极轨与倾斜轨道的混合低轨星座，低轨卫星数量为 144 颗，将轨道预报得到的轨道数据设置为精密轨道，通过在观测仿真中根据不同的 SISRE 在精密轨道和钟差中分别添加相应的误差，以及对流层延迟和电离层延迟等得到仿真数据，最后利用仿真数据进行北斗与低轨星座的联合精密单点定位。\n\n低轨卫星运行速度快，相同的时间内在天空划过的轨迹较长，几何构型变化剧烈，可快速分离模糊度、对流层参数以及接收机钟差之间的相关性，因此联合低轨卫星进行导航增强 PPP 可极大地缩短首次定位收敛时间；同时低轨卫星轨道高度低，其导航信号落地功率高，可实现遮蔽等复杂条件乃至室内条件下的定位，弥补了传统中高轨导航卫星定位能力的不足。\n\n1.低轨导航增强 PPP 的优势：\n由于低轨卫星的几何构型变化剧烈，低轨卫星可以显著加快精密单点定位的收敛速度，而且低轨卫星的数目越多，收敛速度越快。当卫星数达到 66 颗时，收敛时间可以降低 50%，当卫星数达到 192 颗以上时，收敛时间可以降低 90%。\n\n2.低轨卫星的系统差特点：\n目前低轨卫星定轨主要采用简化动力学法，利用星载 GNSS 观测数据和动力学方法实现，因此其卫星轨道误差和卫星钟差模型与传统的 GNSS 系统有所不同。有研究表明，当低轨卫星的轨道误差超过0.35m时，使用低轨卫星定位的精度会降低。\n\n3.低轨卫星的 SISRESISRE 主要用于评估卫星导航系统空间信号强度，表示卫星轨道与钟差在卫星至地面视线方向上的误差。影响 SISRE 的因素包括地面站位置、卫星轨道与钟差、卫星轨道高度以及卫星信号张角。式中、、 分别表示轨道的径向、切向和法向的误差，$\\delta_{{T}}表示卫星钟差误差，\\alpha和\\beta为径向、切向和法向误差系数，c为卫星钟差误差系数。对于低轨卫星，需要重新计算式中的\\alpha和\\beta。$\\begin{aligned}S{\\mathrm{SISRE}}&amp;= \\sqrt{\\int_{0}^{2\\pi}\\int_{\\lambda_{0}}^{\\frac{\\pi}{2}}S_{i\\mathrm{SISRE}}^{2} \\frac{\\cos \\lambda \\mathrm{d}\\lambda \\mathrm{d}\\varphi}{2\\pi ( 1 - \\sin \\lambda_{0} )}} =\\&amp;\\sqrt{( \\alpha\\delta_{R} - c\\delta{T} )^{2} + \\beta^{2}( \\delta{A}^{2} + \\delta{C}^{2} )} ,\\end{aligned}$$式中：$\\lambda{0}与卫星截止高度角E相关，已知\\lambda{0}=\\arcsin \\frac {R{e}\\cos E}{R_{s}}E，当截止高度角为时，\\lambda_{0}=\\arcsin\\frac{R{e}}{R_{s}}。\\alpha与\\beta计算公式如下：$\n对  和  进行定积分计算后，即可得到对应卫星轨道高度  和截止高度角  的 SISRE 系数。\n低轨卫星由于轨道高度较低，信号张角大，SISRE 系数与高中轨卫星差异较大。其中  随着卫星轨道高度的降低而逐渐减小，表明当轨道高度降低时，卫星轨道  方向在 SISRE 中的占比逐渐减小， 随着卫星轨道高度的降低逐渐增加，最大可达到 1/3 以上，说明卫星轨道  和 ​ 方向在 SISRE 中的占比逐渐增加。因此，不同轨道高度的低轨卫星需要采用特定的 SISRE 系数。\n4.PPP 收敛时间的影响因素PPP 的收敛与多种因素有关，包括但不限于低轨卫星轨道与钟差的精度、可视低轨卫星的几何构型、数据采样率、低轨卫星轨道高度以及传播模型误差。\n5.结论\n\n低轨卫星由于轨道高度较低，信号张角大，SISRE 计算中的系数与中高轨卫星存在显著不同 , 其切向和法向占比最大可达 30%，因此卫星轨道切向和法向的误差不可忽略。\n低轨卫星的 SISRE 显著影响着 PPP 的收敛时间。为了保证北斗与低轨联合 PPP 的性能，需要控制低轨卫星的 SISRE，SISRE 最好应小于 0. 06 m，最大不能超过 0.20 m，否则将无法加速 PPP 收敛。\n北斗与低轨联合 PPP 无法显著提升定位精度，随着低轨 SISRE 逐渐增大，平均定位误差逐渐增加，因此为了保证联合 PPP 的定位精度，应将低轨卫星轨道与钟差误差控制在 0.2 m 以下。\n\n\n二、LEO 卫星轨道误差对 GPS/LEO 联合 PPP 的影响(2016)该篇文章主要研究了 GPS 与不同轨道误差的 LEO 卫星联合 PPP 的性能。当 GPS 轨道误差一定，分析对比了不同轨道误差的 LEO 卫星与 GPS 联合 PPP 和单独 GPS  PPP 两种方案的 PPP 收敛时间分布和用户定位精度。文中的仿真实验没有考虑电离层误差的高阶项，硬件延迟以及不同系统之间的钟差的影响，将 GPS 和 LEO 的轨道误差视为服从  的零均值正态分布，并且 LEO 的轨道误差逐渐增大。\n1.传统精密单点定位收敛时间长的原因\nGPS PPP 的定位结果收敛时间过长主要是由于在较短的时间内，GPS 卫星在用户上空的分布变化较小，历元间有较强的相关性，观测矩阵系数阵的状态随时间变化不明显，导致参数估计效果不佳，从而造成浮点解解算结果的精度不理想，模糊度固定时间较长。\n\n2.基本观测模型和数学模型文中采用无电离层组合来消去电离层一阶项误差，并使用 LEO 和 GPS 联合定位：文中采用递推最小二乘对参数进行估计，估计参数为 。由于低轨卫星运动速度快，每颗卫星在视野内的可视时间短，所以在 GPS 与 LEO 卫星联合 PPP 中只对 GPS 的模糊度  进行固定，而 LEO 卫星的模糊度  保持实数解，不对其进行固定。\n3.模糊度固定无电离层组合的模糊度参数可以表示为宽巷模糊度  和 ：本文中无电离层 GPS 模糊度分两步进行固定，首先固定宽巷模糊度 ，再固定窄巷模糊度 。宽巷模糊度可以通过 MW 组合得到：由于宽巷模糊度的波长较长，可以直接将宽巷模糊度固定到与其最近的整数。在得到宽巷模糊度之后可以根据  得到窄巷模糊度的浮点解。窄巷模糊度的波长只有 0.107m，不能直接固定，而且模糊度之间存在相关性，因此需要采用 LAMBDA 算法对窄巷模糊度进行固定。\n4.结论当 LEO 卫星轨道误差  时，在第 50 个时隙，加入 LEO 卫星后浮点解的三维位置偏差为 0.052m，相对于单独 GPS 系统的三维位置偏差 0.094m，减少了 44.7%。随着 LEO 卫星误差的增大，浮点解的定位误差明显增大，当  时，GPS 与 LEO 卫星联合 PPP 所得的浮点解的三维定位误差比单独 GPS PPP 大 0.02m。\n当 LEO 卫星轨道误差小于 0.35m 时，可以加快 GPS 与 LEO 联合 PPP 固定解的收敛，在第 50 个时隙内的收敛概率也会增加。当 LEO 卫星轨道误差达到 0.4m 时，联合系统 PPP 固定解的收敛时间会变长，收敛概率也会降低。\n三、顾及不同LEO卫星数量及轨道误差的 GPS/LEO联合PPP该篇文章通过在 LEO 星座中加入轨道误差，利用实测 GPS 观测数据以及仿真得到的低轨观测数据进行联合 PPP 解算，从 PDOP值、可视卫星数量、PPP 定位精度和收敛时间等方面评估 LEO 星座对 GPS 精密单点定位性能的增强作用。\n1.仿真观测值仿真观测值的基本原理是定位的逆向处理，利用单个历元下可观测到的卫星伪距和载波相位观测数据，经过误差改正之后，通过最小二乘法或卡尔曼滤波估计接收机的位置和钟差。已知测站位置、卫星轨道和卫星钟差，可以计算出测站与卫星间的几何距离。由已有的模型计算从信号发射到接受过程中的所有误差，并考虑观测噪声，最后将这些误差和噪声加入到几何距离上模拟伪距和载波观测数据，即仿真观测值。为简化实验，文中没有考虑卫星钟差以及天线数值，低轨卫星 PCO、天线相位中心变化 PCV 均设置为 0。\n2.结论\n\n相较于 GPS 单系统，GPS+LEO 系统能有效提高观测历元的平均可见卫星数量，由单系统的 8.3 增加至 18.0（120颗）、27.7（240颗）、39.6（360颗），PDOP 平均值由 2.0 降至 1.3（120颗）、1.1（240颗）、0.9（360颗）。\n低轨卫星系统可以增强 GPS 的 PPP 浮点解，在 E ，N 方向上均有一定程度的改善，在 U 方向上的精度提升更加明显，最高可以达到 70%以上。\n单系统 GPS 的平均收敛时间伟 17.1min，分别加入 120颗、240颗、360颗 LEO 卫星后 GPS+LEO 联合 PPP 收敛时间分别为 4.7min、2.1min、1.5min。\n\n\n3.改进方向本文在 LEO 观测数据仿真过程中未考虑 LEO 卫星 PCO、PCV 及钟差的设置及改正，同时 LEO 轨道误差的函数模型过于简单且在解算过程中仅考虑了静态情况。后续研究中将进一步完善 LEO 观测数据仿真过程及 GPS/LEO 联合 PPP 软件 ,以获得更细致的分析结论。\n四、卫星轨道误差对定位精度影响的摄动分析方法(2018)1.摘要\n传统方法通常采用基于精度因子与用户等效伪距误差的方法对定位误差进行评估，但是该方法在其精度表征公式的推导过程中需要对测量方程组系数矩阵 H 以及用户等效伪距误差分布做若干假设，即该方法是一个近似评估公式。本文采用矩阵摄动数学理论研究卫星轨道误差对定位方程组解的影响，利用谱范数条件数对方程组形态进行刻画，该方法无需进行轨道坐标及用户等效伪距误差换算，能够更加直接和准确地评估卫星轨道误差对定位解精度的影响。\n\n2.定位精度的评估全球卫星导航系统的定位误差通常由精度因子（DOP）和用户等效伪距误差（UERE）来表示：对于某一颗给定卫星，用户等效伪距误差表示与该卫星相关的各个误差源产生的影响的和，包括伪距测量误差，大气时延误差，卫星原子钟钟差和卫星轨道误差等。\n基于精度因子与用户等效伪距误差的精度表征方法建立在两个假设条件的基础上：\n（1）观测方程组系数矩阵 H 无随机分量，这个假设使得在推导过程中可以将系数矩阵 H 移到期望算子之外，因此简化了推到结果。但是只有当所有观测卫星成正交分布时，观测方程组系数矩阵 H 中才无随机分量。\n（2）所有的用户等效伪距误差具有相同的均方差 ，且是互不相关的零均值，这个假设使得期望算子可以被简化为 ，但是实际的用户等效伪距误差的构成非常复杂，除了符合高斯白噪声的随机测量误差外，还有大气时延，硬件设备偏差，接收机端的多路径效应等。\n（3）另一方面，在各类定位误差源中，伪距测量误差、大气时延误差及卫星原子钟钟差等属于一维矢量，即卫星至用户视线方向上的误差，因而可以轻易将其换算至用户等效伪距误差中。然而，卫星轨道误差（ 一般认为是卫星广播星历误差）属于三维方向的误差量，难以直接换算至用户等效伪距误差。一般需将卫星广播星历误差从地心地固坐标系转换至该时刻的轨道坐标系下，得到卫星轨道位置在径向（Ｒ）、横向（T）和法向（N）上的偏差。在此基础上，认为径向轨道误差对用户定位精度的影响最大。\n3.观测方程与误差方程全球卫星导航系统单点定位解的线性方程组矩阵形式可表示为：其中，H 表示方向余弦矩阵：其中，， 和  分别为用户估计位置指向第  颗卫星的单位矢量方向余弦。 为近似解与真解之间的改正向量： 为由用户估计位置和钟差估计值计算得到的几何距离与观测伪距之差:当  时，上式为正定方程。其解为 ，\n当  时，上式为超定方程。采用最小二乘法获得冗余解：。\n根据上式得到用户坐标位置误差的协方差为：假设系统矩阵 H 无随机分量，且所有用户等效伪距误差均有相同的方差，则上式可简化为：其中，－ 为用户等效伪距误差换算至用户位置误差时的误差放大因子，它与用户至卫星的单位矢量所勾勒的立体体积成反比。\n根据用户所在坐标参考系的不同，精度因子可以表征不同方向上的位置误差。当用户位于用户等效伪距误差坐标系时，其实际含义为:协方差阵协方差阵\n4.轨道误差摄动分析法当考察卫星轨道误差对定位解的影响时，认为对方程组左端系数矩阵 H 带来摄动误差 ，并给坐标及钟差未知数向量  造成摄动 ，因此有：由于矩阵 H 非奇异，而由卫星轨道误差等造成的摄动误差  甚小，故矩阵  仍然可以保持它的非奇异性。因此得到：对上式左右两端进行范数表达可以得到：进一步有：式中的  表示误差放大率，即解的相对误差为测量数据相对误差的  倍，又可以表示为：因此得到：由于 相对  而言一般已经充分小，因此上式通常可以近似为 。\n这说明当  具有扰动  时，引起的解的相对误差不超过  的相对误的  倍。即当方程组中的  具有扰动误差时，方程组解的误差可以由条件数  决定，其作用可视为误差传递的放大因子。\n五、非组合精密单点定位模型参数估计1.摘要\n本文首先使用非组合精密单点定位模型直接估计电离层延迟，再利用传统非组合精密单点定位模型估计的传统接收机钟差与改进非组合精密单点定位模型估计的改进接收机钟差与接收机差分码偏差组成包含接收机钟差、第一频点和第二频点的硬件延迟的方程组继续计算，最终估计接收机钟差、第一频点和第二频点的硬件延迟。\n\n2.数学模型传统模型传统非组合模型将倾斜路径电离层延迟作为待估参数保留下来：在进行非组合精密单点定位时，需要使用分析中心或者产品中心提供的精密星历产品和精密钟差产品，而多数中心进行数据处理的模型是基于电离层组合模型，所以提供的精密钟差产品是包含组合硬件延迟的钟差，有：式中 ， ， 。\n为了消除秩亏的影响，接收机钟差也采用包含由接收机端伪距硬件延迟的钟差，接收机钟差为：式中，。\n因此最终的传统非组合模型为：$$\\left{\\begin{array}{l}P_i=\\rho_i+c \\overline{\\mathrm{r}}{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}+C_i \\bar{I}^{\\mathrm{s}}+M^{\\mathrm{s}} T+e_P \\L_i=\\rho_i+c \\bar{t}{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}-C_i \\bar{I}^{\\mathrm{s}}+M^{\\mathrm{s}} T+\\lambda_i \\bar{N}_i^{\\mathrm{s}}+e_L\\end{array}\\right.$$\n$$\\bar{I}^{\\mathrm{s}}=I^{\\mathrm{s}}+\\beta\\left(d_{\\mathrm{r}, 1}-d_{\\mathrm{r}, 2}-d_1^{\\mathrm{s}}+d_2^{\\mathrm{s}}\\right) ; \\lambda_i \\bar{N}i^{\\mathrm{s}}=\\lambda_i N_i^{\\mathrm{s}}- \\d{\\mathrm{IF}, \\mathrm{t}}+d_{\\mathrm{IF}}^{\\mathrm{s}}+b_{\\mathrm{r}, i}-b_i^{\\mathrm{s}}+C_i \\beta\\left(d_{\\mathrm{r}, 1}-d_{\\mathrm{r}, 2}-d_1^{\\mathrm{s}}+d_2^{\\mathrm{s}}\\right)$$\n采用双频进行观测时，当观测卫星数为 m 颗时，则每个历元可组成 4m 个观测方程，传统非组合模型的待估参数有接收机的三维坐标、接收机钟差、m 个第一频点的电离层延迟、天顶方向的对流程延迟、2m 个整周模糊度参数，共计 3m+5 个参数，所以最少需要观测 5 颗以上的卫星进行初始化计算。\n改进模型$$\\left{\\right.$$\n采用卫星 DCB 产品来改正卫星差分码偏差，卫星 DCB 产品修正公式为：经过卫星 DCB 产品修正后函数模型变为：$$\\left{\\begin{array}{l}\\hat{P}1=\\rho_1+c t{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}+C_1 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+d_{\\mathrm{r}, 1}+e_P \\L_1=\\rho_1+c t_{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}-C_1 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+\\lambda_1 \\hat{N}1^{\\mathrm{s}}+e_L \\\\hat{P}2=\\rho_2+c t{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}+C_2 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+d{\\mathrm{r}, 2}+e_P \\L_2=\\rho_2+c t_{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}-C_2 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+\\lambda_2 \\hat{N}2^{\\mathrm{s}}+e_L\\end{array}\\right.$$式中：$\\hat{P} {1}= P{1}- C{1}\\beta ( d_{2}^{s}- d_{1}^{s})\\hat{P} {2}= P{2}+ C_{1}\\alpha ( d_{2}^{s}- d_{1}^{s})；\\lambda {i}\\hat{N} {i}^{s}= \\lambda {i}N{i}^{s}+ d{\\mathrm{IF}}^{\\mathrm{s} }+ b{\\mathrm{r} , i}- b_{i}^{\\mathrm{s} }$​ 。\n由于  和  均与  之间存在线性相关，为了消除秩亏的影响，合并相关参数得到：$$\\left{\\begin{array}{l}\\hat{P}1=\\rho_1+c \\tilde{t}{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}+C_1 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+e_P \\L_1=\\rho_1+c \\tilde{t}{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}-C_1 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+\\lambda_1 \\tilde{N}1^{\\mathrm{s}}+e_L \\\\hat{P}2=\\rho_2+c \\tilde{t}{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}+C_2 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+d{\\mathrm{r}, 2}-d{\\mathrm{r}, 1}+e_P \\L_2=\\rho_2+c \\tilde{t}{\\mathrm{r}}-c \\bar{t}^{\\mathrm{s}}-C_2 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+\\lambda_2 \\tilde{N}2^{\\mathrm{s}}+e_L\\end{array}\\right.$$式中$: c\\tilde{t} {\\mathrm{r} }= ct{\\mathrm{r} }+ d{\\mathrm{r} , \\mathrm{l} }\\lambda {i}\\tilde{N} {i}^{\\mathrm{s} }= \\lambda {i}N{i}^{\\mathrm{s} }+ d{\\mathrm{IF}}^{\\mathrm{s} }+ b{\\mathrm{r} , \\mathrm{j} }- b{i}^{\\mathrm{s} }- d_{\\mathrm{r} , \\mathrm{l} }$ 。\n接收机差分码偏差  用  表示，消除卫星钟差，得到最终的改进非组合模型为：$$\\left{\\begin{array}{l}\\tilde{P}1=\\rho_1+c \\tilde{t}{\\mathrm{r}}+C_1 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+e_P \\\\tilde{L}1=\\rho_1+c \\tilde{t}{\\mathrm{r}}-C_1 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+\\lambda_1 \\tilde{N}1^{\\mathrm{s}}+e_L \\\\tilde{P}2=\\rho_2+c \\tilde{t}{\\mathrm{r}}+C_2 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+D C B{\\mathrm{r}}+e_P \\\\tilde{L}2=\\rho_2+c \\tilde{t}{\\mathrm{r}}-C_2 I^{\\mathrm{s}}+M^{\\mathrm{s}} T+\\lambda_2 \\tilde{N}2^{\\mathrm{s}}+e_L\\end{array}\\right.$$待估参数为接收机的三维坐标、接收机钟差、m 个第一频点的电离层延迟、天顶方向的对流层延迟、2m 个整周模糊度参数，$r{DCB}$ 接收机硬件延迟参数、共计 3m+6 个参数，每个历元有 4m 个方程，因此最小需要观测 6 颗卫星进行初始化计算。\n采用传统非组合模型和改进非组合模型分别进行精密单点定位，定位收敛后传统非组合模型估计出接收机钟差 、改进非组合模型估计出的接收机钟差  和接收机的频间差 ，再将相同历元的 3 个估计参数分别组成由接收机钟差 、第一频点的硬件延迟  和第二频点的硬件延迟  组成的方程组，有：\n3.结论\n传统非组合模型定位精度和收敛时间优于改进非组合模型。采用 B1I+B3I 和 B1C+B2a 频点组合的传统非组合模型收敛时间分别为 29 和 33 min，优于改进非组合模型的 42 和 45 min。传统非组合模型的定位精度比改进非组合模型高 10% 左右。\n改进非组合模型计算电离层延迟的修正率较传统非组合模型提高了 1.9%和 2.19%。由于传统非组合模型估计的电离层延迟包含了接收机和卫星硬件延迟，估计精度低于改进非组合模型。\n采用 B1I+B3I 和 B1C+B2a 等 2 种频点组合估计的接收机钟差 RMS 分别为 0.19 和 0.21 ns。绝对值相差 0.09 ns。估计的 B1I、B3I、B1C 和 B2a硬件延迟与标定值差值分别为-0.13、-0.08、0.14、0.06 ns，所有估值和标定值差值在 0.14 ns 以内，取得了较好的估计效果。\n\n六、PCO and hardware delay calibration for LEO satellite antenna downlinking navigation signals\n Using the satellite geometries of Sentinel-3B and Sentinel-6A as examples，the study analyzed the formal precision and bias influences for potential downlink antenna PCOs and hardware delays of LEO satellites under different ground network distributions，and processing periods.\n\n\nBenefiting from the lower altitudes and higher speeds of LEO satellites，the augmentation of the GNSS by LEO satellites has numerous advantages，including providing hundreds to thousands of times stronger signal strength than the GNSS satellites，the more rapid convergence time of precise point positioning (PPP) and the PPP-Real-Time Kinematic positioning due to the rapid geometry change . \n\n\nThe faster speed of the LEO satellites also helps to whiten multipath effects, which changes the behaviors of these typically mismodelled biases attributing them mostly to noise. \n\n\nTo achieve high-accuracy positioning and timing on the ground using LEO navigation signals, the errors and biases in the downlink navigation signals between LEO satellites and ground users must be precisely calibrated, modeled, or eliminated. These errors include LEO satellite orbital and clock errors, ground receiver clock errors, tropospheric delays, ionospheric delays, phase ambiguities, phase center offsets (PCOs) and phase center variations (PCVs), and various hardware delays.\n\n\nThe obtained satellite clock errors often contain the Ionosphere Free (IF) code hardware delays of the downlink navigation sig nal transmitter/antenna.Ground users only apply appropriate DCBs according to the type of code observations used.\n\n\n In contrast to GNSS satellite orbital and clock products that are computed based on signals tracked by a ground tracking network, LEO satellites have a significantly smaller footprint on the Earth’s surface due to their lower orbital altitudes, which can both lead to difficulties in continuous tracking of the LEO satellite navigation signals. As a result, high-accuracy LEO satellite orbital and clock products are often determined using the GNSS signals tracked onboard instead of those tracked by ground stations.\n\n\nHowever, the estimated orbital errors that are typically at the LEO satellite Center of Mass (CoM) and the clock errors, which contain IF code hardware delays of the onboard GNSS receiver/antenna, are not products that can be directly used by ground users.\n\n\nFirst, the IF code hardware delay of the onboard GNSS receiver and antenna contained in the estimated LEO satellite clock errors must be accounted for. Second, the IF code hardware delay of the LEO satellite’s downlink navigation signal transmitter and antenna needs to be added.\n\n\nAfter the determination of the initial values for the hardware delays and orbits at the APCs for the downlink antenna, the increments of the PCOs and hardware delays compared to the on-ground calibration are estimated in the second step.\n\n一、名词缩写\n\nPrecise Orbit Determination (POD)\n\nAntenna Phase Center (APC)\n\nCenter of Mass (CoM)\n\nDifferential Code Biases (DCBs)\n\nPhase Center Variations (PCVs)\n\nPhase Center Offsets (PCOs)\n\nAntenna Reference Point (ARP)\n\nObserved-Minus Computed (O-C)\n\n\n\n二、相关研究背景\n\nGNSS 天线的硬件延迟和温度之间存在很强的联系，因此可以根据温度得到在轨低轨卫星的硬件延迟与温度有关部分的一阶项。\nGNSS 观测值可以被用于计算观测站的坐标，接收机钟差和准天顶对流层延迟。\n在低轨卫星的下行信号中，PCOs 和硬件延迟存在很强的相关性，可以考虑将两者结合。\n在本文中，基于 PCVs 远比 PCOs 小的假设，PCVs 没有被估计。\n\n本文首先介绍了计算低轨卫星下行信号 PCOs 和 IF 码偏差硬件延迟的方法，包括评估它们的量级和多种因为不充分的模型对 PCOs 和 IF 码偏差硬件延迟的影响，然后以两种不同倾角的低轨卫星为例，仿真了下行双频导航信号的 O-C，最后计算了不同分布的地面站网和处理间隔下计算得到的 PCOs 和 硬件延迟。\n\n\n低轨卫星的覆盖面积远小于 GNSS 卫星，不容易对其进行持续跟踪，所以高精度的低轨卫星轨道和时钟产品通过是根据星载的 GNSS 接收机的数据计算得到的。但是这种方法得到的轨道误差是以低轨卫星的质心为基准的，而且时钟产品包含了星载 GNSS 接收机和天线的无电离层码硬件延迟，导致地面用户不能直接使用这些由星载 GNSS 接收机得到的产品。地面用户需要的是以天线相位中心为基准的低轨卫星轨道和包含了下行天线的无电离层码硬件延迟的钟差。\n\n三、研究内容1. In-orbit calibration of the PCO and code hardware delays of the downlink antenna1.1 Determination of the initial LEO satellite APCs and clock errors for downlink purposes.\n确定低轨卫星的初始天线相位中心轨道；\n\n\n确定初始钟差：$$\\widehat{d \\breve{t}}^{L s 0}\\left(t_i\\right)=\\widehat{d \\tilde{t}}^{L s}\\left(t_i\\right)+\\frac{d_{\\mathrm{IF}}^{L 0}+T^{L s}\\left(t_i\\right) \\dot{d}{\\mathrm{IF}}^{L 0}-\\hat{d}{\\mathrm{IF}, \\mathrm{GNSS}}^{L s}}{c}$$ 表示经过后处理得到的精密钟差；\n 表示在轨计算得到的 GNSS 天线的 IF 码硬件延迟；\n 和  表示低轨卫星下行信号的常数项和一次项；\n\n1.2 Determination of the PCO and hardware delay increments for the downlink antenna.\n使用高精度的 GNSS 产品和模型进行改正，包括对流层湿延迟，相位缠绕，潮汐，卫星和接收机的天线相位中心变化和天线相位中心偏差以及无电离层模型可以得到高精度的观测站坐标，每个历元的接收机钟差和每隔一段时间的对流层天顶延迟。\n需要注意的是，由于低轨卫星的位置是根据星载 GNSS 信号接收机的数据计算得到的，而地面观测站接受的是由低轨卫星的下行天线发射的下行信号，因此估计的接收机钟差中还包括 GNSS 信号的无电离层组合的接收机码偏差：$$E\\left(\\widehat{d\\tilde{t}r}\\right)=dt_r+\\frac{d{IF,G}}c在去除掉接收机坐标和卫星坐标，接收机钟差和卫星钟差，以及对流层天顶延迟之后，可以得到用于计算无电离层组合信号的和码硬件延迟的方程：\\begin{aligned}E\\left(\\Delta p_{r,\\mathrm{IF}}^{Ls}\\left(t_{i}\\right)\\right)&amp;=\\left(\\mu_{r}^{Ls}\\left(t_{i}\\right)\\right)^{\\mathrm{T}}R_{\\mathrm{NEU2ECEF}}\\left(t_{i}\\right) \\delta x_{\\mathrm{PCO}}^{Ls}\\&amp;-\\left(\\delta\\tilde{d}{\\mathrm{IF}}^{Ls}+T^{Ls}\\left(t{i}\\right)\\delta\\dot{d}{\\mathrm{IF}}^{Ls}\\right)\\E\\left(\\Delta\\varphi{r,\\mathrm{IF}}^{Ls}\\left(t_{i}\\right)\\right)&amp;=\\left(\\mu_{r}^{Ls}\\left(t_{i}\\right)\\right)^{\\mathrm{T}}R_{\\mathrm{NEU2ECEF}}\\left(t_{i}\\right) \\delta x_{\\mathrm{PCO}}^{Ls}\\&amp;+\\lambda_{\\mathrm{IF}}\\tilde{N}{r,\\mathrm{IF}}^{Ls}\\end{aligned}$$$\\mu{r}^{Ls}(t_{i})表示低轨卫星和地面观测站之间的单位方向向量，\\mu_r^{Ls}\\left(t_i\\right)=\\frac{\\hat{x}_r-\\hat{x}^{Ls0}\\left(t_i\\right)}{\\left|\\hat{x}_r-\\hat{x}^{Ls0}\\left(t_i\\right)\\right|}$；\n  表示相比在地面时计算得到的结果，低轨卫星在轨时无电离层组合信号的 PCO 的增量；\n $\\delta\\tilde{d}{\\mathrm{IF}}^{Ls}表示低轨卫星在轨时无电离层组合信号的码硬件延迟相比在地面时的增量，并且其中包含了低轨卫星下行信号和信号的码硬件延迟的差，$\\delta\\tilde{d}\\mathrm{IF}^{Ls}=\\delta d_\\mathrm{IF}^{Ls}-d_\\mathrm{IF}+d_\\mathrm{IF,G}$$；\n 表示下行天线的无电离层码硬件延迟与温度有关的一阶项；\n基于上式，可以通过批最小二乘得到  ，$\\delta\\tilde{d}{\\mathrm{IF}}^{Ls}和\\delta\\dot{d}{\\mathrm{IF}}^{Ls}$。\n\n1.3 Corrections of the PCO and hardware delays for down link antenna.\n根据上面计算得到增量，以及初始值，即可得到在轨低轨卫星的 PCO 和 码硬件延迟：$$\\Delta\\hat{x}{\\mathrm{PCO}}^{Ls}=\\Delta\\hat{x}{\\mathrm{PCO}}^{Ls0}+\\delta\\hat{x}{\\mathrm{PCO}}^{Ls}\\\\tilde{d}\\mathrm{IF}^{Ls}=d_\\mathrm{IF}^{Ls0}+\\delta\\tilde{d}\\mathrm{IF}^{Ls}\\\\dot{d}\\mathrm{IF}^{Ls}=\\dot{d}\\mathrm{IF}^{Ls0}+\\delta\\dot{d}\\mathrm{IF}^{Ls}.$$\n\n2.Evaluation of formal precision and bias influence\n\n 表示包含 ，$\\delta\\tilde{d}{\\mathrm{IF}}^{Ls}和\\delta\\dot{d}{\\mathrm{IF}}^{Ls}$ 的未知向量；\n 表示包含包含观测值对未知数的偏导数的设计矩阵；\n 表示基于与高度角相关的权重函数的观测方差-协方差矩阵；\n\n\n\n标准差表示为：mis-modeled bias 在待估参数上的投影为：\n\n","categories":["GNSS","阅读笔记"],"tags":["GNSS"]},{"title":"使用nvdla时遇到的各种错误(半成品)","url":"/2022/03/10/%E4%BD%BF%E7%94%A8nvdla%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E5%90%84%E7%A7%8D%E9%94%99%E8%AF%AF/","content":"error: #error “Please port gnulib fseterr.c to your platform!NUC980开源项目24-Please port gnulib freadahead.c to your platform!_Jun626的博客-CSDN博客\n\n\nfreadahead.c:91:3: error: #error “Please port gnulib freadahead.c to your platform! Look at the definition of fflush, fread, ungetc on your system, then report this to bug-gnulib.”\n91 |  #error “Please port gnulib freadahead.c to your platform! Look at the definition of fflush, fread, ungetc on your system, then report this to bug-gnulib.”\n\n解决方法：\n进入目录**/buildroot/output/build/host-m4-1.4.18**\n在该目录下输入以下命令：\nsed -i 's/IO_ftrylockfile/IO_EOF_SEEN/' lib/*.cecho \"#define _IO_IN_BACKUP 0x100\" &gt;&gt; lib/stdio-impl.h\n\nerror: conflicting types for ‘copy_file_range’全志(allwinner)编译过程问题与解决方法汇总 - 旺旺Ever - 博客园 (cnblogs.com)\n\n\nerror: conflicting types for ‘copy_file_range’\nstatic errcode_t copy_file_range(ext2_filsys fs, int fd, ext2_file_t e2_file……\n修改文件：/buildroot/build/host-e2fsprogs-1.43.3/misc/create_inode.c\n\n注释掉：#inlucde &lt;unistd.h&gt;\n\nPackage ‘g++-4.8’ has no installation candidateubuntu系统使用update-alternatives 管理多版本gcc/g++_gcc update-alternatives\n\n原因是系统中并没有安装g++-4..8版本，直接使用20.04版本的Ubuntu镜像源进行安装会找不到4.8版本，需要将镜像源更换为14.4版本的之后再下载g++，安装gcc-4.8时同理。\n安装完成之后，使用以下命令，向系统中添加一个新的alternatives组，“50”表示版本的优先级。\nsudo apt-get install gcc-4.8sudo apt-get install g++-4.8sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 50sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.8 50\n\nls /usr/bin/gcc*\t\t#查看gcc的版本和链接\n\nupdate-alternatives --display &lt;name&gt;\t\t#显示命令&lt;name&gt;的信息及目标文件\n\nupdate-alternatives --config &lt;name&gt;\t\t#配置命令的版本\n\nupdate-alternatives --remove &lt;name&gt; &lt;path&gt;\t#移除系统中注册的某个&lt;name&gt;的某个软件版本&lt;path&gt;\n\n拉取github报错 failed: The TLS connection was non-properly terminated. 解决拉取github仓库报错“gnutls_handshake() failed”问题_烹茶室-CSDN博客\n\n原因可能是代理设置出错，为 http 错误配置了 https 的代理。有时在相同情况下又不会报错，可能还和网络的质量有关。\n解决方法：\n#重置代理git config --global  --unset https.https://github.com.proxy git config --global  --unset http.https://github.com.proxy \n\nGitHub访问加速[github访问加速 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/75994966#:~:text=github访问加速 1 获取GitHub官方CDN地址.. …,2 修改系统Hosts文件.. … 3 刷新系统DNS缓存)\n使用github的镜像网站http://cnpmjs.org，在http://github.com后面加个http://cnpmjs.org即可\n编译报错util/memfd.c:44:12: error: static declaration of ‘memfd_create’ follows non-static declara\n解决方法：修改报错的文件memfd.c\nvim qemu-2.11.0/util/memfd.c\n\n按照报错信息，找到memfd.c文件的第40行，将“static”声明修改为“int”\nstatic int memfd_create(const char *name, unsigned int flags)\nint memfd_create(const char *name, unsigned int flags)\n修改后保存退出，make，然后make clean。\n\nlinu内核编译时出现“make: arm-linux-gcc:command not found”解决方法 linux内核编译时出现“make: arm-linux-gcc:command not found”解决方法\naarch64-linux-gnu交叉编译Qt4.7.3 - 王楼小子 - 博客园 (cnblogs.com)\n\n一般出现这个错误是因为没有设置环境变量。\n解决方法：\n\n首先找到arm-linux-gcc所在目录， 使用命令\n\nfind -name arm-linux-gcc\n\n系统会显示arm-linux-gcc所在的路径，如/home/user/linux/bin/arm-linux-gcc\n\n将arm-linux-gcc所在路径加入系统环境变量。\n\n2.1 如果想对系统内所有用户都生效，可以编辑/etc/profile文件，在文件末尾添加路径。\n​\t  在这之前，需要先登陆超级用户，取得超级用户权限，输入命令\nsudo -s\n\n之后输入当前用户密码，登陆超级用户。\n然后再编辑profile文件，输入命令\nvim /etc/profileexport PATH=$PATH:/home/user/linux/bin\n\n编辑profile文件后，不会立即生效，需要再键入以下命令使之立即生效\nsouce /etc/profile\n\n2.2 如果只对当前用户生效，需要编辑~/.bashrc文件，之后再source该文件使之生效。方法与2.1步骤类似。\n编辑.bashrc文件\nvim ~/.bashrcexport PATH=$PATH:/home/user/linux/bin\n\n使.bashrc文件生效\nsource ~/.bashrc\n\n最后，再运行程序或者编译，如make。系统就能找到arm-linux-gcc工具了。\n\nError: (E115) sc_signal cannot have more than one driver: Error: (E115) sc_signal cannot have more than one driver:_u010144189的专栏-CSDN博客\n\n在执行SystemC程序的时候，出现这种错误时，只需输入以下命令，修改环境变量：\nexport SC_SIGNAL_WRITE_CHECK=DISABLE\n\ninsmod模块时候出现loading out-of-tree module taints kernelinsmod模块时候出现loading out-of-tree module taints kernel - 裸睡的猪 - 博客园 (cnblogs.com)\nmodulename: loading out-of-tree module taints kernel【转】 - sky-heaven - 博客园 (cnblogs.com)\nsrc/caffe/util/io.cpp: In function ‘bool caffe::ReadProtoFromBinaryFilesrc/caffe/util/io.cpp: In function ‘bool caffe::ReadProtoFromBinaryFile(const char*, google::protobu\n\n\nsrc/caffe/util/io.cpp: In function ‘cv::Mat caffe::ReadImageToCVMat(const string&amp;, int, int, bool)’:src/caffe/util/io.cpp:76:34: error: ‘CV_LOAD_IMAGE_COLOR’ was not declared in this scope76 | int cv_read_flag = (is_color ? CV_LOAD_IMAGE_COLOR :\n解决方法：\n编辑/caffe/src/caffe/util/io.cpp这个文件，\n将CV_LOAD_IMAGE_COLOR改成 cv::IMREAD_COLOR\n将CV_LOAD_IMAGE_GRAYSCALE改成 cv::IMREAD_GRAYSCALE\n\n\nfatal error: opencv2/core/core.hpp caffe framework installationfatal error: opencv2/core/core.hpp caffe framework installation\n致命错误：opencv2/core/core.hpp：编译caffe时没有那个文件或目录 - 堆栈内存溢出 (stackoom.com)\n\nCXX src/caffe/layers/data_layer.cppsrc/caffe/layers/data_layer.cpp:2:33:fatal error: opencv2/core/core.hpp: No such file or directory#include &lt;opencv2/core/core.hpp&gt;                       ^compilation terminated.make: *** [.build_release/src/caffe/layers/data_layer.o] Error 1\nsudo apt-get install libopencv-dev\n\n找到/usr/include/opencv4，将其中的/opencv2复制到/include目录下\n\n/usr/bin/ld: cannot find -lxxx 的解决办法ubuntu16.04下编译caffe出现.build_release/lib/libcaffe.so: undefined reference to google ::protobuf…的问题\ncaffe 安装报错解决办法_weixin_38883095的博客-CSDN博客\nUbuntu下caffe 配置（总结踩过的坑） - 小虫子12 - 博客园 (cnblogs.com)\n/usr/bin/ld: cannot find -lxxx 的解决办法 - zhming - 博客园 (cnblogs.com)\n先查找链接文件是否存在，一般都是存在的，如果不存在，重新下载。\n#error This file requires compiler and library support for the ISO C++ 2011enabled -std=c++11 · Issue #6359 · BVLC/caffe (github.com)\n\n#error This file requires compiler and library support for the ISO C++ 2011 standard. This support must be enabled with the -std=c++11 or -std=gnu++11 compiler options.\n\n\n找到当前编译工程中的 CMakeList.txt, 然后修改：\n# ---[ Flagsif(UNIX OR APPLE)set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -Wall\")endif()if()\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\n\n快速删除乱码文件[find: missing argument to `-exec’ - eryoung2 - 博客园 (cnblogs.com)](https://www.cnblogs.com/young233/p/11986633.html)\n","categories":["Debug"],"tags":["新玩具","Pytorch","深度学习"]},{"title":"卫星导航系统精密时间传递技术总结","url":"/2024/05/31/%E5%8D%AB%E6%98%9F%E7%B2%BE%E5%AF%86%E6%97%B6%E9%97%B4%E4%BC%A0%E9%80%92/","content":"一、卫星导航系统时间传递1.卫星导航系统时间传递技术在以下这些时间传递技术中，激光双向法具有最高的精度，但是由于激光时间传递容易受到天气的影响，并且在微弱激光信号检测、高精度伺服系统方面还存在技术难题，目前在各个卫星导航系统中仅作为校准和备份手段使用。\nGNSS共视法和全视法是目前主要的时间传递技术，均可以实现纳秒量级的时间传递频度，而微博双向法由于使用双向差分极大地消除了大气传播误差，可以实现比GNSS共视法更优的亚纳秒级的时间传递精度。\n\n\n\n技术分类\n基本原理\n主要精度限制\n系统实例\n\n\n\n星地微波双向法\n卫星和地面站在本地秒脉冲到来时刻向对方发送测量信号，并触发本地时间间隔计数器开始计数。当接收到对方发送的传递信号后触发本地时间间隔计数器停止计数。然后交换数据，完成时间比对\n卫星、地面站设备时延误差\nBDS\n\n\n星地激光双向法\n地面激光站向卫星发射激光测距信号，经卫星接受并反射后由激光站进行距离测量。获得的距离测量值对伪距进行校正得到钟差\n云层遮挡、激光器伺服程度\nGalileo系统、GLONASS、BDS\n\n\n倒定位法\n4个以上监测站同时接收卫星测距信号，获得的测距值通过统一处理得到卫星与监测站的钟差。钟差解算基本原理与导航接收机钟差解算类似\n大气传播时延、地面站设备时延\nGPS、Galileo系统\n\n\n应答式雷达法\n应答式雷达精确测量卫星与监测站的距离，使用该距离值对监测站测得的卫星到监测站的单项伪距进行修正，得到二者的钟差\n大气传播时延，设备时延\nGLONASS\n\n\n站间微波双向法\n与星地微波双向时间同步技术类似，只是信号需要经过同步轨道通信卫星转发\n地面站设备时延误差\nGPS、GLONASS、Galileo系统、BDS\n\n\nGNSS共视法\n需要进行时间同步的地面站同时观测同一颗GNSS卫星，获得的伪距观测值经统一解算得到二者钟差\n大气传播时延、地面站设备误差时延\nGPS、GLONASS、Galileo系统、BDS\n\n\nGNSS全视法\n两地面站分别观测不同的GNSS卫星，获得的伪距观测量经统一解算得到二者钟差。不同卫星间的精密钟差由国际GNSS服务信息获得\n大气传播时延、地面站设备误差时延\n时频实验室\n\n\n站间光纤双向法\n两站在本地秒脉冲时刻向对方发送时间测量光信号，并触发本地时间间隔计数器，开始计数，信号在光纤中采用单纤双向方式传输，当接收到对方发送的信号后触发本地时间间隔计数器，停止计数，由获得的两个观测量解算出钟差\n地面站设备误差时延\n地面站时频实验室\n\n\n2.GPS的时间传递流程（1）分布在全球的各地面站通过GNSS时间传递技术或基于通信卫星的双向时间传递技术与主控站实现高精度的站间时间传递；\n（2）地面站以本站的铯原子钟为参考基准，接收卫星发射的双频伪码测距信号，4个以上的地面站观测量将能够使用倒定位法对卫星进行轨道测量与时间传递；\n（3）主控站以GPS主钟为参考对来自各监测站的观测量进行钟差解算，得到卫星钟与系统主钟之间的钟差；\n（4）卫星钟差数据通过上行注入站注入卫星，再广播给用户使用，并在适当的时候对星上的原子钟进行改正处理；\n3.时间传递设备时延在时间传递设备的各项误差源中，设备的时延误差是最大误差项，它不仅会受到信号传输和处理时延的影响，还会受到环境温度、信号质量、设备状态等因素的影响。时间传递设备时延的绝对量缺乏直接测量手段，一般都是采用高速示波器采集携带设备时延信息的时标信号与1PPS信号进行比较的方法。\n时间传递设备时延的在线测量和校正技术目前主要有两类技术体制：基准差分法和模拟器转发法。\n基准差分法使用已标定绝对时延值的时间传递设备作为基准设备与被测发射设备组成测量环路，通过对环路时延进行测量，可获得被测设备的时延值，并根据校正算法对其进行改正。模拟转发器通过在时间传递地面站增加模拟转发器和相应测量设备，在地面站增加了额外的测量环路，通过多环路测量值的联合解算获得时间传递设备时延值。\n4.时间频率指标\n1.频率稳定度是对时钟输出频率受噪声影响而产生的随机起伏程度的量化描述。由于原子钟除了白噪声之外还会受到低频分量丰富的调频闪变噪声和调频随机游走噪声的影响，对于不满足平稳遍历条件的能量谱噪声，标准方差的估计会随着采样个数的增加而发散。因此目前比较常用的用于描述频率稳定度的方法是阿伦方差。\n广义阿伦方差表达式：$$\\sigma^{2}\\left(N,T,\\tau\\right)=\\lim_{m\\to\\infty}\\frac{1}{m}\\sum_{j=1}^{m}\\left[\\frac{1}{\\left(N-1\\right)}\\sum_{i=1}^{N}\\left(y_{i}-\\overline{y}{N}\\right)^{2}\\right],\\quad\\overline{y}{N}=\\frac{1}{N}\\sum_{i=1}^{N}y_{i}$$式中：表示采样个数，表示采用周期，表示采样时间，为测量组数。首先分组测量每一组内的采样数据的标准方差，然后对所有组的标准方差求平均值。\n狭义艾伦方差表达式：\n\n\n2.由于受到内在因素和外部环境因素的共同影响，时钟的实际输出频率在一定范围内变化，频率准确度是用来描述时钟的实际输出频率相对于其标称频率的偏差。频率准确度表达式为：式中：为频率准确度，为被测时钟的实际频率，​为标称频率。\n\n\n3.频率偏差描述的是两台时钟输出频率的相对偏差，定义为：\n\n\n4.时间间隔误差是时间传递的最基本观测值，记为时间间隔误差（TIE），测量时钟或数据的每个活动边沿与其理想位置有多大偏差，反映了周期抖动在各个时期的累积效应。\n\n\n5.最大时间间隔误差反应了设备连续运行期间最大的时间变化。\n\n5.GNSS时间传递GNSS伪距观测方程可以写成如下形式：由于接收机本地时间与卫星导航系统时间之间存在不一致，引起伪距存在偏差，因此，利用GNSS伪距进行时间传递的本质就是基于伪距观测量估计相应的时间偏差， 从而最终实现接收机本地时间与GNSS时间的一致。\nGNSS时间传递技术包括GNSS单向时间传递、GNSS共视时间传递、GNSS全视时间传递和精密单点定位时间传递。\n\nGNSS单向时间传递基于接收机测量的GNSS伪距观测量，对卫星和接收机之间的电离层延迟、对流层延迟，卫星钟差等进行改正之后再减去站星之间的几何距离，从而得到接收机钟和 UTC 的钟差，即将 UTC 的时间传递到了接收机钟。GNSS单向时间传递的授时精度可以达到 50ns 。\nGNSS共视时间传递基于地面站的两台接收机同时观测同一颗GNSS卫星，此时根据GNSS单向时间传递的原理可以得到两台接收机与GNSS时间的钟差，再将两台接收机的钟差相减得到两台接收机之间的相对钟差。GNSS共视时间传递的本质就是通过站间差分来进一步消除卫星钟差、卫星轨道偏差，对流层误差、电离层误差等，可以实现纳秒级的授时精度，但是需要其中一台接收机的时间足够精确，而且非常依赖两台接收机可以共视的卫星数目，在时间传递的时候必须严格依据共视时刻表进行观测。\nGNSS全视时间传递基于两台接收机独立观测GNSS卫星，在 IGS 的事后精密轨道和精密钟差的支持下利用伪距解算接收机钟与 IGST 的时间偏差，之后两台接收机再基于该偏差进行求差，从而得到两台接收机之间的时间偏差。GNSS全视时间传递不受共视卫星数目的限制，但是依赖于精密轨道和精密钟差产品，而且需要在全视比对站间建立数据交换链路，存在一定的滞后性。\n精密单点定位时间传递和GNSS全视时间传递类似，但是引入了载波相位观测值来提高精度，可以实现优于 1ns 的时间传递，但是为了解算出载波相位整周模糊度，实时性较差。\n\n二、星地时间传递技术卫星时钟间的不同步会引起用户伪距和载波观测量存在相应偏差，进而导致最终定位结果不收敛于接收机的真实位置。在导航、定位和授时应用中，卫星时钟不同步对其性能的影响主要和两个因素有关：时间传递精度和卫星钟差预报模型。\n1.卫星钟差对定位精度的影响机理按照伪距定位的原理，对一组伪距观测方程进行展开得到：用矩阵表示为：最后根据最小二乘法可以得到：在上式中分解出卫星钟差的影响可以得到：该式描述的是基于最小二乘的单点静态定位误差与卫星钟差间的数学关系。\n2.卫星导航系统时间传递流程在一定时间段内，钟差数据体现了该时间段内的卫星钟特性的变化规律，是产生钟差预报参数的基本依据。由于时间传递观测量中含有多种噪声，钟差数据的生成首先需要经过平滑滤波操作以减弱这些噪声，依据当前时刻之前的历史钟差数据，通过构建合理的钟差模型，可以对未来一段时间段内的钟差变化进行预测。\n卫星钟的钟差预报对两次时间传递操作间隔内的钟差变化进行估计和预测。为了避免频繁的物理调整操作，通常在一定的钟差范围内仅使用钟差预报的方法对卫星钟差实现数学补偿。由于钟差数据相关性随着于预报时间的增长而下降，导致残差随预报时间的增长而增大，为了获得好的钟差预报性能，可以增加时间传递操作和上行注入的频率。\n另一方面，钟差预报是一种数学补偿方法，并不能减小卫星钟差随着时间的累积，随着数据龄期的增加，卫星钟差的累积将可能增大到毫秒级。此时应采用物理调整操作使钟差归零。\n站间时间传递、星地时间传递、钟差预报以及卫星钟调整组成了卫星导航系统时间传递的环路。\n\n3.星地时间传递方法（1）倒定位法倒定位法通过多个地面站同时观测同一颗卫星实现对卫星的精确定位和钟差解算。具体流程为：\n\n通过GNSS共视或卫星双向时间传递实现系统内所有地面观测站间的时间传递；\n超过 4 个地面观测站同时对同一颗卫星进行伪距观测；\n各观测站通过通信链路将获得的观测量传送至系统中心站；\n中心站对所有观测量进行解算处理获得卫星的精密星历；\n使用精密星历和观测站精确站址求解出精密站星距；\n将对应时刻的伪距和精密站星距作差获得卫星钟差序列。\n\n倒定位法仅利用单向伪距观测量进行钟差解算，其误差来源包括电离层误差，对流层误差和相位中心偏差。该方法可以同时获得精密星历和精密卫星钟差，但是对地面站和卫星间的几何布局要求较高，需要全球均匀布站以实现良好的几何精度衰减因子。\n（2）应答式雷达辅助法地面观测站的雷达发送的测距信号经卫星应答机相干转发后由观测站接收，测量获得不包含钟差的星地空间距离。该距离值与观测站获得的伪距相减得到与卫星和地面站间的钟差。具体流程为：\n\n通过GNSS共视或者TWSTFT时间传递实现系统内所有地面观测站的时间传递；\n观测站在其对卫星的跟踪时间段内对卫星持续发射雷达测距信号；\n卫星连续播发导航信号，同时对接收到的观测站信号进行相干转发；\n观测站接收卫星播发的导航信号和转发的雷达信号，完成空间距离和伪距测量；\n基于测得的空间距离和伪距求解卫星与观测站间的钟差。\n\n由于单向伪距和雷达测距值是通过两套不同的测量系统获得的，这两个测量通道间的时延差也会被引入钟差解算结果中，因此需要事先确定将这两个时延差。应答式雷达辅助法的误差源主要包括：设备时延误差，电离层延迟和对流层延迟。\n（3）星地无线电双向时间比对地面站和卫星在某一时刻互发信号，同时开始计数，最后以接收到对方的信号作为计数器的关门信号，卫星再将自己的观测数据发送给地面站，这样两组观测数据中就包括了卫星和地面站间的星地钟差，以及非常接近的传播过程中的大气延迟等。\n\n设地面站 A 和卫星 S 分别在自己的钟面时  (对应时间  ) 和  ( 对应时间  ) 时刻互发信号，该信号也分别作为地面站 A 和卫星 S 时间间隔计数器的开门信号，经过信号传播时延  和  后，分别被卫星 S 在自己钟面时  ( 对应坐标  ) 和地面站 A 在自己钟面时  ( 对应坐标时 ) 时刻接收，并作为卫星 S 和地面站 A 时间间隔计数器的关门信号，地面站 A 和卫星 S 的时间计数器因此分别得到两个观测量  和 ，卫星将自己的观测数据发送给地面站，经两个观测数据求差就能获得高准确度的星地钟差。根据星地无线电双向时间比对的基本原理，有  和  。\n令​分别为地面站 A 和卫星 S 时间计数器的观测量，则有基于上式可得： 和 ​ 表示地面站 A 与卫星 S 之间信号传播时延，可以详细描述地表示为：\n\n（4）星地激光双向时间比对地面观测站和卫星的秒脉冲由于钟差  的存在并不一致，首先由地面观测站在时刻 A 向卫星发射激光信号，该信号到达卫星后卫星记录该信号到达时刻  B 与卫星的秒脉冲的时间间隔 ，信号被反射回地面站后地面站再记录地面信号发射时刻 A 与地面站秒脉冲的时间间隔 。若地面站和卫星的时钟完全对齐，则两者分别记录的秒脉冲时间间隔应该一致。\n\n\n由上图可见，卫星时和地面时之间的钟差可以用秒脉冲之间的时间间隔描述，星地激光双向时间比对中主要误差包括大气时延误差、激光传播时延测量误差、星历误差、地面站位置误差、地球自转误差和设备时延误差等。大气时延中的电离层延迟对星地激光双向时间比对结果的影响与测站激光信号往返电离层延迟的一半，以及激光上行信号的电离层延迟之差有关。由于激光信号的频率很高，电离层延迟对单程激光结果的影响为 ​ ，可以忽略。卫星星历误差和地面站位置误差对激光双向测距的一半，以及激光上行测距的影响基本相同，因此经两者求差后，基本消除了对流层延迟的影响。\n星地激光双向时间比对具有精度高、系统误差少等优点，但是受天气影响较大，一般通过无线电时间比对实现星地时间同步，而星地双向激时间比对常用来进行外部比对和校核。\n三、卫星双向时间频率传递技术卫星双向时间频率传递技术（TWSTFT）是卫星导航系统中实现远距离地面站之间时间频率传递的一种重要手段。\n地面测站 A 和 B 在各自的本地时刻   和  发送测距信号，信号发送时刻会触发 A、B 两地的时间间隔计数器开始计数。A 站发出的上行信号经通信卫星转发后变为下行信号，该信号在某个时刻  被 B 站接收并触发 B 站的时间间隔计数器结束计数，根据 A、B 两站的时间间隔计数器的读数  和 ，可以得出 A 站发送的测距信号的路径传播时延  。同样， B 站发出的测距信号经通信卫星转发后在某时刻  被 A 站的接收设备接收，并触发 A 站时间间隔计数器结束计数，得到 B 站发送的测距信号的路径传播时延。\n\n\n地面站A 、B 卫星双向时间频率传递所获得的钟差可以表示为：联立得到：对各种误差项详细展开后得到站间卫星双向时间传递的函数模型为：该式等号右边第二项表示 A、B 两站的时间信号在空间传播引起的几何距离时延差；第三项表示 A，B 两站发射，接收设备的时延差；第四项表示 A，B 两站时间信号的电离层误差时延差；第五项表示 A，B 站时间信号的对流层误差时延差；第六项表示 A，B 两站时间信号的卫星转发时延差。\n\n\n\n\n四、GNSS时间传递技术GNSS 共视时间比对和GNSS全视时间比对主要涉及到GNSS导航信号处理、数据处理、数据传输和比对处理计算等多层次信号和信息处理的内容，具体包括电离层、对流层等误差模型和改正方法，时间比对模型，数据预处理，参数估计和时延校准等。\n1.GNSS时间比对模型（1）时间比对原型GNSS时间比对的基本原理如下图所示，首先定义本地钟相对于参考时间(GNSS 时间或 IGST 等)的钟差  。\n其中： 表示  时刻对应的本地时间；表示  时刻对应的参考时间。因此在相同的 GNSS 时刻 ， A、B 两站对 GNSS 时间的钟差分别可以记为：为消除观测中的共同误差，两式相减得：$$\\Delta T_{{AB}}\\left(t\\right)=\\Delta T{A}\\left(t\\right)-\\Delta T{B}\\left(t\\right)=T{A}\\left(t\\right)-T{B}\\left(t\\right)$$式中 $\\Delta T{\\Lambda{\\mathrm{R}}}(t)即为t$ 时刻 A、B 两站之间的钟差。\n\n\n（2）共视时间比对模型GNSS共视时间比对本质上以相同的共视卫星作为共同的参考，位于异地的待同步地面站按照共视时间表接收卫星信号，通过数据处理获得本地时间实验室与观测卫星间的相对钟差，通过比较两站的相对钟差，即可获得异地同步地面站间的钟差。\n在某一时刻 A 和 B 卫星按照共视表同时对卫星进行观测，得到的伪距观测值为：其中： 和  分别为 A 站和 B 站的伪距观测量； 为光速； 和 分别为 A 站、B 站接收到卫星信号的本地接收时刻； 和  分别为 A 站和 B 站按收卫星信号的系统时刻； 为卫星发射信号时的本地时刻；为卫星发射信号时的系统时刻。\n对上式中包含的各种误差项进行展开可以得到：\n（3）全视时间比对模型GNSS全视时间比对是在精密轨道和精密钟差的支持下，基于伪距、载波相位解算本地参考时间与精密钟差对应的参考时间  之间的偏差，对该偏差进行求差即可得到各地面站之间的时间偏差 。\n一般的全视时间比对模型以伪距观测值为基础，利用双频组合消去电离层延迟，在精密单点定位时间比对中则以双频载波和伪距消电离层组合为观测量，降低伪距噪声的影响。具体解算流程同精密单点定位。\n","categories":["GNSS","阅读笔记"],"tags":["GNSS"]},{"title":"表格布局检测与数据提取解决方案","url":"/2025/05/05/%E8%A1%A8%E6%A0%BC%E5%B8%83%E5%B1%80%E6%A3%80%E6%B5%8B%E4%B8%8EOCR%E8%AF%86%E5%88%AB/","content":"表格布局检测与数据提取解决方案一、方案逻辑（一）表格图像预处理对表格图像进行预处理，以增强表格的可见性并减少干扰。\n（二）表格布局检测利用 Layout-Parser 库中的 Detectron2LayoutModel 模型，PaddleOCR 的 PP-Structure 或者 DocLayout-YOLO 对预处理后的图像进行表格布局检测，识别出表格区域。\n（三）表格识别与数据提取将检测到的表格区域裁剪出来，然后使用开源的 OCR 工具（如 Tesseract-OCR）对裁剪后的表格区域进行文字识别，提取出表格中的数据内容。\n（四）数据整理与保存将提取到的表格数据整理为合适的数据结构，再利用 Pandas 库将其保存为 Excel 文件。\n二、环境搭建因为 Detectron2 并不支持 Windows 系统，使用 Docker 搭建工作环境。\n\n基础配置：以 python:3.8-slim-buster 镜像为基础，设置非交互模式并创建工作目录。\n创建工作目录：创建并切换到 /app 目录，作为后续操作的工作目录。\n系统依赖安装：更新系统包列表，安装必要的系统依赖（如 HTTPS 支持、证书、编译工具、Git、OCR 工具等），并清理缓存。\n源切换：将 APT 源切换为 HTTPS，使用官方安全仓库。\nPython 依赖安装：升级 pip，安装指定版本的 PyTorch、torchvision、Pillow、LayoutParser 及其 OCR 依赖，以及从源安装 Detectron2。\n项目文件复制：将当前目录下的项目文件复制到容器的 /document-layout-detection 目录。\n使用 VSCode 远程连接配置的 Docker 容器进行开发和测试\n\nDockerfile 与 最终搭建完成的工作环境如下所示：FROM python:3.8-slim-buster# 非交互模式ENV DEBIAN_FRONTEND=noninteractiveWORKDIR /app# 1) 安装 HTTPS 支持与证书，清理缓存RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\        apt-transport-https \\        ca-certificates \\    &amp;&amp; rm -rf /var/lib/apt/lists/*# 2) 切换所有 APT 源到 HTTPS，并使用官方安全仓库路径RUN sed -i 's|http://deb.debian.org|https://deb.debian.org|g' /etc/apt/sources.list \\    &amp;&amp; sed -i 's|http://security.debian.org|https://security.debian.org|g' /etc/apt/sources.list# 3) 再次更新并安装系统依赖RUN apt-get update &amp;&amp; apt-get install -y --no-insatall-recommends \\        build-essential \\        git \\        poppler-utils \\        tesseract-ocr \\        libgl1 \\    &amp;&amp; rm -rf /var/lib/apt/lists/*# 4) 安装 PyTorch 和 torchvision（CPU 版本）RUN pip install --no-cache-dir --upgrade pip \\    &amp;&amp; pip install --no-cache-dir \\        torch==1.12.1+cpu \\        torchvision==0.13.1+cpu \\        -f https://download.pytorch.org/whl/torch_stable.html# 5) 安装其他 Python 包RUN pip install --no-cache-dir Pillow==9.5.0RUN pip install --no-cache-dir \\        layoutparser[ocr] \\        \"git+https://github.com/facebookresearch/detectron2.git@v0.6\"# 6) 复制项目文件COPY . /document-layout-detection# 7) 设置默认命令\n\n\n三、运行结果1.Detectron2LayoutModel 模型对目标图片进行表格布局检测的结果：Figure (31.291555404663086, 48.364593505859375, 2329.773193359375, 2434.570068359375) NoneFigure (19.404632568359375, 1747.72509765625, 2302.79443359375, 3780.911865234375) None\n\n\n\n2.Tesseract OCR 从检测到的表格中提取中文文字并保存到 Excel 文件：\n四、实验代码详见附件中 layout_detection.py 和 table_extraction.py。\n","categories":["阅读笔记"],"tags":["新玩具","深度学习"]},{"title":"论文笔记-2024-03-低轨增强精密单点定位技术与应用研究","url":"/2024/05/16/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-2024-03-%E4%BD%8E%E8%BD%A8%E5%A2%9E%E5%BC%BA%E7%B2%BE%E5%AF%86%E5%8D%95%E7%82%B9%E5%AE%9A%E4%BD%8D%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6/","content":"1.低轨导航增强 GNSS发展综述（1）GNSS系统的缺点：\n基本导航服务的定位精度通常只有10m，无法满足高精度定位需求；\n电磁信号依赖环境状况，不能在室内、森林和城市峡谷等提供连续可靠的服务；\n信号功率低，容易受到干扰和欺骗；\n\n（2）信息增强与信号增强1）信息增强信息增强是指通过修正卫星导航定位系统的误差来提高导航定位精度和可靠性等的一种技术方式。\n信息增强不提供额外的距离观测量，只提供消除GNSS误差的修正信息和完好性信息。具体有地基增强和星基增强方式，网络 RTK、星基差分等技术是典型的信息增强系统。\n2）信号增强信号增强是指通过除导航卫星以外的平台来发射导航信号，用户同时可以接收导航卫星系统本身的导航信号，以及其他额外的导航信号，进而提高导航定位精度和可用性的方法。\n信号增强需要信号发射机来为用户提供测量信息，可以与GNSS联合定位或者独立定位。同样有地基增强和星基增强两种方式。\n在具体应用中，信号增强可以从根本上增强GNSS的信号强度，拥有比信息增强更广泛的应用场景。\n（3）GNSS增强系统现状不管是信息增强还是信号增强，采用地基增强GNSS的系统的覆盖范围有限,无法提供全球无缝统一的高精度服务，传统的以信息增强为主的星基增强系统均以GEO卫星作为增强信息的发射平台，但是GEO卫星轨道资源有限，转发通信时延高，并且不适用高纬度地区。\n传统以高轨卫星为信号播发载体建成的星基增强系统，其信号增强的贡献比较有限，在信号遮蔽区域和室内仍然无法提供连续可用的导航定位服务。\n（4）低轨星座的特点与中高轨卫星相比，低轨卫星的特点有距离地面近、运行速度快、受到的摄动影响大。\n1）低轨星座优势\n轨道低，体型小，发射成本低，信号传播距离短，自由空间损耗少，有助于改善信号受遮蔽环境下的定位效果，提升抗干扰、防欺骗性能，有利于数据传输和通信；\n低轨卫星运行速度快，多普勒频移现象明显，有利于提高测速精度和基于多普勒观测值的载波相位周跳探测效果，而且由于速度快，低轨卫星的几何图形变化快，使得定位过程中历元间观测方程的相关性减弱，参数的可估性大大增强,有望从根本上解决载波相位模糊度参数收敛和固定慢的问题；\n同时由于轨道低，低轨卫星对重力场更敏感，借助低轨卫星可以反演出更高阶数的地球重力场模型；\n\n2）低轨星座劣势\n由于卫星轨道低，单星地面覆盖范围小，因此需要更多的卫星才能实现全球覆盖，同时对接收机信号捕获性能的要求更高；\n由于受到的大气阻力、地球非球形引力和广义相对论作用更强，低轨卫星的精密定轨与预报难度更大，需要使用更精细的力学模型参数和处理策略进行精密定轨才能达到厘米级精度；\n因为大气阻力更大，低轨卫星需要频繁启动进行轨道维持，所以卫星寿命也更短；\n\n（5）低轨导航增强带来的机遇1）联合定轨传统的GNSS卫星精密定轨是利用全球均匀分布的大量地面监测站，对导航卫星进行伪距和载波相位测量,再结合精确的轨道动力学模型和误差改正模型进行数据处理，确定 GNSS卫星的精密轨道。\n但是我国难以实现全球均匀布站，而使用低轨卫星参与高中低卫星联合定轨，可以弥补地面站的不足，极大增强GNSS卫星跟踪网的图形强度，使轨道和力模型参数估计得更准确，实现区域监测站条件下的导航卫星精密定位；\n2）快速精密定位低轨卫星几何图形的快速变化有助于模糊度的快速解算，而模糊度快速解算是实现快速精密定位的关键；\n3）空间天气检测更多的可用卫星,可以提取出数量更多的倾斜路径延迟，短时间内能够提供更多有效的观测数据，有利于实现快速的大气建模，单位时间内低轨卫星划过的轨迹长，高度角和方位角变化大，使得有效监测范围扩大；\n4）室内定位低轨卫星距离地球表面近，地面接收信号强度高，有利于改善信号受遮蔽环境下的定位性能；\n2.GNSS精密单点定位及其质量控制的理论和方法（1）精密单点定位的数学模型的发展早起的PPP研究主要针对其函数模型，最常用的函数模型是基于载波和伪距观测量的消电离层组合模型，但是该模型也将观测量的噪声扩大了倍，相比之下，半和模型则将伪距观测量的噪声减小了一半。\n研究者已经证明了基于非组合原始观测量与基于组合观测量的​数据处理方法在理论上是等价的，两种处理方式的定位精度相当，但是通常非组合模型的收敛速度较慢，可以使用较高精度的电离层延迟作为先验信息约束来加快收敛速度。目前，对于单系统 PPP，消电离层组合 PPP 模型、星间单差 PPP 模型与基于原始观测量的 PPP 模型在理论上是等价的，实际定位结果差别也不大。\n（2）多模多频组合精密单点定位多系统之间的组合定位需要保证系统的性能接近，当 GLONASS 卫星的轨道和钟差质量较差时，GPS/GLONASS 组合 PPP 的定位精度反而不如单 GPS PPP。在非组合 PPP 模式下，通过 GPS/GLONASS 原始观测量的组合，不仅能够加快 PPP 的收敛速度，而且能够直接获得电离层延迟和差分码偏差。\n在多系统组合随机模型方面，对于不同系统间观测量的定权通常采用经验模型。一方面，通过分析不同系统间伪距和载波相位的验后残差，进而给出系统间伪距以及相位观测量的权比。另一方面，验后残差主要受观测量中未模型化误差的影响，如精密轨道和钟差的残余误差、多路径等，因此采用不同分析中心的星历产品其验后残差大小不同。\n（3）区域增强精密单点定位当前对于PPP的快速初始化主要基于两种思路：一方面，类似网络RTK，通过区域的参考站网，估计实时精密卫星轨道和钟差、非差的大气延迟改正以及FCB产品，然后播发给用户端进行改正，以实现PPP的快速初始化。另一方面，通过全球或区域稀疏的参考站估计出FCB产品，结合多模多频GNSS的发展，实现用户端多系统组合PPP的快速初始化及模糊度固定，获得相比单一系统PPP及PPP-AR更快的初始化效果。\n（4）精密单点定位的观测方程GNSS 原始伪距和载波相位观测方程可以表示为：式中，、分别为原始载波相位和伪距观测量，以米为单位；上标  为卫星编号， 为某一卫星导航系统，下标为测站编号，表示某一频率。为信号发射时刻的站星几何距离，为接收机钟差，为卫星钟差，为卫星轨道误差，为对流层延迟， 为 L1 频点的电离层延迟， 和分别为接收机端和卫星端伪距和载波相位硬件延迟偏差，为模糊度参数，为第个频点的载波波长。、分别为载波相位和伪距的观测噪声以及其它未模型化误差。\n1）硬件延迟的问题接收机端和卫星端的伪距硬件延迟偏差无法直接与其他参数分离，分别与接收机钟差和卫星钟差合并估计。在PPP模型中载波观测量中含有模糊度参数，与钟差参数线性相关，因此载波观测方程的钟差参数的初始值由伪距观测量估计获得，此时伪距硬件延迟将随钟差参数引入载波观测方程中。\n2）无电离层组合的观测方程将载波相位中的硬件延迟偏差归入模糊度参数中，此时以L1 和 L2 双频消电离层组合为例，伪距和载波观测方程表示为：由于 IGS 分析中心提供的精密卫星钟差采用消电离层组合观测量生成，因此精密钟差产品中含有消电离层组合硬件延迟，此时与消电离层组合 PPP 模型中的卫星钟差相一致，可以采用精密钟差产品直接改正，否则应进行相应的 DCB 改正。\n3）非差非组合观测方程由于 IGS 分析中心提供的精密卫星钟差产品包含消电离层组合的硬件延迟，为了能够直接采用精密星历进行改正，此时在原始伪距和载波观测量的基础上，将非差非组合 PPP 模型中的卫星钟差和接收机钟差与消电离层组合伪距硬件延迟进行合并，其余项并入电离层延迟和模糊度参数，此时非差非组合 PPP 观测方程表示为：对于 L1 和 L2 双频观测量，电离层延迟中的硬件延迟偏差可以进一步表示为：其中，，，则 L1 频点估计的电离层斜向延迟估值为：$$\\hat{I}{1,r}^{s,j}=I{1,r}^{s,j}-\\frac{1}{1-\\gamma_{2}}DCB_{P1P2}^{s,r}+\\frac{1}{1-\\gamma_{2}}DCB_{P1P2}^{s,j}令：\\begin{aligned}&amp;\\delta\\hat{t}{r}^{s}=\\delta t{r}^{s}+d_{IF,r}^{s} ; \\delta\\hat{t}^{s,j}=\\delta t^{s,j}+d_{IF}^{s,j} \\&amp;\\gamma_{i}\\hat{I}{1,r}^{s,j}=\\gamma{i}I_{1,r}^{s,j}-\\frac{\\gamma_{i}}{1-\\gamma_{2}}(DCB_{P1P2}^{s,r}-DCB_{P1P2}^{s,j}) \\&amp;\\lambda_{i}^{s}B_{i,r}^{s,j}=2(d_{iF}^{s,j}-d_{iF,r}^{s})+(d_{i,r}^{s}-d_{i}^{s,j})+(b_{i,r}^{s}-b_{i}^{s,j})+\\lambda_{i}^{s}N_{i,r}^{s,j}\\end{aligned}此时非组合观测方程为：P_{i,r}^{s,j}=\\rho_{r}^{s,j}+\\delta\\hat{t}{r}^{s}-\\delta\\hat{t}^{s,j}+d{or b}^{s,j}+T_{r}^{s,j}+\\gamma_{i}\\hat{I}{1,r}^{s,j}+\\varepsilon(P{i,r}^{s,j})\\L_{i,r}^{s,j}= \\rho_{r}^{s,j}+\\delta\\hat{t}{r}^{s}-\\delta\\hat{t}^{s,j}+d{orb}^{s,j}+T_{r}^{s,j}-\\gamma_{i}\\hat{I}{1,r}^{s,j}+\\lambda{i}^{s}B_{i,r}^{s,j}+\\varepsilon(L_{i,r}^{s,j})$$式中标记详细说明：\n\n\n上标为卫星编号，为某一卫星导航系统，下标为测站编号，表示为某一频率；\n表示卫星导航系统的编号为的卫星，在频率下和接收机的伪距观测值；\n为经过天线相位中心改正的信号发射时刻的站星几何距离；\n$\\delta\\hat{t}r^s表示吸收了无电离层组合接收机伪距硬件延迟d{IF,r}^s$的接收机钟差；\n表示吸收了无电离层组合卫星伪距硬件延迟的卫星钟差；\n表示卫星导航系统的编号为的卫星的卫星轨道误差；\n表示卫星导航系统的编号为的卫星和接收机之间的对流层延迟；\n表示电离层频点系数；\n$\\hat{I}{1,r}^{s,j}表示卫星导航系统s的编号为j的卫星在频率下和接收机r之间的电离层延迟，并且吸收了和在接收机和卫星上的差分码偏差，其中\\frac{1}{1-\\gamma}DCB{P1,P2}^{s,j}=T_{GD}$；\n表示卫星导航系统的编号为的卫星在频率下和接收机之间的吸收了载波硬件延迟、接收机硬件延迟和无电离组合硬件延迟的整周模糊度；\n\n\n这种组合方式将载波观测方程和伪距观测方程的接收机钟差、卫星钟差统一起来，将硬件延迟分别吸收到接收机钟差、卫星钟差、电离层延迟和整周模糊度。\n3.GNSS精密单点定位技术及应用进展精密单点定位指的是用户利用一台GNSS接收机的载波相位和伪距观测值，采用高精度的卫星轨道和钟差产品，并通过模型改正或参数估计的方法精细考虑与卫星端、信号传播路径及接收机端有关误差对定位的影响，实现高精度定位的一种方法。\n（1）PPP发展过程\n浮点解到固定解，主要围绕如何固定非差模糊度\n后处理到实时，主要围绕高精度实时卫星轨道和高频卫星钟差产品处理\n单系统到双系统再到多系统集成，主要围绕系统间偏差、频间偏差和频内偏差\n\n1）实数解PPP进展非差消电离层组合模型——半和模型——星间单差消除接收机钟差——非差非组合PPP\n总体上讲，双频GPS实数解PPP的理论、模型和算法已经基本成熟，关键技术已经基本解决，目前已进入实用化阶段。\n2）固定解PPP进展整周模糊度与硬件延迟偏差高度线性相关，很难直接分离。整周模糊度固定的核心是分离接收机端和GNSS卫星端的初始相位和硬件延迟偏差，进而恢复非差模糊度的整数特性。为此，先后提出了UPD方法、整数钟方法和钟差去耦等方法。\n\nUPD方法：采用星间单差模型，使用全球大约180个GPS跟踪站的观测数据逐站取平均计算卫星端星间单差的未检校的相位延迟，实现了固定星间单差模糊度的PPP定位。PPP用户使用这套UPD产品即可通过后处理实现星间单差模糊度的整数固定解。\n宽巷固定方法：根据宽巷模糊度和无电离层组合模糊度，使用UPD产品改正窄巷模糊度之后使用LAMBDA方法搜索非差窄巷模糊度。\n整数钟方法：首先估计非差宽巷UPD改正数，然后在钟差估计过程中，利用卫星钟差参数吸收非差窄巷UPDs。使用IRC卫星钟差改正数，PPP求解的模糊度参数具有整数特性，进而可以采用传统的模糊度固定方法进行确定。\n钟差去耦方法：在钟差去耦模型中，伪距对应的GPS卫星钟差由伪距确定，而载波相位对应的GPS卫星钟差由载波相位确定，载波相位模糊度不再受伪距硬件延迟的影响，通过对伪距和载波使用不同的卫星钟改正数恢复非差模糊度的整数特性。\n\n3）实时PPP进展目前国际上GFZ、CNES等机构正在研发实时精密单点定位系统，并在系统开发方面取得了一些初步成果，实时PPP在平面方向的定位精度为5cm，高程方向为10cm左右。\n从当前全球已实现的商业化的实时PPP系统来看，限制实时PPP应用的技术瓶颈依然存在，主要表现在已有的实时PPP商用产品的定位初始化时间较长，首次初始化时间以及卫星失锁后的重新初始化时间一般需要20min甚至更长。\n4）PPP-RTK进展基本思想是融合PPP和RTK两种技术的优势，利用局域网观测数据，精化求解相位偏差、大气延迟等参数，重新生成各类改正信息，并单独播发给流动站使用。\n为了保持PPP非差模糊度的快速（3~5min）初始化还具有相当的难度，这也是目前制约厘米级实时PPP应用的技术瓶颈。\n4.低轨互联网系统在导航增强服务中的应用前景及挑战（1）低轨卫星特点\n低轨卫星发射成本低，更新换代容易，通过搭载卫星导航增强载荷可提高导航增强服务。\n低轨卫星在获取更高精度源头数据方面有天然优势：传输路径短，信号损耗低，信号落地强度更高；传输容量大，速度快，用于导航增强可以减少接收星历的时间。\n低轨卫星受力模制难度大，轨道参数模型的预报性能挑战大，低轨卫星信号仅穿越部分电离层导致广播电离层模型参数适应度降低。\n目前低轨卫星互联网系统的发展需要解决的问题包括：电离层等误差模型、低轨卫星构型电离层建模、广播星历参数设计和低轨星座构型等。\n\n（2）国内外部分低轨星座\n\n（3）低轨导航增强关键技术1）广播星历参数设计与中地球轨道和地球静止轨道相比，低轨卫星摄动力更加复杂，运动速度更快。若直接将适用于 MEO或GEO的广播星历用于LEO，很可能会造成参数变化范围超限、拟合精度达不到要求。\n2）低轨星座构型低轨星座构型主要有近圆形轨道和椭圆型轨道。针对全球导航增强主要采用基于近圆形轨道的星座构型，对区域增强导航主要采用椭圆形轨道。\n3）低轨电离层建模利用LEO星载接收机接收GNSS卫星信号，可实现LEO卫星下部和上部的电离层探测。可用卫星越多,提取出的倾斜路径延迟就越多，短时间内可提供的有效观测数据就越多，因此，数量庞大的LEO卫星为建立高精度的电离层模型提供可能。LEO高度角和方位角单位时间内变化大的特点使得有效监测范围扩大。\n（4）低轨互联网系统的应用及效益\n当前卫星导航系统建设仅能布设有限数量且分布有限的地面监测站，因此对导航卫星的跟踪测量非常有限，制约了GNSS实时导航电文精度的提高和完好性服务能力的提升。星载GNSS接收机的数据可以参与GNSS卫星的精密定轨与钟差解算，在全球范围内实现电离层观测，提供对GNSS卫星下行信号的全弧段实时多重跟踪测量，为全弧段完好性监测提高可靠的数据源。\n借助LEO卫星的快速变化几何构型可实现精密定位的快速收敛，LEO增强的GNSS精密定位收敛时间可以从0.25h缩短到0.05h。\n低轨卫星实际接收信号的载噪比更高，能以较低的载荷代价提供更高的落地功率，为信号增强系统提供观测量，弥补GNSS导航信号落地功率不足的问题。\n\n5.地面基准站网与星载 GNSS 融合的高中低轨卫星联合精密定轨\n利用地面基准站网和低轨卫星观测的GNSS数据进行高中低轨卫星联合定轨，即联合定轨，包括”两步法“和”一步法“。\n一步法先基于地面站网确定导航星的轨道和钟差，再将导航星轨道视为已知值，利用低轨卫星星载GNSS数据对低轨卫星精密定轨；\n两步法结合地面站网和星载GNSS数据联合处理同时得到导航卫星和低轨卫星的轨道和钟差。\n卫星精密定轨本质上是对卫星轨道的测量数据进行高精度建模，将卫星初始历元轨道等视为待估模型参数，通过调整此类参数使模型能够在某种准则下最佳拟合测量数据的过程。\n\n联合精密定轨函数模型：\n线性化和重组之后为：$$\\begin{aligned}p_{g, \\mathrm{IF}}^s &amp; =\\boldsymbol{e}g^s \\cdot \\boldsymbol{\\varphi}\\left(t, t_0\\right)^s \\boldsymbol{O}0^s+\\mathrm{c} \\cdot\\left(\\delta t_g-\\delta t^s\\right)+Z{g, T r o}^s+\\mathrm{c} \\cdot\\left(b{g, \\mathrm{IF}}-b_{\\mathrm{IF}}^s\\right)+\\varepsilon_{g, \\mathrm{IF}}^s \\p_{\\mathrm{LEO}, \\mathrm{IF}}^s &amp; =\\boldsymbol{e}{\\mathrm{LEO}}^s \\cdot \\boldsymbol{\\varphi}\\left(t, t_0\\right)^s \\boldsymbol{O}0^s-\\boldsymbol{e}{\\mathrm{LEO}}^s \\cdot \\boldsymbol{\\varphi}\\left(t, t_0\\right){\\mathrm{LEO}} \\boldsymbol{O}{\\mathrm{LEO}, 0}+\\mathrm{c} \\cdot\\left(\\delta t{\\mathrm{LEO}}-\\delta t^s\\right)+\\mathrm{c} \\cdot\\left(b_{\\mathrm{LEO}, \\mathrm{IF}}-b_{\\mathrm{IF}}^s\\right)+\\varepsilon_{\\mathrm{LEO}, \\mathrm{IF}}^s \\l_{g, \\mathrm{IF}}^s &amp; =\\boldsymbol{e}g^s \\cdot \\boldsymbol{\\varphi}\\left(t, t_0\\right)^s \\boldsymbol{O}0^s+\\mathrm{c} \\cdot\\left(\\delta t_g-\\delta t^s\\right)+Z{g, T r o}^s+\\lambda{\\mathrm{IF}} \\cdot\\left(B_{g, \\mathrm{IF}}-B_{\\mathrm{IF}}^s+N_{g, \\mathrm{IF}}^s\\right)+\\omega_{g, \\mathrm{IF}}^s \\l_{\\mathrm{LEO}, \\mathrm{IF}}^s &amp; =\\boldsymbol{e}{\\mathrm{LEO}}^s \\cdot \\boldsymbol{\\varphi}\\left(t, t_0\\right)^s \\boldsymbol{O}0^s-\\boldsymbol{e}{\\mathrm{LEO}}^s \\cdot \\boldsymbol{\\varphi}\\left(t, t_0\\right){\\mathrm{LEO}} \\boldsymbol{O}{\\mathrm{LEO}, 0}+\\mathrm{c} \\cdot\\left(\\delta t{\\mathrm{LEO}}-\\delta t^s\\right)+\\lambda_{\\mathrm{IF}} \\cdot\\left(B_{\\mathrm{LEO}, \\mathrm{IF}}-B_{\\mathrm{IF}}^s+N_{\\mathrm{LEO}, \\mathrm{IF}}^s\\right)+\\omega_{\\mathrm{LEO}, \\mathrm{IF}}^s\\end{aligned}$$\n\n\n和表示地面测站和星载接收机伪距观测量的残差（观测值-模型计算值）；\n$\\boldsymbol{e}{\\mathrm{g}}^s和\\boldsymbol{e}{\\mathrm{LEO}}^s$分别表示从地面测站到导航卫星和从低轨卫星到导航卫星方向上的单位向量；\n和分别是导航卫星和低轨卫星的状态转移矩阵；\n表示导航卫星初始状态，主要包括导航卫星在初始历元的位置、速度以及力模型参数；\n$\\boldsymbol{O}{\\mathrm{LEO}, 0}表示低轨卫星在初始历元的位置(x{LEO,0},y_{LEO,0},z_{LEO,0})、速度(v_{LEO,x},v_{LEO,y},v_{LEO,z})$以及力模型参数；\n表示地面测站对流层参数，包括对流层湿分量、南北向对流层梯度参数和东西向对流层梯度参数；\n\n\n对于联合精密定轨，其待估模型参数可表示为：$$X=(O_{0}^{s},O_{\\mathrm{LEO},0},\\delta\\tau_{g},\\delta\\tau_{\\mathrm{LEO}},\\delta\\tau^{s},Z_{g,Tro}^{s},\\tilde{N}{g}^{s},\\tilde{N}{\\mathrm{LEO}}^{s})^{T}$$式中： 分别表示经参数重组后的地面接收机、低轨星星载接收机和导航卫星钟差； $\\tilde{N}{g}^{s},\\tilde{N}{\\mathrm{LEO}}^{s}分别表示经参数重组后的地面和低轨星星载接收机载波相位整周模糊度，吸收了无电离层组合在卫星和接收机端的载波硬件延迟；O_0^s，O_{\\mathrm{LEO,0}}分别表示地面站和低轨卫星的在初始历元的位置、速度和力学模型参数，Z_{g,Tro}^s$表示地面测站对流层参数，包括湿分量和南北向对流层梯度参数，东西向对流层梯度参数。\n式中既含有接收机钟差又含有导航星钟差，无法直接利用相位和伪距观测值同时对两者进行求解，因此需要固定某一卫星钟或接收机钟作为基准钟，然后确定其他接收机和卫星与所选定基准钟间的相对钟差。\n联合精密定轨参数配置：\n\n联合精密定轨算法流程：\n\n当地面测站数量较多且全球分布时， 低轨星对导航星轨道的增强不明显，约为20%; 在测站数量较少且区域分布的情况下，低轨星作为高动态天基测站的优势体现的较为明显，对导航星定轨精度的提升程度可达90%。\n6.基于低轨星座与参考站联合增强的GNSS 多频精密单点定位关键技术研究（1）GNSS精密单点定位函数模式观测模型体现了GNSS观测值与待估参数及各误差项之间的函数关系，是实现PPP精密定位的基础，非差伪距和载波的原始观测方程可表示为：其中：上标 和下标， 分别表示卫星、接收机和频率； 和 分别为伪距观测值和载波观测值；表示站星距； 表示信号传播路径上的对流层延迟； 表示真空中的光速；和分别表示接收机钟差和卫星钟差；表示信号传播路径上对应频率1的电离层延迟； 为频率 相对于频率1的电离层延迟放大因子；表示频率 的载波波长；表示相位整周模糊度；和分别表示接收机端和卫星端对应频率 的载波相位硬件延迟； 和 分别表示接收机端和卫星端对应频率 的伪距硬件延迟；和 分别表示包括多径在内的伪距和载波观测值噪声。\n1）传统双频无电离层组合模型传统双频无电离层组合模型主要对卫星钟差和接收机钟差进行重参化，在卫星端，依据IGS的处理规范，在精密钟差的估计过程中，通常采用某一特定的无电离层组合，卫星端无电离层组合形式的伪距硬件延迟以及随时间变化的相位硬件延迟被最终的钟差参数吸收；而在接收机端，钟差基准取决于伪距观测值，无电离层组合形式的伪距硬件延迟可与真实接收机钟差合并为一个参数。$$\\begin{cases}c\\cdot\\tilde{t}^s=c\\cdot t^s+d_{\\mathrm{IF}{12}}^s+\\left(\\alpha{12}\\cdot\\lambda_1\\cdot\\tilde{b}1^s+\\beta{12}\\cdot\\lambda_2\\cdot\\tilde{b}2^s\\right)\\c\\cdot\\tilde{t}{r_{12}}=c\\cdot t_r+d_{r,\\mathrm{IF}_{12}}\\end{cases}$$\n$$\\begin{cases}\\alpha_{ij}=f_{i}^{2}\\Big/\\Big(f_{i}^{2}-f_{j}^{2}\\Big)\\beta_{ij}=-f_{j}^{2}\\Big/\\Big(f_{i}^{2}-f_{j}^{2}\\Big)\\d_{\\mathrm{IF}{ij}}^{s}=\\alpha{ij}\\cdot d_{i}^{s}+\\beta_{ij}\\cdot d_{j}^{s}\\d_{r,\\mathrm{IF}{ij}}=\\alpha{ij}\\cdot d_{r,i}+\\beta_{ij}\\cdot d_{r,j}\\end{cases}$$\n对于双频观测组合，重参化之后得到如下函数模型：$$\\begin{cases}P_{r,\\mathrm{IF}{12}}^{s}=\\rho{r}^{s}+T_{r}^{s}+c\\cdot\\tilde{t}{r{2}}-c\\cdot\\tilde{t}^{s}+\\zeta_{\\mathrm{IF}{12}}+e{r,\\mathrm{IF}{12}}^{s}\\L{r,\\mathrm{IF}{12}}^{s}=\\rho{r}^{s}+T_{r}^{s}+c\\cdot\\tilde{t}{r{12}}-c\\cdot\\tilde{t}^{s}+\\lambda_{\\mathrm{IF}{12}}\\overline{N}{r,\\mathrm{IF}{12}}^{s}+\\varepsilon{r,\\mathrm{IF}{12}}^{s}\\end{cases}$$$P{r,\\mathrm{IF}{12}}^s和L{r,\\mathrm{IF}{12}}^s分别为无电离层组合伪距观测值和载波观测值；\\lambda\\mathrm{IF_{12}}$ 为无电离层组合的波长；\n$\\overline{N}{r,\\mathrm{IF}{12}}^s$为重参化的无电离层组合模糊度，包含无电离层组合的接收机端和卫星端载波相位硬件延迟和伪距硬件延迟，以及L1频率和L2频率的整周模糊度；\n为伪距方程残余的硬件偏差，包含L1频率和L2频率的载波相位硬件延迟；\n$e_{r,\\mathrm{IF}{12}}^s和\\varepsilon{r,\\mathrm{IF}_{12}}^s$分别为无电离层组合伪距和载波观测值噪声。\n$P_{r,\\mathrm{IF}{12}}^s,L{r,\\mathrm{IF}{12}}^s,\\zeta{\\mathrm{IF}{12}}^s和\\overline{N}{r,\\mathrm{IF}{12}}^s的具体表达式如下：$\\begin{cases}P{r,\\mathrm{IF}{12}}^s=\\alpha{12}\\cdot P_{r,1}^s+\\beta_{12}\\cdot P_{r,2}^s\\[2ex]L_{r,\\mathrm{IF}{12}}^s=\\alpha{12}\\cdot L_{r,1}^s+\\beta_{12}\\cdot L_{r,2}^s\\end{cases}\\$$\n$$\\zeta_{\\mathrm{IF}{12}}=\\alpha{12}\\cdot\\lambda_{1}\\cdot\\tilde{b}{1}^{s}+\\beta{12}\\cdot\\lambda_{2}\\cdot\\tilde{b}{2}^{s}\\\\overline{N}{r,\\mathrm{IF}{12}}^{s}=\\left(N{r,\\mathrm{IF}{12}}^{s}+b{r,\\mathrm{IF}{12}}-\\overline{b}{\\mathrm{IF}{12}}^{s}\\right)-\\frac{d{r,\\mathrm{IF}{12}}-d{\\mathrm{IF}{12}}^{s}}{\\lambda{\\mathrm{IF}_{12}}}$$\n$$\\begin{cases}N_{r,\\mathrm{IF}{12}}^s=\\frac{\\left(\\alpha{12}\\cdot\\lambda_1\\cdot N_1+\\beta_{12}\\cdot\\lambda_2\\cdot N_2\\right)}{\\lambda_{\\mathrm{IF}{12}}}\\b{r,\\mathrm{IF}{12}}=\\frac{\\left(\\alpha{12}\\cdot\\lambda_1\\cdot b_{r,1}+\\beta_{12}\\cdot\\lambda_2\\cdot b_{r,2}\\right)}{\\lambda_{\\mathrm{IF}{12}}}\\\\overline{b}{\\mathrm{IF}{12}}^s=\\frac{\\left(\\alpha{12}\\cdot\\lambda_1\\cdot\\overline{b}1^s+\\beta{12}\\cdot\\lambda_2\\cdot\\overline{b}2^s\\right)}{\\lambda{\\mathrm{IF}_{12}}}\\end{cases}$$\n双频无电离层模型的待估参数向量为：$$X=\\begin{bmatrix}x,y,z,\\tilde{t}{r{12}},T_{r}^{s},\\lambda_{\\mathrm{IF_{12}}}\\overline{N}{r,\\mathrm{IF{12}}}^{s}\\end{bmatrix}$$为了使无电离层组合能够兼容更多频率的观测值，目前主要有两种处理方法：一种是构造一个统一的多频无电离层组合；另一种是根据噪声放大水平，选择较优的多个双频无电离层组合联立进行解算。但需要注意的是，多个双频无电离层组合观测值间通常存在相关性，随机模型也需相应调整。\n（1 统一多频无电离层组合模型建立统一多频无电离层模型首先需确定每个频率对应的组合系数，唯一确定组合系数，通常需要满足以下三个条件：①几何项系数和为1；②消除电离层一阶项延迟；③组合观测值噪声最小，即：根据以上条件确定组合系数之后，可得到统一多频无电离层组合观测模型：$$\\begin{cases}P_{r,\\mathrm{IF}}^{s}=\\rho_{r}^{s}+T_{r}^{s}+c\\cdot\\tilde{t}{r}-c\\cdot\\tilde{t}^{s}-D{\\mathrm{IF}}^{s}+\\zeta_{\\mathrm{IF}{2}}+e{r,\\mathrm{IF}}^{s}\\L_{r,\\mathrm{IF}}^{s}=\\rho_{r}^{s}+T_{r}^{s}+c\\cdot\\tilde{t}{r}-c\\cdot\\tilde{t}^{s}+\\lambda{\\mathrm{IF}}\\overline{N}{r,\\mathrm{IF}}^{s}+\\delta{\\mathrm{IF}}+\\varepsilon_{r,\\mathrm{IF}}^{s}\\end{cases}式中各项具体含义为：\\begin{gathered} \\\\tilde{t}{r}=t{r}+\\sum_{i=1}^{n}\\left(a_{i}\\cdot d_{r,i}\\right) \\c\\cdot\\tilde{t}^s=c\\cdot t^s+d_{\\mathrm{IF}{12}}^s+\\left(\\sum{i=1}^n(a_i \\cdot \\lambda_i \\cdot b_{r, i} )+\\sum_{i=1}^n(a_i \\cdot \\lambda_i \\cdot \\bar{b}i^s)\\right)\\D{\\mathrm{IF}}^{s}= \\left(a_{2}-\\beta_{12}\\right)\\cdot DCB_{12}^{s}+\\sum_{i=3}^{n}\\left(a_{i}\\cdot DCB_{1i}^{s}\\right) \\\\overline{N}{r,\\mathrm{IF}}^{s} =\\left(N{r,\\mathrm{IF}}^{s}+b_{r,\\mathrm{IF}}-\\overline{b}{\\mathrm{IF}}^{s}\\right)-\\frac{\\sum{i=1}^{n}\\left(a_{i}\\cdot d_{r,i}\\right)-d_{\\mathrm{IF}{12}}^{s}}{\\lambda{\\mathrm{IF}}} \\\\delta_{\\mathrm{IF}}= \\left(\\alpha_{12}\\cdot\\lambda_{1}\\cdot\\tilde{b}{1}^{s}+\\beta{12}\\cdot\\lambda_{2}\\cdot\\tilde{b}{2}^{s}\\right)-\\sum{i=1}^{n}\\left(a_{i}\\cdot\\lambda_{i}\\cdot\\tilde{b}{i}^{s}\\right)\\end{gathered}$$其中，$N{r,\\mathrm{IF}}^s、b_{r,\\mathrm{IF}}与\\overline{b}\\mathrm{lF}^s的具体表达式如下，$\\left{\\begin{array}{l}N{r, \\mathrm{IF}}^s=\\frac{\\sum_{i=1}^n\\left(a_i \\cdot \\lambda_i \\cdot N_i\\right)}{\\lambda_{\\mathrm{IF}}} \\b_{r, \\mathrm{IF}}=\\frac{\\sum_{i=1}^n\\left(a_i \\cdot \\lambda_i \\cdot b_{r, i}\\right)}{\\lambda_{\\mathrm{IF}}} \\\\bar{b}{\\mathrm{IF}}^s=\\frac{\\sum{i=1}^n\\left(a_i \\cdot \\lambda_i \\cdot \\bar{b}i^s\\right)}{\\lambda{\\mathrm{IF}}}\\end{array}\\right.统一多频无电离层组合的待估参数向量如下：X=\\begin{bmatrix}x,y,z,\\tilde{t}r,T_r^s,\\lambda\\mathrm{IF}\\overline{N}_{r,\\mathrm{IF}}^s\\end{bmatrix}$$\n（2 多个双频无电离层组合模型在传统双频无电离层组合的基础上，保持接收机钟差和卫星钟差的重参化过程不变，依据噪声放大水平，选择其它较优的无电离层组合进行联立，可以得到多个双频无电离层组合观测模型：$$\\left{\\begin{array}{l}P_{r, \\mathrm{IF}{12}}^s=\\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r_{12}}-c \\cdot \\tilde{t}^s+\\zeta_{\\mathrm{IF}{12}}+e{r, \\mathrm{FF}{12}}^s \\L{r, \\mathrm{IF}{12}}^s=\\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r_{12}}-c \\cdot \\tilde{t}^s+\\lambda_{\\mathrm{IF}{12}} \\bar{N}{r, \\mathrm{IF}{12}}^s+\\varepsilon{r, \\mathrm{FF}{12}}^s \\P{r, \\mathrm{IF}{1 i}}^s=\\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r_{12}}-c \\cdot \\tilde{t}^s+D_{r, \\mathrm{IF}{1 i}}-D{\\mathrm{IF}{1 i}}^s+\\zeta{\\mathrm{IF}{1 i}}+e{r, \\mathrm{IF}{1 i}}^s \\L{r, \\mathrm{FF}{1 i}}^s=\\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r_{12}}-c \\cdot \\tilde{t}^s+\\lambda_{\\mathrm{IF}{1 i}} \\bar{N}{r, \\mathrm{IF}{1 i}}^s+\\delta{\\mathrm{IF}{1 i}}+\\varepsilon{r, \\mathrm{IF}{1 i}}^s\\end{array}\\right.式中各项具体内容为：\\begin{gathered}\\begin{cases}P{r,\\mathrm{IF}{1i}}^{s}=\\alpha{1i}\\cdot P_{r,1}^{s}+\\beta_{1i}\\cdot P_{r,i}^{s}\\L_{r,\\mathrm{IF}{1i}}^{s}=\\alpha{1i}\\cdot L_{r,1}^{s}+\\beta_{1i}\\cdot L_{r,i}^{s}\\end{cases} \\D_{r,\\mathrm{IF}{1i}}=\\beta{\\mathrm{l}i}\\cdot DCB_{r,\\mathrm{l}i}-\\beta_{12}\\cdot DCB_{r,\\mathrm{l}2} \\D_{\\mathrm{IF}{\\mathrm{li}}}^{s}=\\beta{\\mathrm{li}}\\cdot DCB_{\\mathrm{li}}^{s}-\\beta_{\\mathrm{l2}}\\cdot DCB_{\\mathrm{l2}}^{s} \\\\overline{N}{r,\\mathrm{IF}{0i}}^{s}=\\left(N_{r,\\mathrm{IF}{1i}}^{s}+b{r,\\mathrm{IF}{1i}}-\\overline{b}{\\mathrm{IF}{1i}}^{s}\\right)-\\frac{d{r,\\mathrm{IF}{1i}}-d{\\mathrm{IF}{1i}}^{s}}{\\lambda{\\mathrm{IF}{1i}}} \\\\delta{\\mathrm{IF}{\\mathrm{li}}}= \\left(\\alpha{12}\\cdot\\lambda_{1}\\cdot\\tilde{b}{1}^{s}+\\beta{12}\\cdot\\lambda_{2}\\cdot\\tilde{b}{2}^{s}\\right)-\\left(\\alpha{1i}\\cdot\\lambda_{1}\\cdot\\tilde{b}{1}^{s}+\\beta{1i}\\cdot\\lambda_{i}\\cdot\\tilde{b}{i}^{s}\\right)\\end{gathered}其中，\\begin{cases}N{r,\\mathrm{IF}{1i}}^{s}&amp;=&amp;\\frac{\\left(\\alpha{1i}\\cdot\\lambda_{1}\\cdot N_{1}+\\beta_{1i}\\cdot\\lambda_{i}\\cdot N_{i}\\right)}{\\lambda_{\\mathrm{IF}{1i}}}\\b{r,\\mathrm{IF}{1i}}&amp;=&amp;\\frac{\\left(\\alpha{1i}\\cdot\\lambda_{1}\\cdot b_{r,1}+\\beta_{1i}\\cdot\\lambda_{i}\\cdot b_{r,i}\\right)}{\\lambda_{\\mathrm{IF}{1i}}}\\\\overline{b}{\\mathrm{IF}{1i}}^{s}&amp;=&amp;\\frac{\\left(\\alpha{1i}\\cdot\\lambda_{1}\\cdot\\overline{b}{1}^{s}+\\beta{1i}\\cdot\\lambda_{i}\\cdot\\overline{b}{i}^{s}\\right)}{\\lambda{\\mathrm{IF}_{1i}}}\\end{cases}$$由上述分析可知，对于新增加的双频无电离层组合，需要额外对新频率引入的硬件偏差进行处，其中，伪距观测方程中卫星端的DCB可采用事后产品进行改正，接收机端的IFB参数与接收机相关，通常作为常数进行估计；同样地，载波观测方程中也需要考虑 IFCB 的改正。\n多个双频无电离层组合模型的待估参数向量如下：$$X=\\begin{bmatrix}x,y,z,\\tilde{t}{r{12}},T_{r}^{s},D_{r,\\mathrm{E}{11}},\\lambda{\\mathrm{IF}{12}}\\overline{N}{r,\\mathrm{IF}{12}}^{s},\\lambda{\\mathrm{IF}{11}}\\overline{N}{r,\\mathrm{IF}_{11}}^{s}\\end{bmatrix}$$\n（3 基于原始观测值的非组合模型由于伪距硬件延迟与电离层参数强相关，在缺少外部电离层信息约束的情况下，二者无法分离，因此需要进一步对电离层与伪距硬件延迟重参化：$$\\tilde{I}{r,1}^s=I{r,1}^s-\\beta_{12}\\cdot\\left(\\mathrm{DCB}{r,12}-\\mathrm{DCB}{12}^s\\right)-\\beta_{12}\\cdot\\left(\\tilde{b}2^s-\\tilde{b}1^s\\right)$$其中，$\\tilde{I}{r,1}^s为重参化的电离层参数，由此得到非组合模型为：$\\left{\\begin{array}{ccc}P{r, 1}^s = \\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r{12}}-c \\cdot \\tilde{t}^s+\\gamma_1 \\cdot \\tilde{I}{r, 1}^s+\\zeta_1+e{r, 1}^s \\P_{r, 2}^s  =  \\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r{12}}-c \\cdot \\tilde{t}^s+\\gamma_2 \\cdot \\tilde{I}{r, 1}^s+\\zeta_2+e{r, 2}^s \\\\vdots  \\vdots  \\vdots \\P_{r, i}^s+D_i^s=  \\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r{12}}-c \\cdot \\tilde{t}^s+\\gamma_i \\cdot \\tilde{I}{r, 1}^s+D{r, i}+\\zeta_i+e_{r, i}^s \\L_{r, 1}^s  =  \\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r{12}}-c \\cdot \\tilde{t}^s-\\gamma_1 \\cdot \\tilde{I}{r, 1}^s+\\lambda_1 \\bar{N}{r, 1}^s+\\varepsilon_{r, 1}^s \\L_{r, 2}^s  =  \\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r{12}}-c \\cdot \\tilde{t}^s-\\gamma_2 \\cdot \\tilde{I}{r, 1}^s+\\lambda_2 \\bar{N}{r, 2}^s+\\varepsilon_{r, 2}^s \\\\vdots  \\vdots  \\vdots \\L_{r, i}^s   \\rho_r^s+T_r^s+c \\cdot \\tilde{t}{r{12}}-c \\cdot \\tilde{t}^s-\\gamma_i \\cdot \\tilde{I}{r, 1}^s+\\lambda_i \\bar{N}{r, i}^s+\\delta_i+\\varepsilon_{r, i}^s\\end{array}\\right.非组合模型的待估参数向量如下：X=\\begin{bmatrix}x,y,z,\\tilde{t}{r{12}},T_r^s,D_{r,i},\\tilde{I}{r,1}^s,\\overline{N}{r,i}^s\\end{bmatrix}$$由上述分析可知，不论无电离层组合还是非组合模型，由于重参化的卫星钟差包含了随时间变化的星端相位硬件延迟，因此在伪距观测方程中均引入了残余的硬件偏差，考虑到伪距观测值的噪声以及多径远大于载波观测值，数据处理中设定的权重也远小于载波，因此，无需对这部分偏差进行额外的改正。\n（4 多系统组合PPP模型上述的无电离层组合以及非组合模型中，待估接收机钟差为重参化后的接收机钟差，其中吸收了接收机端的伪距硬件偏差。对于多系统组合PPP，由于不同系统在接收机端的通道时延存在差异，通常需要为每个系统估计一个接收机钟差，或者选定一个系统的接收机钟差作为基准，估计其余系统相对于基准的ISB参数。因此，多系统组合PPP模型的待估参数向量为：$$X=\\left[x,y,z,\\underbrace{\\tilde{t}_r^{S_1},\\tilde{t}r^{S_2},\\cdots,\\tilde{t}r^{S_m}}{\\text{各系统接收机钟差}},T_r^s,D_r,\\tilde{I}{r,1}^s,\\overline{N}r^s\\right]\\\\text{或}\\X=\\left[x,y,z,\\underbrace{\\tilde{t}r^{S_1},ISB_r^{S_1S_2},\\cdots,ISB_r^{S_1S_m}}{\\text{基准接收机钟差与系统间偏差}},T_r^s,D_r,\\tilde{I}{r,1}^s,\\overline{N}_r^s\\right]$$其中，上标表示不同的导航系统。PPP 中全部待估参数通常包括以下6种：1.接收机三维位置；2.接收机钟差（或系统间偏差）；3.天顶对流层湿延迟；4.接收机端IFB；5.重参化倾斜电离层延迟；6.浮点模糊度。\n","categories":["GNSS","阅读笔记"],"tags":["GNSS"]},{"title":"论文笔记-2024-03-精密单点定位技术研究","url":"/2024/03/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-2024-03-%E7%B2%BE%E5%AF%86%E5%8D%95%E7%82%B9%E5%AE%9A%E4%BD%8D%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/","content":"1.GNSS差分定位中的误差源分析影响用户接收机定位精度的主要为电离层、对 流层误差、星历、星钟误差等。根据基准站播发的 差分修正量类型的不同，差分技术可以分为位置差 分、伪距差分和载波相位差分三种。\n（1）位置差分基准站接收机根据伪距观测量与星历等信息 计算自身位置坐标，并与基准站已知位置坐标作差， 将得到的位置修正量发送给用户接收机。用户接收 机用接收到的修正量校正自身的定位信息，从而得 到精确度较高的自身位置坐标。 \n设基准站精准位置坐标为 ,  为基准站利用导航卫星所得到的定位坐标, 此时位置差分的修正量为:将此修正量发送给用户接收机, 用户接收机对自身的位置坐标进行修正, 便可以得到精确的自身位置坐标。这种方法需要确保基准 站与用户接收机所使用的卫星是相同的，并且要求 基准站与用户接收机的解算算法相同\n（2）伪距差分基准站接收机根据已知的自身位置坐标与卫 星位置坐标求得与卫星真实几何距离r，并计算\n其 与伪距观测量的误差Δρ。将此误差发送给用户接 收机，用户接收机通过此误差修正自身的伪距观测 量，利用修正后的伪距进行定位解算，从而得到较 为精准的自身坐标。\n设基准站接收到的第  颗卫星的伪距观测量为:\n式中,  为基准站与卫星的真实几何距离,该距离可由基准站的已知位置  与通过星历计算出的卫星位置  求出。基准站播发的伪距修正量为伪距观测量与真实几何距离的差, 可以表示为:\n式中,  : 基准站的钟差;  : 卫星钟差;  :卫星星历误差;  : 电离层误差;  : 对流层误差;  : 多径效应引起的误差;  : 基准站接收机误差。\n用户接收机观测与基准站相同的卫星, 并通过  来修正不同卫星的伪距观测量, 减小伪距观测误差, 提高定位精度。\n（3）载波相位差分载波相位差分中，用户接收机利用基准站播发的载波整周数对自身进行修正，这种 差分定位的精度能够达到厘米级甚至毫米级。具体实现过程和伪距差分类似。\n2.ＢＤＳ／ＧＰＳ 双系统精密单点定位关键问题研究本文在ＧＮＳＳ精密单点定位基本算法原理的基础上进行系列研宄， 包括：ＰＰＰ无电离层组合模型、非差非组合模型定位性能比较与分析；ＢＤＳ ２／ ＢＤＳ－３、 ＧＰＳ／ ＢＤＳ系统间偏差相关问题研宄； 非差非组合ＰＰＰ附加电离层约束定权方法研宄； 多系统ＰＰＰ定位性能分析等内容。\n（1）非组合模型定位性能优于无电离层组合，附加先验电离层信息约束的非差非组合模型能有效提高模糊度固定率。\n（2）多星座全球导航卫星系统测量数据的集成可以有效提高导航定位方案的精度和可靠性，而系统间偏差（ISB）是影响系统兼容性的关键问题。系统间偏差由不同系统的卫星钟差基准差异与接收机硬件延迟误差两部分组成，其具体形式可表现为不同系统间包含无电离层组合硬件延迟的接收机钟差之差。\n（3）非差非组合 ＰＰＰ 不仅可以获得高精度的电离层延迟信息， 而且可以通过引入先验电离层虚拟观测值对非差非组合ＰＰＰ进行约束， 达到加速收敛并提高定位精度的目的。不过目前引入的电离层虚拟观测值精度有限，使虚拟电离层观测值与实际观测值之间的权比较难确定。\n（1）定权方法：1）常数约束方法：将虚拟电离层观测值与实际观测值间的权比设置为依靠经验获得的常数\n2）逐步约束方法：将虚拟电离层观测值与实际观测值之间的权比设置为随着滤波时间逐步降低的线性函数\n3）时空约束方法：将虚拟电离层观测量的先验方差与测站电离层穿刺点处的当地时间和纬度相结合来确定各历元虚拟电离层观测量权重的方法\n4）定权方法比较：选用常数约束方法和时空约束方法收敛后定位精度远低于未附加电离层约束时的定位精度，而选用逐步松弛约束方法收敛后定位精度与未附加电离层约束的定位精度基本相当。同时，选用逐步松弛约束方法时收敛速度提升幅度最大。\n（2）精密单点定位原始观测值方程\n式中参数依次表示：接收机与卫星间的几何距离，接收机钟差和卫星钟差，对流层延迟，电离层延迟放大因子，频率信号传播中的电离层延迟，接收机和卫星的伪距硬件延迟，接收机和卫星的相位硬件延迟，接收机和卫星的初始相位偏差，未参数化和模型化的其他误差。\n（3）无电离层组合模型和非差非组合模型双频无电离层组合模型可以消除一 阶电离层延迟， 降低电离层延迟在定位解算中影响的同时减少了待估参数，但因线性组合导致其观测噪声放大约三倍。\n与无电离组合模型相比，基于原始观测值的非差非组合模型应运而生。该模型不仅将电离层延迟量作为参数进行估计，而且充分利用了原始观测值信息，在频率使用上也更具备灵活性。\n（4）随机模型1）基于高度角的随机模型\n基于高度角的随机模型根据高度角与观测值噪声间的关系，以高度角为变量建立随机模型来对观测值方差进行估计。2）基于信噪比的随机模型\n通常信噪比随高度角平稳变化，信噪比越大，观测量的精度越高，若信噪比出现跳跃或抖动则说明数据质量可能异常。\n（5）多系统融合的函数模型ＧＮＳＳ多系统数据融合时，通常将各系统时间基准统一到ＧＰＳ时间，但不同系统在解算过程中选取的基准钟卫星的不同仍会导致各系统的卫星钟差基准存在差异，同时由于各卫星导航信号的调制方式、接收机硬件和固件版本等差异较大导致不同导航系统在多模接收机内部的硬件延迟时间存在系统性差异。将不同系统的卫星钟差基准差异与接收机硬件延迟误差表示为系统间偏差（ＩＳＢ），其具体形式可表现为不同系统间包含无电离层组合硬件延迟的接收机钟差之差。$$\\begin{aligned}&amp;&amp;&amp;P_{IF}^{C2}=\\rho^{C2}+cd{t^{\\prime}}{r}^{G}+c\\cdot ISB{G,C2}+T^{C2}+M_{{PE}}^{C2}+\\varepsilon{{PF}}^{C2} \\&amp;&amp;&amp;L{IF}^{C2}=. \\rho^{C2}+cdt^{\\prime}{}^{G}+c\\cdot ISB_{G,C2}+T^{C2}+\\lambda_{F}^{C2}N{F}^{\\prime C2}+M{{LF}}^{C2}+\\varepsilon{{LF}}^{C2}  \\&amp;&amp;&amp;P{IF}^{C3}=\\rho^{C3}+cd{t^{\\prime}}{,}^{G}+c\\cdot ISB{G,C3}+T^{C3}+M_{{PF}}^{C3}+\\varepsilon{{PF}}^{C3} \\&amp;&amp;&amp;L{IF}^{C3}=\\rho^{C3}+cdt_{r}^{G}+c\\cdot ISB_{G,C3}+T^{C3}+\\lambda_{F}^{C3}N{F}^{\\prime C3}+M{{LF}}^{C3}+\\varepsilon{{LF}}^{C3} \\\\text{其中:} \\&amp;&amp;&amp;c\\cdot ISB{G,C2}=cdt_{r}^{\\prime}{}^{C2}-cdt_{r}^{\\prime G}=c(dt_{r}^{C2}-dt_{r}^{G})+b_{r,IF}^{C2}-b_{r,IF}^{G} \\&amp;&amp;&amp;c\\cdot ISB_{G,C3}=cdt_{r}^{\\prime C3}-cdt_{r}^{\\prime G}=c(dt_{r}^{C3}-dt_{r}^{G})+b_{r,IF}^{C3}-b_{r,IF}^{G} \\\\end{aligned}$$\n（6）非差非组合 ＰＰＰ 附加电离层约束的定权研究非差非组合ＰＰＰ不仅可以获得精确的电离层延迟信息，而且可以通过引入外部的先验电离层虚拟观测值作为约束条件，加速位置参数的收敛。\n对于事后ＰＰＰ而言，最常使用全球电离层格网图ＧＩＭ作为先验电离层虚拟观测值。\n估计的电离层斜延迟量除电离层斜延迟信息外还包含接收机ＤＣＢ与卫星ＤＣＢ。 没有额外基准条件时，估计的电离层斜延迟信息与站星ＤＣＢ无法直接分离。\n文中使用ＣＯＤＥ提供的卫星ＤＣＢ产品以及ＭＧＥＸ提供的多系统ＤＣＢ产品在伪距观测值中对卫星端ＤＣＢ进行事先改正。由于接收机ＤＣＢ与接收机固件和接收机温度变化密切相关的特性，使接收机ＤＣＢ可能在短时段内发生显著变化。文中引进接收机ＤＣＢ参数，对其进行参数估计，以便得到纯电离层斜延迟信息。\n增加电离层虚拟观测值后，附加电离层约束的非差非组合模型可表示如下：$$P_{j}=\\rho+c(dt_{r}^{\\prime}-dt^{\\prime s})+T+\\gamma_{j}I_{1}+\\frac{\\gamma_{j}}{\\gamma_{2}-1}\\cdot B_{r}+M_{p_{j}}+\\varepsilon_{p_{j}}\\\\begin{aligned}&amp;L_{j}=\\rho+c(dt_{r}^{\\prime}-dt^{\\prime s})+T-\\gamma_{j}I_{1}-\\frac{\\gamma_{j}}{\\gamma_{2}-1}\\cdot B_{r}+\\lambda_{j}N_{j}^{\\prime\\prime}+M_{L_{j}}+\\varepsilon_{L_{j}}\\end{aligned}\\{I}{I}=I{I}+\\varepsilon_{I_{I}}$$其中，$\\tilde{I}{1}代表使用外部电离层产品计算得到的电离层斜延迟；I{1}代表待估的电离层斜延迟；\\varepsilon_{\\tilde{I}i}代表伪观测值\\widetilde{I}{\\mathrm{i}}$对应的测量噪声。\n（7）一种改进的权因子搜索约束方法改进的权因子搜索约束方法的基本思路是在考虑电离层时空信息的基础上， 使用权因子对电离层虚拟观测值的方差阵进行调整。使用调整后的电离层虚拟观测值结合原始观测值的方差阵进行验后平差计算，当验后残差加权平方和达到最小值时确定最优权因子。后辅以逐步松弛算法以逐步减少电离层虚拟观测量对收敛后 PPP 定位精度的影响，以此来达到最大限度利用电离层虚拟观测值改善定位结果的目的。\n改进的权因子搜索约束方法的具体流程为： \n1）根据电离层时空信息，确定某历元电离层虚拟观测量初始方差Missing \\left or extra \\right \\sigma^2(i)=\\begin{cases}\\sigma_0^2/\\sin^2(E),&amp;t&lt;8\\mathrm{or}t&gt;20\\mathrm{or}B&gt;\\pi/3\\\\left(\\sigma_0^2+\\sigma_1^2\\cos{(B)}\\cos(\\frac{t-14}{12}\\pi)\\right)/\\sin^2(E),&amp;\\text{其它}\\end{cases} 其中， 表示虚拟电离层观测量的初始方差 ( 表示随时间和空间变化而变化的先验方差(; 表示卫星高度角(弧度);  表示接收机到卫星连线方向上的电离层穿刺点的地理纬度〈弧度〉; t 表示穿刺点处的当地时间(h);变量  和  可设置为 0.09 。\n2）首次搜索将权因子Ｋ设置为１;\n3）根据权因子，确定电离层虚拟观测值方差矩阵：其中，根据第一步确定的某历元m颗卫星的电离层虚拟观测量初始方差矩阵。由此可以将权因子的具体含义表达为虚拟观测值初始方差矩阵的整数倍。\n4）将与原始观测值的方差矩阵相结合进行验后平差计算，记录验后残差加权平方和;\n5）判定验后加权平方和是否达到收敛阈值，若没有达到收敛阈值，则以1的步长逐次累加，循环3~5步直到满足收敛条件。满足后，得到该历元最优权因子，同时得到最优电离层虚拟观测值方差矩阵。\n6）在最优权比的基础上，加入逐步松弛约束：其中，位方差变化率，取值;  为以分钟为单位的该历元观测时间与初始观测时间的时间间隔。\n\n\n\n\n3.低轨卫星和GNSS精密单点定位研究1）函数模型采用双频无电离层组合的观测方程如下：$$\\begin{aligned}&amp;P_r^s=\\rho_r^s+c\\Big(dt_r-dt^s\\Big)+T+b_r-b^s+\\mathcal{E}{P_r^s}\\&amp;\\Phi_r^s=\\rho_r^s+c\\Big(dt_r-dt^s\\Big)+T+\\delta_r-\\delta^s+\\lambda N^s+\\mathcal{E}{\\Phi_r^s}\\end{aligned}$$ 和 s 分别表示接收机和卫星， 和 表示对应端钟差，、 表示无电离层组合对应端伪距硬件延迟；、表示无电离层组合对应端相位硬件延迟；\n在实际数据处理中，卫星端消电离层伪距硬件延迟已被实际改正数 吸收，接收机端亦是如此，即\n相位硬件延迟和模糊度呈线性相关，而且相位延迟在一定时间内具有极高的稳定性，因此相位硬件延迟将被模糊度参数吸收，一般在浮点解解算中不予考虑。解算时不予考虑。经过精密产品改正后，公式可以表示为：\n其中，为吸收了 DCB 和 UPD 的 组合模\n糊度。\n多系统融合PPP定位模型的待估参数向量为：\n也可以用下式来表示，两个表达式是等价的。\n2）数据处理策略文中使用递归最小二乘的参数估计方法\n3） LEO星载GPS PPP周期误差特性分析对于LEO卫星来说，在PPP动态定位过程中运动速度过快会导致噪声被放 大从而产生高频误差，还可能会产生其他的误差，在PPP数据处理过程中这种周 期性误差如果不能有效消除，对卫星定位有很大的负面影响，因此，需要采取一 定策略进行误差消除。 \n1.基于中位数建模的周期误差消除方法若一组数据序列在不同天表现出相同的重复性波动起伏现象，即在每天横坐 标相同时纵坐标大致相同，表现出比较明显的以天为周期的特性，可以采用中位 数建模方法消除这种波动。对于一组数据序列可以得到每个横坐标下对应的纵坐 标的中位数，建立中位数时间序列模型，然后将横坐标相同时刻的数据序列减去 中位数，得到一个新的数据序列，从而消除这种以天为周期的波动性误差。中位 数建模方法的前提是不同天数据序列的波形具有很好的重合度，若部分数据与中 位数偏离较大，在模型改正后精度可能会变差。\n2. 基于平移预报的周期误差消除方法若一组数据序列在不同天的波形存在明显重复现象但是波形在时间上存在 错位趋势，即横坐标平移一段距离后纵坐标大致相同，可以采用平移预报的方法 消除周期性误差。对于一组数据序列，可以采用平移横坐标的方法使得两个或多 个序列的纵坐标大致重合，将重合对齐后的数据作为预报值，来修正未来一天对 应时间的数据序列，从而来达到减小周期性误差的目的。\n4.安卓智能手机多模 GNSS 观测数据质量评估及定位算法研究诸如智能手机、平板电脑等常见智能终端通常使用小型化、低成本的内置高灵敏度线性极化天线。这种天线虽能在弱信号强度下捕获更多的卫星观测数据，以便进行定位；但同时也更容易受到多路径效应的干扰，观测数据中含有很多粗差和周跳。多路径效应是智能手机实现高精度定位的主要障碍，且智能手机天的GNSS天线相位中心难以确定，\n本文采用常见智能手机进行实验，对其在不同环境下采集的GNSS观测数据从信噪比、多路径效应、数据完整率、周跳和双频数据可用率等方面进行深入分析，研究了智能手机在不同环境下的GNSS观测数据质量特征，随后介绍了基于载波相位观测值和多普勒观测值平滑伪距的单点定位的算法流程，最后分别研究了在不同环境下的伪距单点定位和精密单点定位两种模式的定位精度，并对单系统和多系统融合定位的结果进行分析评估。\n1.安卓系统的架构应用程序层、应用框架层、系统运行库层、Linux内核层\n2.安卓终端 GNSS 模块架构图基带处理模块负责获取、跟踪GNSS信号并将收到的信号进行解码，获得伪距、多普勒、载波相位观测值和导航电文。\n安卓 GNSS 定位接口给出的某些时间是各个系统时间框架下的，计算伪距观测值时需要把他们统一到共同时间基准上。\n3.精密单点定位观测方程\n其中，表示载波相位观测值，表示卫星和地面测站的经过天线相位中心改正的真实卫地距离，如果没有经过改正，则表示卫星和接收机的天线相位中心之间的距离，表示相应载波频率的波长，为整周相位模糊度参数，表示载波相位观测值的接收机钟差，表示载波相位观测值的卫星钟差，表示载波相位观测值的噪声。\n经过精密轨道和精密卫星钟差产品改正之后线性化：\n4.伪距平滑 SPP 定位算法伪距平滑利用相邻历元载波相位观测值的差来获得两个历元之间由于环境变化带来的未被模型改正的误差，从而提高伪距观测值的精度。\n5.载波相位平滑伪距载波相位平滑伪距能够有效降低伪距观测值的噪声水平。Hatch 滤波被广泛地运用在载波相位平滑伪距算法中，其公式如下：$$\\left.\\left{\\right.\\right.$$其中，、分别表示第 k、k-1 历元的载波相位整周数；表示相应频率的波长，表示第 k 历元的伪距观测值；表示经载波相位平滑后第 k 历元的伪距观测值，n 为平滑窗口长度。\n6.多普勒平滑伪距多普勒观测值则不受周跳和“duty cycle”机制的影响，因而通过多普勒平滑伪距可以有效提高伪距的精度，降低其噪声。多普勒平滑伪距的公式可以表示为：其中，，分别表示相应频率的波长、采样间隔时间；、分别为第 k、 k-1 历元的多普勒观测值。\n5.ＧＮＳＳ精密单点定位技术及应用进展研究主题：1.PPP定位模型\n2.数据预处理\n3.误差精细建模\n4.模糊度固定\n5.快速初始化\n发展主线：1.浮点解到固定解\n主要围绕如何固定非差模糊度\n2.后处理到实时\n主要围绕高精度实时卫星轨道和高频卫星钟差产品处理\n3.单系统到多系统乃至多系统集成\n主要围绕系统间偏差、频间偏差和频内偏差的估计与建模\n6.基于RTKLIB的GNSS精密单点定位技术研究（1）文章主要内容：1.研究精密单点定位的基础理论和方法，介绍了各种线性组合\n2.以MGEX测站的观测数据为基础，分析了卫星可见性、信噪比、卫星星座的几何精度因子、多路径误差等内容\n（2）卫星定位的实质卫星定位的实质是在统一的坐标系统和时间系统下，通过已知的卫星坐标和接收机测定的卫地距离，构建一定函数模型，解算出测定点在同一坐标系统下的绝对坐标。\n（3）组合模式详细介绍\n，分别表示，上的载波相位观测值；\n表示线性组合的虚拟观测值；\n表示组合相位观测值的频率，；\n表示组合相位观测值的波长，；\n表示组合相位观测值的整周模糊度，；\n表示组合相位观测值的电离层延迟，，其中；\n表示组合相位观测值的观测噪声，${\\mathcal E}{\\phi{n,m}}=\\sqrt{\\left(n{\\mathcal E}{\\phi{1}}\\right)^{2}+\\left(m{\\mathcal E}{\\phi{2}}\\right)^{2}}$.\n1.宽巷组合取：n=1，m=-1，得到组合观测值：\n式中宽巷组合观测值的波长，比原始观测值的波长更长，组合观测值的噪声更大，通常只用于周跳的探测和修复以及辅助求解整周未知数。\n2.窄巷组合取：n=1，m=1，得到组合观测值：窄巷组合观测值的波长比原始观测值更小。\n3.无几何距离组合取：n=，m=，得到组合观测值：无几何距离组合观测值与几何距离无关，消除了中性大气层延迟，卫星轨道误差，卫星钟差，接收机钟差，组合观测值中仅包含电离层延迟，整周模糊度和噪声，可以用于探测和修复周跳。\n4.无电离层组合取：n=，m=，得到组合观测值：无电离层组合消除了电离层的一阶项，模糊度失去了整数特性。\n5.MW组合\n只受观测噪声和多路径效应的影响，可以用来探测周跳，但是或频率出现周跳都会导致观测值的异常，因此不能分辨具体是哪个频率出现异常，而无几何组合对的周跳组合不敏感，因此实际应用中会同时使用MW组合和GF组合检测周跳。\n（4）细节问题1）对精密单点定位来说，伪距硬件延迟被接收机钟差吸收，载波相位的硬件延迟被整周未知数吸收。\n2）P码伪距观测值多路径误差约为15m。载波相位的多路径误差比伪距观测值要小的多，通常不大于。\n3）信噪比（SNR）值越大，那么说明观测信号的质量越好，观测精度越高。GDOP的数值越大，所代表的单位矢量形体体积越小，即接收机至空间卫星的角度十分相似导致的结果，此时的GDOP会导致定位精度变差。好的 GDOP，指其数值小，代表大的单位矢量形体体积，导致高的定位精度。\n4）在RTKLIB 软件中实现精密单点定位采用的是 RTKPOST 模块，这一模块主要包含六个部分：文件读取与处理、数据预处理、观 测量建模与误差改正、扩展Kalman滤波器、模糊度求解、结果输出。RTKLIB内部使用GPST和ECEF。\n（5）文中采用的五个方案1）分析不同精密星历产品对精密单点定位结果的影响；\n2）利用同一分析中心的精密星历，使用不同系统进行精密单点定位静态结算；\n3）利用同一分析中心的精密星历，进行双系统组合精密单点定位静态结算；\n4）设置不同的卫星截至高度角，分析精密单点定位结果精度和卫星截止高度角的关系；\n5）采用不同的定位结算模式，包括静态结算（PPP Static），动态模式（PPP Kinematic），固定模式（PPP Fixed）。\n动态精密单点定位模式，其原理是利用MGEX精密星历和精密钟差对单台 GNSS双频接收机的观测数据进行动态非差处理，以达到高精度的动态定位。\n（6）精密单点定位固定模式（PPP Fixed）精密单点定位固定模式是针对于定位中的整周模糊度提出的，由于整周模糊度 在解算时引入了硬件延迟误差和电离层残余误差，因此只能求出浮点解，此模式采 用模糊度固定方法，剔除残留误差项，从而恢复模糊度的整数特性，达到了提高定 位精度的目的。\n","categories":["GNSS","阅读笔记"],"tags":["GNSS"]},{"title":"《深度学习》课程笔记 part1 引言,CNN,RNN","url":"/2025/05/13/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20part1%20%E5%BC%95%E8%A8%80,CNN,RNN/","content":"Chapter 1 – Introduction一、深度学习的起源与发展1.1 第一阶段（1943-1969）\n1943年：Warren McCulloch 和 Walter Pitts 提出M-P神经元模型，模拟人类神经元的结构和工作原理。  \n1949年：生物心理学家 Donald Hebb 提出Hebb学习规则，它的主要思想是当神经元之间反复发送信号时神经元之间的连接权重会增加，反之会减小，奠定了人工神经网络学习算法的基础。\n1957年：Frank Rosenblatt 提出了由两层神经元组成的感知机（Perceptron）。  \n1969年：Minsky 和 Papert 指出感知机无法解决线性不可分问题（如XOR），导致神经网络的研究进入低谷。\n\n1.2 第二阶段（1980-1998）\n1980年：福岛邦彦提出了模拟生物视觉传导通路的神经认知机，被认为是卷积神经网络的原始模型。\n\n1982年：John Hopfield 提出 Hopfield神经网络，有连续型和离散型两种类型，分别用于优化计算和联想记忆。\n\n1986年：Rumelhart、Hinton 和 Williams 重新提出反向传播算法（BP），并指出多层感知机可以解决异或操作（XOR）这样的线性不可分问题。\n\n1986年与1990年：出现了 Jordan Network 与 Elman Network 两种循环神经网络（Recurrent Neural Networks，RNN）。\n\n1995年：Corinna Cortes 和 Vladimir Vapnik 提出了支持向量机（Support\nVector Machine，SVM），除了其简单的训练方法与优越的性能超过了人工神经网络之外，其良好的可解释性使得人工神经网络研究再次进入低谷期。\n\n1997年：Schmidhuber 和 Hochreiter 提出了长短期记忆网络（LSTM），极大地提高了循环神经网络的效率和实用性。\n\n1998年：Yann LeCun 提出了 LeNet-5，率先将人工神经网络应用于图像识别任务，但在当时也没有引起大的轰动。\n\n\n1.3 第三阶段（2006-2023）\n2006年：Geoffrey Hinton 和他的同事提出了一种称作深度信念网络（Deep Belief Networks，DBN）的多层网络并进行了有效的训练，同时提出了一种通过多层神经网络进行数据降维的方法，正式提出了深度学习的概念。\n2012年：Frank Seide 等人使用深度神经网络进行语音识别，相比于传统的 GMM 和 HMM，识别错误率下降了 20%-30%，取得了突破性的进展。\n2012年：Alex Krizhevsky 等人提出了 AlexNet，它引入了 ReLU 激活函数，并使用 GPU 进行加速。在著名的 ImageNet 图像识别大赛中，AlexNet 使得图像识别错误率从 26% 下降到了 15%，并夺得 2012 年的冠军。\n2014年：R-CNN提出，开启基于深度学习的目标检测时代。后续Fast R-CNN（2015）、Faster R-CNN（2015）进一步优化速度和精度。**GAN（生成对抗网络）**由 Ian Goodfellow 提出，通过生成器与判别器的对抗训练实现无监督数据生成，掀起图像生成研究热潮。**GoogleNet（Inception）**和 **VGGNet **相继发布，推动卷积神经网络结构优化。\n2015年：**ResNet（残差网络）**由何恺明团队提出，通过跳跃连接解决深层网络梯度消失问题，将 ImageNet 错误率降至 3.57%，模型深度突破千层。YOLO 首次提出端到端目标检测框架，大幅提升实时检测效率。\n2017年：SENet 的图像识别错误率已经下降到了 2.25%，由于错误率已经到了极限，导致 ImageNet 图像识别大赛从 2018 年开始不再举办。Transformer 架构由 Vaswani 等人提出，基于自注意力机制替代 RNN，成为NLP 领域基石。\n2018年：BERT（基于Transformer的双向预训练模型）发布，刷新11项NLP任务记录，其核心创新在于双向上下文建模与大规模预训练-微调范式，直接推动了 NLP 任务性能的飞跃。\n2020年-2023年：OpenAI 推出 ChatGPT，基于 1750 亿参数的 GPT-3.5 架构，首次实现大规模语言模型的强交互能力，显著提升文本生成、问答和代码编写等任务的表现。\n2024年：GPT-4 参数规模达万亿级，支持更复杂的多模态任务，如文生图、视频生成等。\n2024年：OpenAI 发布 Sora，实现从文本到高质量视频的生成，标志着生成式AI 从单模态（文本、图像）向多模态（文本+图像+视频）跨越。\n\n\n二、深度学习与机器学习、AI 的关系\n人工智能（AI）：模拟人类智能的广义领域（1956年达特茅斯会议提出）。  \n人工智能的概念最早在 1956 年的美国达特茅斯会议（Dartmouth Conference）上提出，当时会议的主题是用机器来模仿人类学习以及其它方面的智能。因此，1956 年被认为是人工智能的元年。\n一般认为，人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新兴学科。\n\n\n机器学习：AI 的子领域，通过数据学习规律并预测未知。  \n让计算机具有像人一样的学习和思考能力的技术的总称。具体来说是从已知数据中获得规律，并利用规律对未知数据进行预测的技术。\n机器学习是 AI 的一个子集，它关注如何通过算法让计算机从数据中自动学习，而不是通过显式编程实现规则。ML 系统通常依赖特征工程和统计模型来完成分类、回归或聚类等任务。\n\n\n深度学习：深度学习是机器学习的一个分支，它以多层人工神经网络为核心，通过多次非线性变换自动学习数据的多级抽象特征，尤其适合处理图像、语音和文本等非结构化数据。\n\n\n\n\n三. 深度学习的基本概念与典型算法3.1 基本概念3.1.1. 深度学习定义\n深度学习是机器学习的一个分支，核心在于使用多层神经网络（含三层及以上隐藏层）自动学习数据的多级抽象表示，无需手工设计特征。\n与传统 ML 强调特征工程不同，深度学习模型通过反向传播（back-propagation）算法结合梯度下降自动更新连接权重，完成端到端的训练过程。\n\n3.1.2 神经网络结构\n多层感知机（MLP）：最基础的前馈网络，由全连接层堆叠而成，适用于表格数据与简单分类/回归任务。\n激活函数（Activation）：如 ReLU、Sigmoid、Tanh，引入非线性能力，使网络能拟合复杂函数。\n损失函数（Loss）：如交叉熵（Cross-Entropy）、均方误差（MSE），衡量预测与真实值差距，用于指导梯度更新。\n优化器（Optimizer）：如 SGD、Adam、RMSProp 等，通过不同策略调整学习率与动量，加速收敛并避免陷入局部最优。\n\n3.1.3 训练与正则化\n反向传播（Back-Propagation）：计算损失梯度并沿网络反向传播，以更新每层权重。\n正则化：Dropout、L2 正则化等技术防止过拟合；归一化（BatchNorm、LayerNorm）可加速训练并稳定网络。\n\n\n3.2 典型深度学习算法3.2.1 多层感知机（MLP）\n结构：输入层—多个全连接隐藏层—输出层。\n应用：表格数据分类/回归。\n特点：网络最简单，但参数量大，对高维输入不高效。\n\n\n\n3.2.2 卷积神经网络（CNN）\n核心：使用卷积层提取局部空间特征，池化层下采样，最后用全连接层进行分类或回归。\n优势：参数共享与稀疏连接，大幅减少模型参数，擅长图像、视频等空间数据处理。\n代表：LeNet、AlexNet、VGG、ResNet、EfficientNet。\n\n3.2.3 循环神经网络（RNN）\n核心：对序列数据使用循环连接，将历史信息编码到隐藏状态，用于时序建模。\n变体：LSTM、GRU，通过门控机制缓解长程依赖与梯度消失问题。\n应用：机器翻译、语音识别、时间序列预测。\n\n\n\n3.2.4  自编码器（Autoencoder）\n结构：编码器—瓶颈层—解码器，学习无监督的特征表示。\n变体：稀疏自编码器、去噪自编码器、变分自编码器（VAE）。\n应用：降维、异常检测、数据生成。\n\n3.2.5 生成对抗网络（GAN）\n核心：由生成器与判别器对抗训练，生成器学习伪造真实样本，判别器学习区分真伪。\n代表：DCGAN、Pix2Pix、StyleGAN。\n应用：图像生成、图像修复、风格迁移。\n\n\n\n3.2.6 深度置信网络（DBN）\n基于**受限玻尔兹曼机（RBM）**逐层预训练，然后全局微调。\n在监督学习普及前，用于无监督特征学习与初期深度网络训练。\n\n3.2.7 Transformer 与自注意力（Self-Attention）\n核心：用自注意力机制取代 RNN/CNN，直接在序列上并行计算相似度，加速长程依赖建模。\n代表：Transformer、BERT、GPT 系列、Vision Transformer。\n应用：NLP、图像理解、跨模态学习。\n\n\nChapter 2 – 卷积神经网络一、卷积神经网络的起源与发展卷积神经网络（CNN）的发展规律体现为技术驱动与需求牵引的双重演进逻辑：其架构设计从生物启发的局部感知逐步转向数学优化的模块化组合（如残差连接、注意力机制），通过深度增加与效率提升的螺旋平衡（如轻量化模型与自动化网络搜索）解决复杂任务与资源约束的矛盾；应用场景从单一图像处理向多模态融合与边缘实时计算扩展，推动模型从实验室向工业级泛化；同时，技术迭代始终围绕数据效率（如自监督学习降低标注依赖）与可解释性（如可视化工具与因果推理结合）两大核心挑战展开，未来将在跨模态统一架构、生物仿生低功耗设计及绿色可持续优化等方向持续突破，形成“理论创新-场景适配-社会价值”闭环。\n1.1 起源阶段（1960s-1990s）\n生物学启发与早期模型\n1959-1968年：神经科学家 David Hubel 和 Torsten Wiesel 通过猫的视觉皮层实验，发现简单细胞（边缘检测）与复杂细胞（组合特征）的层级响应机制，为 CNN 的局部感知与层级结构奠定理论基础。\n1980年：福岛邦彦提出神经认知机（Neocognitron），首次模拟视觉系统的层级结构，包含**卷积层（特征提取）与池化层（空间降采样）**的雏形，用于手写字符识别。\n\n\n首个实用化 CNN 诞生\n1989年：Yann LeCun 在贝尔实验室开发首个卷积网络原型，用于手写数字识别，但受限于计算能力未广泛应用。\n1998年：LeCun 团队正式提出 LeNet-5，包含以下核心结构：\n卷积层：使用 5×5 卷积核提取局部特征（如笔画方向）。\n池化层：通过 2×2 平均池化降低特征维度。\n全连接层：输出分类结果。\n\n\n\n\n\n\n二、技术突破阶段（2012-2015）\nImageNet 竞赛与深度学习革命\n2012年：Alex Krizhevsky 等人提出 AlexNet，在 ImageNet 图像分类任务中以 15.3% 的错误率夺冠（传统方法为 26.2%），核心创新包括：\nReLU 激活函数：解决梯度消失问题，加速训练。\nDropout：随机失活神经元防止过拟合。\nGPU 并行训练：利用双 NVIDIA GTX 580 GPU，训练时间从数月缩短至 5-6 天。\n\n\n\n\n网络深度与结构的优化\n2013年：**ZFNet **通过可视化卷积核响应，解释 CNN 的层次特征学习机制，推动模型可解释性研究。\n2014年：\nVGGNet：使用堆叠的 3×3 小卷积核构建 16-19 层网络，证明深度对性能的关键作用（ImageNet 错误率 7.3%）。\nGoogLeNet（Inception v1）：提出 Inception 模块（并行 1×1、3×3、5×5 卷积与池化），参数量仅 500万（AlexNet 为6000万）。\n\n\n2015年：何恺明团队提出 ResNet，通过**残差连接（Skip Connection）**解决深层网络梯度消失问题，模型深度达 152 层（ResNet-152），ImageNet 错误率降至 3.57%。\n\n\n\n\n三、多样化发展与行业应用（2016-2022）\n目标检测与分割的突破\n目标检测：\nR-CNN系列（2014-2015）：从区域提议（Selective Search）到端到端区域生成网络（Faster R-CNN），检测精度（mAP）从 53.3% 提升至 73.2%。\nYOLO系列（2016-2020）：YOLOv1 实现每秒 45 帧实时检测，YOLOv4 通过 Mosaic 数据增强与 CSPNet 结构优化，mAP 达到 43.5%。\n\n\n图像分割：\nFCN（2015）：首个全卷积网络，支持像素级分类，PASCAL VOC 分割 mIoU 达到 62.2%。\nU-Net（2015）：通过跳跃连接保留细节信息，成为医学影像分割标杆模型（如细胞分割 Dice 系数超90%）。\n\n\n\n\n轻量化与移动端部署\nMobileNet系列（2017-2019）：\nMobileNet v1：引入深度可分离卷积（Depthwise + Pointwise），计算量减少至标准卷积的 1/8~1/9。\nMobileNet v2：反向残差结构（Expand→Depthwise→Project）提升特征表达能力。\n\n\nEfficientNet（2019）：通过复合缩放系数（深度、宽度、分辨率）平衡模型效率与精度，ImageNet Top-1 准确率达到 84.4%（仅66M参数）。\n\n\n注意力机制与Transformer融合\nVision Transformer（ViT，2020）：将图像切分为 16×16 图块，通过 Transformer 编码器建模全局关系，ImageNet 准确率达到 88.55%（需在 JFT-300M 大规模数据集预训练）。\nSwin Transformer（2021）：通过局部窗口注意力与层级下采样，在 COCO 目标检测任务中 mAP 达 58.7%。\nHybrid 模型：ConvNeXt（2022）将 CNN 与 Transformer 结合，通过大卷积核（7×7）模拟全局注意力，ImageNet 准确率达到 87.8%。\n\n\n\n\n\n\n二、卷积神经网络的基本结构在卷积神经网络（CNN）中，信息流经一系列结构化层级，从输入层开始，依次通过多个卷积层＋激活函数＋池化层模块，最后通过展平与全连接层将高维特征映射到输出类别或回归值。每个卷积层利用可训练的滤波器提取局部空间特征，激活函数（如 ReLU）引入非线性，池化层则下采样以降低维度并增强平移不变性；多次堆叠后，网络通过全连接层整合提取到的多级特征，并以 Softmax 或其他输出层形式给出最终预测。这种分层设计既减少了参数量，又能自动学习从低级到高级的抽象表示，是图像与其他网格化数据处理的核心架构。\n\n2.1 卷积层卷积层（Convolutional Layer）是卷积神经网络（CNN）的核心模块，用于从输入张量中提取局部空间特征。它由可学习的滤波器（kernel）组成，这些滤波器在输入的宽度和高度方向上滑动（convolve），并与输入做点积运算生成特征图（feature map），从而让网络能够识别图像中的边缘、纹理等模式并通过层级堆叠逐步捕获更复杂的特征。\n\n\n定义与核心概念\n卷积层的定义：卷积层是一种参数化的神经网络层，其参数为一组大小固定、可学习的滤波器（或卷积核），用于在输入张量的空间维度上进行局部点积运算，以产生输出特征图。\n可学习滤波器：每个滤波器在训练过程中通过反向传播不断更新，以自动学习对任务有用的空间特征，如边缘、角点或纹理。\n局部感受野：卷积核只与输入的一个小区域相连（称为感受野），这使得卷积层能够利用输入的空间结构，实现参数共享并减少计算量。每个卷积核只关注局部区域，参数共享＋局部感受野保证了对输入小幅平移的等变响应，而非重新学习特征。\n\n工作原理\n滑动窗口与点积运算：在前向传播时，每个滤波器沿宽度和高度方向逐步滑动，与其覆盖的输入子区域做点积，生成一个二维激活映射。\n激活图堆叠：所有滤波器的激活映射沿深度方向堆叠，构成卷积层的输出张量，其中每个通道对应一个滤波器检测到的特征。\n参数共享：同一个滤波器在输入的所有空间位置重复使用，实现了参数共享，极大地减少了模型参数数量和过拟合风险。\n\n关键超参数\n滤波器数量（Filters）：决定输出特征图的深度，也即网络在该层能学习到多少种不同特征。\n卷积核大小（Kernel Size）：通常为 3×3、5×5 等，控制感受野的大小；较大核可捕捉更大范围的上下文信息。\n步幅（Stride）：卷积核在滑动时每次移动的像素数，步幅越大输出尺寸越小，可用于下采样。\n填充（Padding）：在输入边缘补零（或其他值）以控制输出空间尺寸。在使用 PyTorch 等深度学习框架时，Padding 参数有三种选择：Full、Valid 和 Same。Full 表示需要填充，当卷积核与输入开始接触时进行卷积操作，Valid 表示不需要填充，Same 表示需要填充并保证输出与输入具有相同的尺寸。\n\n常见变体与应用\nConv1D / Conv2D / Conv3D：一维卷积用于时序数据，二维卷积用于图像，三维卷积用于体数据或视频。\n转置卷积（Conv2DTranspose）：又称反卷积，用于上采样，如图像生成与语义分割。\n可分离卷积（SeparableConv2D）：先做逐通道深度卷积（depthwise），再做 1×1 点卷积（pointwise），减少计算成本。\n条件卷积（CondConv）：为每个样本动态选择或生成卷积核，实现更灵活的特征提取。\n\n参考博客\n一文搞懂卷积核的基本概况 ！！-CSDN博客\n深度学习：卷积神经网络中的卷积核-CSDN博客\n一文读懂|CNN卷积神经网络:从基本概念、模型定义、训练、验证全流程指南\n\n2.2 激活函数激活函数（Activation）用于引入非线性，使网络能逼近更复杂函数，增强神经网络的非线性表达能力。\n2.2.1 常见激活函数Sigmoid 激活函数Sigmoid 激活函数存在梯度饱和效应问题，即 Sigmoid 激活函数两端梯度都趋于 0，因此在使用误差反向传播算法进行网络训练时，该区域的误差无法传递到前一层，从而导致网络训练失败。\n\n\nTanh 激活函数Tanh 激活函数同样存在梯度饱和效应问题，即 Tanh 激活函数两端梯度也都趋于 0，因此在使用误差反向传播算法进行网络训练时，该区域的误差也无法传递到前一层，从而导致网络训练失败。\n\n\nReLU 激活函数与 Sigmoid 激活函数相比，ReLU 在 x≥0 部分消除了梯度饱和效应，且 ReLU 的计算更简单，计算速度更快。但 ReLU 本身也存在缺陷，如果输入为负值，其梯度等于0，导致神经元死亡，将无法进行权重更新，进而无法完成网络训练。ReLU 是当前深度学习领域中最为常用的激活函数之一。\n\n\nPReLU 激活函数PReLU 激活函数的优点是比 Sigmoid 激活函数收敛快，解决了 ReLU 激活函数的神经元死亡问题。PReLU 激活函数的缺点是需要再学习一个参数 𝛼，工作量变大。\n\n\nELU 激活函数ELU 激活函数的优点是处理含有噪声的数据有优势，与 Sigmoid 激活函数相比更容易收敛。ELU 激活函数的缺点是计算量较大，与 ReLU 激活函数相比，收敛速度较慢。\n\n\nMaxout 激活函数\n\n2.2.2 常见激活函数一览\n\n\nSigmoid 与 Tanh\n数学形式\n输出范围\n主要特点\n\n\n\nSigmoid\n\n\n输出(0,1)，易饱和，梯度消失\n\n\nTanh\n\n\n输出(−1,1)，中心化，仍易饱和\n\n\n\n\n\nReLU 及其轻量变体\n数学形式\n输出范围\n主要特点\n\n\n\nReLU\n\n\n稀疏激活，无梯度消失（正区间），高效\n\n\nLeaky ReLU\n\n\n解决 ReLU 死亡神经元\n\n\nPReLU\n可学习\n\n自适应负斜率，性能优于 Leaky ReLU\n\n\n\n\n\n平滑型\n数学形式\n输出范围\n主要特点\n\n\n\nELU\n\n\n平滑负区间，收敛更快\n\n\nSELU\n\n\n自归一化，保持均值方差\n\n\nSoftplus\n\n\n平滑 ReLU，连续可导\n\n\n\n\n\n学习型与多态激活\n数学形式\n输出范围\n主要特点\n\n\n\nMaxout\n\n视参数而定\n学习多个线性片段，能近似任意凸函数\n\n\nSwish (SiLU)\n\n\n平滑、非单调，优于 ReLU\n\n\nGELU\n为标准正态\n\n平滑近似 ReLU，Transformer 默认\n\n\nMish\n\n\n平滑、双曲非线性，能提升鲁棒性\n\n\n2.2.3 激活函数选型建议\n浅层/小规模任务：可优先尝试 ReLU 或 Leaky ReLU，计算效率高且易调参。\n深层网络：若出现训练不稳定，可考虑 ELU、SELU 等自归一化函数，减少手动归一化操作。\n大型预训练模型：Swish、GELU 等平滑非单调激活在 Transformer 及大模型中已被验证效果卓越。\n资源受限场景：若算力或内存有限，ReLU 家族依旧是最经济的选择；Maxout、PReLU 等需付出更多参数与计算成本。\n\n2.3 池化层池化层是卷积神经网络（CNN）中用于下采样特征图的重要模块，通过在局部区域内对激活值进行聚合（如取最大值或平均值），有效减少空间维度、提取显著特征并增强平移鲁棒性。常见的池化操作包括最大池化、平均池化、全局池化及若干高级变体（Lp 池化、混合池化、随机池化等），它们在控制模型复杂度、抑制过拟合和提升计算效率方面发挥关键作用。\n\n\n2.4 归一化层归一化层（Normalization Layers）通过对神经网络中间激活或参数进行标准化处理，能够缓解训练中的内部协变量偏移、加速收敛并提升泛化性能。主要的归一化技术包括：\n\nBatch Normalization：基于小批量样本统计量对通道进行标准化，可显著稳定训练并支持更高学习率。\nLayer Normalization：对单个样本在特征维度上进行归一化，无需依赖批次大小，适用于 RNN/Transformer 等模型。\nInstance Normalization：对每个样本的每个通道独立归一化，常用于风格迁移等任务以去除实例级别的对比度信息。\nGroup Normalization：将通道分组后在组内归一化，兼顾了 BatchNorm 与 InstanceNorm 的优点，对小批量场景尤为有效。\nWeight Normalization：对网络权重向量进行重参数化，解耦权重的幅度与方向，加速优化但不依赖批量间统计。\n\n下面将对这些归一化层的定义、原理、参数和适用场景进行详尽阐述。\n\n1. Batch Normalization（批归一化）定义与公式\nBatchNorm 在训练时对每个 mini-batch 的激活按通道维度计算均值  和方差 ，并进行归一化：其中  和  为可学习的缩放和平移参数。\n参数与统计量\n\n可学习参数：每个通道对应的  与 ，共 2C 个。\n非训练量：用于推理的滑动平均  与 ，不参与梯度更新。\n\n优势与局限\n\n优势：大幅加速收敛、允许使用更大学习率，同时具有轻度正则化效果，可减少 Dropout 依赖 。\n局限：对批次大小敏感；在小批量或序列模型（如 RNN/Transformer）中效果不佳 。\n\n\n2. Layer Normalization（层归一化）定义与公式\nLayerNorm 对单个样本在特征维度上归一化：其中  和  分别为当前样本所有激活的均值与方差。\n参数\n\n可学习参数：同 BatchNorm，为每个特征维度提供 ，，共 2D 个（D 为隐藏维度）。\n统计量：无滑动平均，训练与推理一致。\n\n适用场景\n\nRNN/Transformer：无需批量间统计，稳定性更好；已成为 Transformer 中的标准配置 。\n\n\n3. Instance Normalization（实例归一化）定义与公式\nInstanceNorm 对每个样本的每个通道在空间维度 （H,W） 上独立归一化：$$\\hat{x}{i,c} = \\frac{x{i,c} - \\mu_{i,c}}{\\sqrt{\\sigma_{i,c}^2 + \\epsilon}},\\quad y_{i,c} = \\gamma,\\hat{x}{i,c} + \\beta$$其中 $\\mu{i,c}，\\sigma_{i,c}^2$ 为第 i 个样本第 c 通道的均值与方差 。\n应用\n\n风格迁移：抹去实例对比度信息，强化风格转移效果；但在大规模数据集上可能导致训练不稳定。\n\n\n4. Group Normalization（组归一化）定义与公式\nGroupNorm 将 C 个通道分为 G 组，分别在每组的空间位置和组内通道维度上计算 ，：$$\\hat{x}{i,g} = \\frac{x{i,g} - \\mu_{i,g}}{\\sqrt{\\sigma_{i,g}^2 + \\epsilon}},\\quad y_{i,g} = \\gamma,\\hat{x}_{i,g} + \\beta$$其中每组包含  个通道。\n优势\n\n与批次大小无关：可在小批次或单样本下稳定使用；\n中和 IN 与 LN：当 G=1 等价于 LayerNorm，G=C 等价于 InstanceNorm 。\n\n特点\n\n优化改进：改善了参数条件数，使收敛更快；\n无批量依赖：不引入样本间耦合，适用于任意 batch size。\n\n2.5 全连接层全连接层（Fully Connected Layer），又称密集层（Dense Layer），是神经网络中最基础且常用的层类型之一，其特点是上一层的每个神经元都与当前层的每个神经元相连。全连接层通过线性变换与激活函数组合，将输入特征映射到输出空间，为分类或回归任务提供决策能力。\n\n\n2.6 输出层输出层（Output Layer）是神经网络的最后一层，负责将隐藏层提取到的特征映射为模型的最终预测结果或类别概率。\n\n\n分类任务\n二分类：通常使用单个输出节点并配合 Sigmoid 激活，将输出映射到 [0,1] 区间，表征正类的概率。\n多分类：使用与类别数相同的输出节点，并在其后添加 Softmax 激活，将输出向量转换为概率分布，且各类别概率和为 1。\n\n回归任务\n连续值预测：输出层一般无需非线性激活（或使用恒等/线性激活），直接输出实数值，以最小化均方误差等回归损失函数。\n\n\n三、卷积神经网络的训练3.1 基本训练步骤以图像分类任务为例：\n\n用随机数初始化网络需要训练的参数（权重，偏置）\n将训练图像作为输入，进行卷积层、ReLU、池化层以及全连接层的前向传播，并计算每个类别的对应输出概率\n计算输出层的总误差：总误差 = -∑(目标概率×log(输出概率) )\n使用 BP 算法计算总误差相对于所有参数的梯度，并用梯度下降法或其他优化算法更新所有参数的值，以使输出误差最小\n\n3.2 要训练的参数\n\n\n模块\n参数类型\n数量计算公式\n\n\n\n卷积层\n滤波器权重  与偏置 \n;  ； 总\n\n\n批归一化（BatchNorm）\n缩放系数  与 平移系数 \n，，共 2C\n\n\n层归一化（LayerNorm）\n缩放系数  与 平移系数 \n，，共 2D （D为隐藏维度）\n\n\n组归一化（GroupNorm）\n缩放系数  与 平移系数 \n，，共 2C\n\n\nPReLU 激活\n负区间斜率 \n（全局）／C（按通道）／N（逐神经元）\n\n\n全连接层\n权重矩阵 、偏置向量 \n，，共 \n\n\n嵌入层（Embedding）\n嵌入矩阵\n\n\n\n自注意力子层\n投影矩阵 、 输出映射 \n若 ：总   \n\n\n\n符号说明：\n，：卷积层的输入/输出通道数\n，：卷积核高/宽\nC：通道数；C：隐藏维度；n，m：全连接层输入/输出维度\nV：词表大小；d：嵌入维度\n，，：Transformer 中的模型维度与键/值维度\n\n\n\n3.3 具体训练过程\n\n\n\n\n\n\n\n\n\n\n\n\n\n四、典型卷积神经网络LeNet-5\n\nAlexNet\n\n\n\nVGG-16\n\nGoogleNet\n\n\n\nChapter 3 – 循环神经网络循环神经网络的起源与发展一、起源阶段（1980s–1990s）\n1982 年：John Hopfield 提出 Hopfield 网络，引入双向对称的循环连接结构，用于联想记忆和优化问题，尽管其主要作为能量最小化模型，不直接用于序列建模，但极大地启发了后续RNN的循环架构设计思想。\n\n1986 年：Jordan 在输出层到隐藏层引入反馈连接，提出 Jordan Network，成为基于输出反馈的早期循环模型之一 。\n\n1990 年：Jeffrey Elman 在《Finding Structure in Time》中提出 Elman 网络（Simple Recurrent Network, SRN），通过隐藏层到上下文单元的自连接，正式奠定了现代 RNN 的雏形，并在语言学习与时间序列处理上展现出强大潜力。\n\n1991 年：Sepp 在博士论文中首次分析并正式提出梯度消失与爆炸问题，指出 RNN 因长链式乘积导致早期权重梯度呈指数衰减或爆炸，严重制约了长序列依赖的学习。\n\n\n\n二、技术突破阶段\n1997年：Sepp 与 Jürgen 在 NIPS ’96 会议论文中提出 LSTM，通过输入门、遗忘门与输出门的设计，以及恒定误差旋转（Constant Error Carousel），有效维持长期记忆流动，彻底缓解了梯度消失问题，成为长序列建模的里程碑。\n\n2014年：Cho 等提出 GRU，将 LSTM 的三门结构简化为更新门与重置门，保留长程依赖的建模能力，显著降低参数量与计算开销。\n\n\n\n三、挑战与转型阶段（2017–2020）\n2017年：“Attention Is All You Need” 提出 Transformer 架构，以自注意力机制替代全部或部分循环结构，实现了并行化的序列处理，迅速在机器翻译等任务上超越 RNN，并成为大规模预训练模型的基石。\n\n2018年：Quoc V. Le 团队提出 IndRNN，每个神经元拥有独立的循环权重，极大改善了梯度稳定性并支持更深的 RNN 堆叠。\n\n2020年：Lei D. et al. 提出 Simple Recurrent Unit (SRU)  通过与硬件（CUDA）深度集成，将 RNN 的推理效率提升数倍，适用于实时在线推断场景 。\n\n\n\n循环神经网络1. 定义循环神经网络（Recurrent Neural Network, RNN） 是一类专为处理序列数据设计的神经网络，其核心特点是引入循环连接（时间上的反馈机制），允许网络在不同时间步之间传递状态信息，从而捕捉序列中的时序依赖关系。\n\n适用场景：自然语言处理（文本生成、机器翻译）、语音识别、时间序列预测（股票价格、传感器数据）等。\n\n\n2. 核心结构RNN通过隐藏状态（Hidden State） 在不同时间步之间传递信息，结构包含以下关键组件：\n\n\n\n输入层（Input Layer）\n\n接收当前时间步的输入数据 （如一个单词的词向量、一个时间点的传感器值）。\n\n\n循环层（Recurrent Layer）\n\n隐藏状态更新公式：\n\n\n：输入到隐藏层的权重矩阵。\n：隐藏层到自身的权重矩阵（循环连接）。\n：前一时间步的隐藏状态。\n：激活函数（通常为 Tanh 或 ReLU）。\n\n\n\n\n输出层（Output Layer）\n\n根据当前隐藏状态生成输出 ：\n\n\n：隐藏层到输出层的权重矩阵。\n：激活函数（分类任务常用 Softmax，回归任务用线性函数）。\n\n\n\n\n\n\n3. 经典变体与改进为解决传统 RNN 的梯度消失/爆炸问题，研究者提出多种改进结构：\n\n\n\n模型\n核心机制\n优势\n\n\n\nLSTM（长短期记忆网络）\n引入输入门、遗忘门、输出门，控制信息流动。\n有效缓解梯度消失，支持长序列建模。\n\n\nGRU（门控循环单元）\n合并LSTM的门控机制为更新门与重置门，简化结构。\n计算效率高，适合资源受限场景。\n\n\n双向RNN（Bi-RNN）\n同时包含正向和反向循环层，捕捉前后文依赖。\n增强对上下文信息的理解（如机器翻译）。\n\n\n\n4. 训练方式RNN 通过反向传播通过时间（Backpropagation Through Time, BPTT） 进行训练，核心步骤如下：\n\n前向传播：\n按时间步展开网络，依次计算每个时间步的隐藏状态  和输出 。\n\n\n损失计算：\n对序列所有时间步的输出计算损失（如交叉熵损失、均方误差）。\n\n\n反向传播：\n从最后一个时间步向前传播梯度，更新权重矩阵 ，，。\n\n\n\n优化技巧：\n\n梯度裁剪（Gradient Clipping）：限制梯度范围，防止梯度爆炸。\n截断BPTT（Truncated BPTT）：仅回传固定长度时间步的梯度，降低计算复杂度。\n\n\n\n\n5. 局限性及解决方案\n\n\n问题\n原因\n解决方案\n\n\n\n梯度消失/爆炸\n长序列中梯度连乘导致数值不稳定。\n使用LSTM/GRU、梯度裁剪、残差连接。\n\n\n计算效率低\n时间步展开导致串行计算。\n使用Transformer（并行注意力机制）。\n\n\n短期记忆限制\n传统RNN难以捕捉长距离依赖。\n结合注意力机制（如LSTM+Attention）。\n\n\n\n长短时记忆网络1. 定义与背景长短时记忆网络（Long Short-Term Memory, LSTM） 是一种特殊的循环神经网络（RNN），由 Sepp Hochreiter 和 Jürgen Schmidhuber 于 1997 年提出，旨在解决传统 RNN 的长期依赖问题（梯度消失或爆炸）。通过引入门控机制，LSTM 能够选择性地保留或遗忘信息，从而有效捕捉长序列中的时序依赖关系。\n\n2. 核心结构LSTM 的核心是记忆单元（Cell State） 和三个门控机制：（注：示意图展示输入门、遗忘门、输出门与记忆单元的交互）\n\n遗忘门（Forget Gate）\n\n作用：决定从记忆单元中丢弃哪些信息。\n\n公式：\n\n：Sigmoid 函数，输出范围 [0, 1]。\n：上一时间步的隐藏状态，：当前输入。\n\n\n\n\n\n输入门（Input Gate）\n\n作用：决定当前输入中有多少信息存入记忆单元。\n\n公式：Misplaced & i_t=σ(W_i⋅[h_{t−1},x_t]+b_i)&amp;(门控信号) \\ \\hat{C_t}=tanh⁡(W_C⋅[h_{t−1},x_t]+b_C)&amp;(候选记忆) \n\n\n\n记忆单元更新\n\n公式：\n\n：逐元素乘法，结合遗忘与新增信息。\n\n\n\n\n输出门（Output Gate）\n\n作用：控制记忆单元对当前输出的贡献。\n\n公式：\n\n\n\n\n\n3. 训练方式\n反向传播通过时间（BPTT）：与传统 RNN 类似，但 LSTM 的梯度通过记忆单元和门控机制传播，显著缓解梯度消失问题。\n优化技巧：\n梯度裁剪：防止梯度爆炸。\n参数初始化：权重矩阵需合理初始化（如 Xavier 初始化）。\n\n\n\n\n4. 优势与局限性\n\n\n优势\n局限性\n\n\n\n1. 有效捕捉长期依赖关系。\n1. 参数量大，计算复杂度高。\n\n\n2. 梯度传播稳定，适合长序列任务。\n2. 训练时间较长，资源消耗大。\n\n\n3. 灵活控制信息流，适应复杂场景。\n3. 对超参数（如学习率）敏感。\n\n\n\n5. 典型变体与改进\n门控循环单元（GRU）\n合并输入门与遗忘门为更新门，去除输出门，参数更少，适合轻量化场景。\n\n\n双向LSTM（Bi-LSTM）\n结合正向与反向序列信息，增强上下文理解（如文本分类）。\n\n\nLSTM+注意力机制\n在记忆单元基础上引入注意力权重，提升关键信息聚焦能力。\n\n\n\n循环神经网络的变种\n\n\n\n\n\n\n\n\n\n\n\n","categories":["阅读笔记"],"tags":["Pytorch","深度学习"]},{"title":"leo_obs_simu 与 Rtklibnewest 软件测试记录","url":"/2025/05/12/leo_obs_simu%20%E4%B8%8E%20Rtklibnewest%20%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E8%AE%B0%E5%BD%95/","content":"leo_obs_simu 软件测试1. 示例文件测试\n基于甲方提供的示例文件进行仿真测试，验证固定率、收敛时间、定位精度等核心指标。\n测试结果：固定率≥98%，收敛时间均值20分钟，定位精度水平方向±2 cm、高程方向±4 cm，与文档声明性能一致，功能基线验证通过。\n\n2. 全球多纬度观测站仿真测试选取全球不同纬度、均匀分布的观测站进行测试，发现以下问题：\n\n性能波动显著：仅30%站点性能与示例文件相当（固定率95%±3%），其余站点固定率60%~90%、收敛时间15~50分钟，表明软件对站点环境（如多路径干扰）或配置参数（如采样间隔）敏感，需优化算法鲁棒性。\n文件格式兼容性不足：部分站点因OBS文件版本（如RINEX 3.03与3.05）不兼容导致流程中断，需升级程序以支持主流版本自适应解析。\n接收机钟差计算异常：5%站点钟差跳变超±10 ms，经代码调试确认钟差迭代函数未处理卫星钟数据异常值，需修复容错逻辑。\n仿真冗余耗时：当前同时仿真GNSS与LEO观测值，而真实场景中GNSS观测值应直接调用实测数据。冗余仿真导致耗时增加40%，需优化逻辑支持LEO单独仿真模式。\n误差模型理想化：现有策略未引入多路径效应、电离层延迟抖动等实际误差，仿真结果偏离真实场景，需集成误差模拟模块（如高斯噪声、硬件频偏）。\n\n3. 其他需优化问题\n跨平台路径兼容性：脚本中路径符号（如Windows \\与Linux /）未自动适配，需增加路径标准化处理函数。\n日志提示模糊：异常提示如“文件缺失”未指明具体文件（如/clk/brdm0010.21c），需细化至文件名及完整路径，提升调试效率。\n\nRtklibnewest 软件测试1. 功能测试\n配置参数校验\n\n验证配置文件（*.conf）中各项参数（如pos1-posmode、pos1-elmask、pos1-navsys、pos2-leoarmode等）能否正确识别、解析和生效。\n测试非法值、缺失字段、格式错误等异常场景，确认软件报错提示或使用默认值的逻辑符合预期 。\n\n\nPPP 流程\n\n在“静态模式”（ppp-static）和“动态模式”（ppp-kinematic）下，分别测试 GNSS-only、LEO-only 及 GNSS+LEO 三种解算组合的结果正确性。\n\n\n数据预处理与误差改正\n\n针对不同卫星系统（GPS/GLONASS/Galileo/BDS/LEO）测试观测文件读取及file-satantfile、file-rcvantfile、file-dcbfile、file-blqfile等改正文件生效情况。\n\n\n解算模式控制逻辑优化\n原来的程序在控制是否输出偏差投影时使用了两个配置参数，这两个参数的功能存在重叠，只需要保留一个，将需要输出的导航系统编号(2进制)作为控制参数，并且使用当前导航系统编号 &amp; 需要输出的导航系统编号的结果来决定是否输出偏差投影。\n\n\n\n\n2. 跨平台兼容性测试\nWindows 平台\nVS2022 打开、编译解决方案（sln），检查命令参数 (-k, -o 等) 在 GUI 调试与命令行两种模式下均可正常传参与运行。\n\n\n\n\n3. 性能与稳定性测试\n批处理性能\n多天、多站点数据并行处理时的吞吐量与时延测试，监控 CPU、内存、I/O 占用，确保在常见硬件配置下完成批处理在可接受时间内。\n高并发调用 Rtklibnewest.exe 或 Python 脚本时的资源隔离与进程管理，避免出现僵尸进程或内存泄漏。\n\n\n收敛速度与资源占用\n统计不同模式（浮点／固定）下的收敛时间（TTFF）和固定率，评估算法性能。\n在开启／关闭 LEO 协同模式时，对比收敛时间与计算开销。\n\n\n\n一般软件测试的主要逻辑软件测试的核心逻辑是通过系统化的方法验证软件的功能、性能和可靠性，确保其符合需求并满足用户预期。主要逻辑可分为以下步骤：\n\n1. 需求分析与测试目标定义\n输入：用户需求文档、设计规格说明书。\n逻辑：\n明确测试范围（功能模块、性能指标、兼容性要求等）。\n定义测试目标（如发现缺陷、验证功能正确性、评估性能极限）。\n确定测试类型（功能测试、性能测试、安全测试等）。\n\n\n\n\n2. 测试计划与策略制定\n逻辑：\n分层测试：按粒度划分测试阶段（单元测试→集成测试→系统测试→验收测试）。\n方法选择：\n黑盒测试：关注输入输出是否符合需求（如等价类划分、边界值分析）。\n白盒测试：基于代码逻辑设计用例（如路径覆盖、条件覆盖）。\n自动化测试：对重复性高或复杂场景使用工具（如Selenium、JUnit）。\n\n\n资源分配：规划测试环境、工具、人员及时间。\n\n\n\n\n3. 测试用例设计与数据准备\n逻辑：\n正向用例：验证功能正常场景（如用户登录成功）。\n反向用例：覆盖异常场景（如输入非法字符、网络中断）。\n边界用例：测试极限条件（如最大并发用户数、数据溢出）。\n数据驱动：准备多样化测试数据（有效值、无效值、空值）。\n\n\n\n\n4. 测试执行与缺陷管理\n逻辑：\n执行用例：按计划运行测试，记录实际结果。\n缺陷跟踪：\n发现缺陷：记录复现步骤、环境、现象。\n优先级划分：按严重程度（崩溃→功能失效→界面问题）分类。\n回归测试：修复缺陷后重新验证相关功能。\n\n\n\n\n\n\n5. 结果分析与报告输出\n逻辑：\n覆盖率分析：统计代码/需求覆盖比例，确保无遗漏。\n性能评估：对比响应时间、吞吐量等指标是否达标。\n风险提示：标注未覆盖场景或潜在隐患。\n测试报告：总结测试过程、缺陷分布、改进建议。\n\n\n\n\n6. 测试闭环与持续改进\n逻辑：\n反馈开发：将测试结果同步至开发团队，优化代码质量。\n流程复盘：分析测试效率不足（如用例冗余、工具瓶颈）。\n自动化维护：更新自动化脚本以适应需求变更。\n\n\n\n\n","categories":["阅读笔记"],"tags":["软件测试","PPP","leo_obs_simu","Rtklibnewest"]},{"title":"游戏测试开发基础知识","url":"/2025/05/19/2025-05-19-%E6%B8%B8%E6%88%8F%E6%B5%8B%E8%AF%95%E5%BC%80%E5%8F%91%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","content":"一、自我介绍与测试理解1. 为什么选择测试开发岗位？示范回答：\n\n我本身就很喜欢玩游戏，对游戏体验也比较敏感，尤其是遇到卡顿、bug 或者数值异常的时候，我会特别想知道背后的原因并设法去解决。我选择测试开发这个方向，是因为它结合了开发和测试的优势——既可以写代码开发测试工具，也能站在用户的角度关注游戏的整体质量，这种技术性和实用性结合的工作方式对我很有吸引力。\n\n扩展讨论：可结合自己的项目经历，说明如何通过自动化脚本或工具提高测试效率，并简要阐述测试开发在 CI/CD 流水线中的角色。\n2. 游戏测试 vs 互联网测试开发的区别？示范回答：\n\n我觉得游戏测试更偏体验导向，需要模拟玩家的各种操作路径，比如技能连招、场景切换、断网重连等，这些都比较依赖场景组合和交互逻辑。而互联网测试，比如电商或者工具类产品，更关注接口正确性、数据一致性、高可用性，很多时候是靠接口自动化和性能压测来保障质量的。所以在游戏测试里，我认为稳定性、实时性和用户体验是非常重要的考察维度。\n此外，游戏测试还涉及到很多特有问题，比如资源加载、帧率控制、弱网环境适配、多端一致性、硬件兼容性等等。这些在传统互联网应用中可能不会同时出现。测试方法上，也更依赖模拟器、引擎层的接口测试、Unity/UE 自动化工具链、甚至用 AI 做自动游玩测试。\n\n扩展讨论：可提及游戏多媒体资源加载、帧率稳定性测试，以及网络延迟抖动对游戏体验的特殊影响。\n3. 你印象最深的游戏及测试思路？示范回答：\n\n我深入玩过《明日方舟》，每次活动更新的时候都会去 PRTS 网站查看新角色和新敌人的数据。\n\n1. 干员测试\n目标：验证新干员的技能、属性和潜能是否符合设计预期，确保在各种战斗环境下的稳定性和表现。\n测试方法：\n\n技能效果验证：在不同关卡中测试技能的实际效果，确保描述与实际一致。\n属性边界测试：测试干员在极限条件下的表现，如最大/最小攻击力、生命值等。\n潜能提升测试：验证潜能提升对干员性能的影响，确保数值增长符合预期。\n\n示例：在测试干员“卡达”时，逐级提升其技能专精等级，并在实际战斗中观察其输出变化，确保每一级提升都带来预期的性能增强。\n\n2. 关卡测试\n目标：确保新关卡的设计合理，敌人配置、地形布局和任务目标等元素能够提供适当的挑战性和趣味性。\n测试方法：\n\n敌人行为测试：观察敌人的移动路径、攻击模式和技能使用，确保其行为符合设计。\n地形交互测试：测试干员与地形元素的交互，如高地、阻挡物等，确保没有穿模或卡位等问题。\n任务目标验证：确保关卡的胜利条件和失败条件能够正确触发。\n\n示例：在测试H9-1关卡时，重点关注敌人群体的抗性配置和攻击频率，确保玩家需要合理配置干员阵容以应对挑战。 \n\n3. 系统测试\n目标：验证游戏系统功能的稳定性和一致性，确保玩家在不同设备和网络环境下的良好体验。\n测试方法：\n\n资源加载测试：测试游戏在不同网络条件下的资源加载速度和稳定性。\n存档系统测试：验证游戏的存档和读取功能，确保数据不会丢失或损坏。\n多设备兼容性测试：在不同型号的设备上测试游戏的运行情况，确保界面显示和操作响应一致。\n\n示例：在测试《明日方舟：终末地》的工业生产系统时，确保玩家在完成“通电”任务后，能够顺利解锁并使用新的生产设施，如滑索设备和储物箱等。\n\n4. 用户体验测试\n目标：从玩家的角度出发，评估游戏的可玩性、界面友好性和整体体验，提出优化建议。\n测试方法：\n\n新手引导测试：评估游戏对新玩家的引导是否清晰，是否能够帮助玩家快速上手。\n界面交互测试：测试游戏界面的布局、按钮响应和信息展示，确保操作直观。\n反馈机制测试：验证游戏对玩家操作的反馈是否及时和准确，如技能释放的视觉和音效反馈。\n\n\n扩展讨论：推荐带图示的测试流程图或概率模型，可用于展现量化思路。\n\n二、数据结构与算法1. 如何打印一个字符串的全排列？示范回答：\n\n可用递归交换法：从索引 i 开始，对每个 j≥i 交换 s[i] 与 s[j]，递归处理下一个位置，回溯后还原交换。\ndef permute(s):    res = []    def backtrack(path, remaining):        if not remaining:            res.append(path)        else:            for i in range(len(remaining)):                backtrack(path + remaining[i], remaining[:i] + remaining[i+1:])    backtrack(\"\", s)    return res# 示例result = permute(\"abc\")for p in result:    print(p)\n扩展讨论：可补充非递归字典序法实现，以及如何在面试中分析时间复杂度 O(n!×n)O(n! \\times n)O(n!×n)。\n2. 八皇后问题怎样求解？示范回答：\n\n用回溯法，逐行放置皇后，记录列、主对角线、副对角线的占用情况；当某行无合适列时回溯，上限解空间约 O(n!)。\ndef solve_n_queens(n):    def is_safe(positions, row, col):        for i in range(row):            if positions[i] == col or \\               abs(positions[i] - col) == abs(i - row):                return False        return True    def backtrack(row, positions, solutions):        if row == n:            solutions.append(positions[:])            return        for col in range(n):            if is_safe(positions, row, col):                positions[row] = col                backtrack(row + 1, positions, solutions)    solutions = []    positions = [-1] * n    backtrack(0, positions, solutions)    return solutions# 示例：求解八皇后问题solutions = solve_n_queens(8)print(f\"共有 {len(solutions)} 种解法。\")for sol in solutions:    print(sol)\n扩展讨论：可提及基于位运算的高效实现，适用于大规模 n。\n3. 给定 1000 个数，找出最大的 100 个，若数据量为 1T 如何处理？示范回答：\n\n编号 0–999 数据可用大小为 M=1000 的最小堆维护前 100 大元素；当数据量 1T 时，可采用外部归并排序：先分块排序、生成有序子文件，再多路归并并取前 100。\n当数据量达到 1TB 时，无法将所有数据加载到内存中，需要采用以下方法：方法一：使用最小堆进行流式处理遍历数据流，使用最小堆维护当前最大的 100 个数。python复制编辑import heapqdef find_top_k(file_path, k):    min_heap = []    with open(file_path, 'r') as file:        for line in file:            num = int(line.strip())            if len(min_heap) &lt; k:                heapq.heappush(min_heap, num)            else:                if num &gt; min_heap[0]:                    heapq.heappushpop(min_heap, num)    return sorted(min_heap, reverse=True)这种方法适用于数据以文本形式存储，每行一个数字的情况。方法二：MapReduce 分布式处理将数据分割成多个块，分别在不同的节点上处理每个块，找出每个块的前 100 个最大数，然后将所有结果合并，再找出最终的前 100 个最大数。这种方法适用于使用 Hadoop、Spark 等分布式计算框架的场景。方法三：使用外部排序将数据分割成多个可以加载到内存的小块，对每个小块进行排序并保存到磁盘，然后使用多路归并排序的方法合并所有小块，找出前 100 个最大数。这种方法适用于无法使用分布式计算框架，但可以使用磁盘进行中间存储的场景。\n扩展讨论：讨论内存限制下的 MapReduce 或 Spark 实现思路。\n\n三、计算机网络1. 在浏览器中输入 URL 并按下回车之后会发生什么\n\nURL 解析：输入 URL 后，浏览器会解析出协议、主机、端口、路径等信息，并构造出一个 HTTP 请求，浏览器会根据请求头判断是否有 HTTP 缓存，并根据是否有缓存决定是从服务器获取资源还是使用缓存资源。\nDNS 域名解析：在发送 HTTP 请求之前，浏览器需要知道要访问的网页对应的 IP 地址，这就需要使用 DNS 域名解析服务。\n建立 TCP 连接：客户端和服务器之间进行 HTTP 请求和 HTTP 响应的过程中，需要建立 TCP 连接，TCP 连接需要进行三次握手。\n浏览器发送 HTTP/HTTPS 请求到 WEB 服务器：一般请求报文包含：请求方法，资源类型，语言，编码，cookie。\n服务器处理 HTTP 请求报文并返回 HTTP 响应报文：服务器会接收请求并将其传递给请求处理程序并发送 HTTP 响应，一般响应报文包含：请求的网页以及状态码，压缩类型，设置的 cookie。\n浏览器渲染页面\n断开 TCP 连接：客户端和服务器之间断开连接需要进⾏四次挥⼿。\n\n\n2. DNS 基本概念DNS 是什么\nDNS（Domain Name System）是一种用于将域名（例如 www.baidu.com）转换为 IP 地址（例如 220.181.111.188 ）的分布式系统。在互联⽹上，计算机和其他⽹络设备使用 IP 地址来相互识别和通信。然⽽，IP 地址是⼀串数字，不太方便使用和记忆，所以使用域名来代替复杂的 IP 地址。\n\nDNS 解析过程\n\n先查询浏览器缓存是否有该域名对应的 IP 地址；\n如果浏览器缓存中没有，则会去计算机本地的 Host 文件中查询是否有对应的缓存；\n如果 Host 文件中也没有，则会向本地的 DNS 服务器发送一个 DNS 查询请求；\n如果本地 DNS 解析器有该域名的 ip 地址，就会直接返回，如果没有缓存该域名的解析记录，它会向根 DNS 服务器发出查询请求。根 DNS 服务器并不负责解析域名，但它能告诉本地 DNS 解析器应该向哪个顶级域（.com/.net/.org）的 DNS 服务器继续查询；\n本地 DNS 解析器接着向指定的顶级域名 DNS 服务器发出查询请求。顶级域 DNS 服务器也不负责具体的域名解析，但它能告诉本地 DNS 解析器应该前往哪个权威 DNS 服务器查询下一步的信息；\n本地 DNS 解析器最后向权威 DNS 服务器发送查询请求。权威 DNS 服务器是负责存储特定域名和 IP 地址映射的服务器。当权威 DNS 服务器收到查询请求时，它会查找 “example.com” 域名对应的IP地址，并将结果返回给本地 DNS 解析器；\n本地 DNS 解析器将收到的 IP 地址返回给浏览器，并且还会将域名解析结果缓存在本地，以便下次访问时更快地响应；\n浏览器使用该 IP 地址与目标服务器建立连接，开始获取网页内容。\n\n\n递归查询和迭代查询\n\n递归查询和迭代查询是在 DNS 解析过程中用于获取域名解析信息的两种不同方法。递归查询适合普通用户和客户端，而迭代查询适用于 DNS 服务器之间的通信。\n递归查询在递归查询中，DNS 客户端（通常是本地 DNS 解析器）向上层 DNS 服务器（如根域名服务器、顶级域名服务器）发起查询请求，并要求这些服务器直接提供完整的解析结果。递归查询的特点是，DNS 客户端只需要发送一个查询请求，然后等待完整的解析结果。上层 DNS 服务器会自行查询下一级的服务器，并将最终结果返回给 DNS 客户端。迭代查询在迭代查询中，DNS 客户端向上层 DNS 服务器发起查询请求，但不要求直接提供完整的解析结果。相反，DNS 客户端只是询问上层服务器一个更高级的域名服务器的地址，然后再自行向那个更高级的服务器发起查询请求，以此类推，直到获取完整的解析结果为止。\n\n\n3. HTTP 基本概念\nHTTP（HyperText Transfer Protocol）是用于客户端和服务器之间通信的协议，采用请求-响应模式，基于 TCP/IP 协议。\nHTTPS 是在 HTTP 的基础上加入了 SSL/TLS 加密层，提供数据加密、身份验证和数据完整性保护，防止中间人攻击和数据窃听。\n\n常见的 HTTP 方法\n\nGET：获取资源\nPOST：提交数据\nPUT：更新资源\nDELETE：删除资源\nPATCH：部分更新资源\nHEAD：获取响应头\nOPTIONS：获取服务器支持的方法\n\n\nGET 与 POST 的区别\n\nGET 请求参数附加在 URL 中，数据长度有限，适用于获取数据；\nPOST 将参数放在请求体中，数据长度较大，适用于提交数据。\n\n\n常见的 HTTP 状态码\n\n200 OK：请求成功\n301 Moved Permanently：永久重定向\n302 Found：临时重定向\n400 Bad Request：请求语法错误\n401 Unauthorized：未授权\n403 Forbidden：禁止访问\n404 Not Found：资源未找到\n500 Internal Server Error：服务器内部错误\n\n\n4.TCP 基本概念\n\nTCP 三次握手\n\n第一次握手，SYN 报文：客户端随机初始化序列号client_isn，放进 TCP 首部序列号段，然后把 SYN 置 1。把 SYN 报文发送给服务端，表示发起连接，之后客户端处于 SYN-SENT 状态。\n\n第二次握手，SYN+ACK 报文：服务端收到客户端的 SYN 报文之后，会把自己随机初始化的序号 server_isn 放进 TCP 首部序列号段，确认应答号填入 client_isn+1，把 SYN 和 ACK 置为 1。把 SYN+ACK 报文发送给客户端，然后进入 SYN-RCVD 状态，表示服务器接受了客户端的请求，并希望建立连接。\n\n第三次握手，ACK 报文：客户端接收到服务端报文之后，还要向服务端回应最后一个应答报文。首先该应答报文 TCP 首部 ACK 标志位置为 1，其次确认应答号字段填入 server_isn+1，最后把报文发送给服务端。这次报文可以携带客户端到服务端的数据，之后客户端处于ESTABLISHED状态，表示客户端已经准备好与服务器进行数据传输。\n\n\n为什么需要三次握手？\n阻止重复历史连接的初始化：当因为网络阻塞原因，客户端向服务端发送了两次 SYN 报文，旧的 SYN 报文先到达服务端，服务端返回一个 ACK+SYN 报文，客户端接收到后根据自己上下文判断这是一个历史连接，因为序列号过期或者超时，此时客户端会发送 RST 报文给服务端，表示中止这次连接。当新的 SYN 报文抵达后，客户端和服务器之前进行正常的三次握手。\n同步双方的初始序列号：两次握手只能保证一方的初始序列号被对方成功接收，没办法保证双方的初始序列号都能被确认接收。四次握⼿也能保证双方的初始化序号同步，但是可以省略成三次。\n两次握手：无法阻止历史连接的建立，会造成双方资源的浪费，也无法可靠地同步双方序列号；\n四次握手：三次握手就已经足够理论上最少可靠连接的建立，不需要更多的通信次数。\n\n\nTCP 四次挥手\n\n第一次挥手：客户端关闭连接，发送一个 TCP 首部 FIN 被置为 1 的 FIN 报文给服务端，此时客户端处于FIN_WATI1状态；\n\n第二次挥手：服务端收到以后，向客户端发送 ACK 应答报文，且把客户端的序列号值+1 作为 ACK 报文的序列号值，表示已经收到客户端的报文，此时服务端处于CLOSE_WAIT状态；\n\n第三次挥手：等待服务端处理完数据，向客户端发送 FIN 报文，此时服务端处于LAST_ACK状态；\n\n第四次挥手：客户端收到 FIN 报文之后向服务端发送 ACK 应答报文，之后客户端处于TIME_WAIT状态；\n\n服务器收到 ACK 报文之后，进入CLOSE状态，服务器完成连接的关闭，客户端在经过2MSL(最大报文最大生成时间)之后，自动进入CLOSE状态，客户端也完成连接的关闭。\n\n\n为什么需要四次挥手？关闭连接时，服务端可能还要数据需要处理和发送，所以先回⼀个 ACK 应答报文，等到其不再发送数据时，才发送 FIN 报⽂给客户端表示同意关闭连接。\n为什么需要 TIME_WAIT 状态？\n防止历史连接中的数据被后面相同四元组的连接错误的接收，确保旧连接的报文在网络中消失，避免对新的连接造成干扰。\n确保服务器收到最后的 ACK 报文，如果服务器没有收到 ACK 报文，会重发 FIN 报文，客户端在 TIME_WAIT 状态下可以重新发送 ACK 报文。\n\n\nTCP 的重传机制\n\n超时重传：当超过指定的时间后，没有收到对方的确认 ACK 报文，就会重发该数据；\n快速重传：不以时间，而是以数据驱动重传。当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。 \n选择性确认：SACK 机制允许接收方告知发送方已成功接收的非连续数据段，帮助发送方仅重传丢失的数据段。\n工作原理：\n接收方在 ACK 中添加 SACK 选项，指明已接收的非连续数据段的序号范围。\n发送方根据 SACK 信息，仅重传丢失的数据段，避免重复发送已接收的数据。\n\n\n示例：\n发送方发送数据段1、2、3、4、5。\n数据段 2 丢失，接收方收到 1、3、4、5，发送 ACK 2，并在 SACK 选项中指明已接收的 3-5。\n发送方根据 SACK 信息，仅重传数据段 2。\n\n\n优点：\n提高了网络带宽利用率，减少了不必要的重传。\n\n\n注意事项：\nSACK 需双方支持，并在 TCP 选项中协商启用。\n\n\n\n\n重复 SACK：D-SACK 是 SACK 的扩展，允许接收方告知发送方已接收的重复数据段，帮助发送方识别不必要的重传。\n工作原理：\n接收方在 SACK 选项中报告已接收的重复数据段的序号范围。\n发送方根据 D-SACK 信息，识别并调整重传策略，避免重复发送。\n\n\n示例：\n发送方误认为数据段 2 丢失，重传数据段 2。\n接收方已接收数据段 2，收到重复数据段 2 后，在 SACK 中报告重复接收的数据段 2。\n发送方根据 D-SACK 信息，调整重传策略。\n\n\n优点：\n优化了重传策略，减少了不必要的重传，提高了网络效率。\n\n\n注意事项：\nD-SACK 需双方支持，并在 TCP 选项中协商启用。\n\n\n\n\n\n\nTCP 的流量控制\nTCP 流量控制的基本原理是使用滑动窗口机制，其中接收方可以通过调整窗口大小来告诉发送方其当前处理数据的能力。\n\n接收窗口：接收方维护一个接收窗口，表示可以接收的数据段的范围。窗口大小可以根据接收方的处理能力进行调整。\n通告窗口大小：接收方通过 TCP 报文中的确认信息，通告当前的接收窗口大小给发送方。发送方会根据这个窗口大小来控制发送数据的速率。\n窗口滑动：随着数据的发送和接收，窗口会不断地向前滑动：\n发送方：当收到接收方的ACK确认后，已确认的数据从发送窗口中移除，窗口前移，允许发送新的数据。\n接收方：当应用层读取了接收缓冲区中的数据，释放了空间，接收窗口前移，并在ACK中通告新的窗口大小。\n\n\n发送速率控制：发送方会根据接收方通告的窗口大小来控制发送数据的速率。如果接收窗口变小，表示接收方的处理能力减弱，发送方会减慢发送速率，避免数据拥塞。\n动态调整：TCP 流量控制是动态的，适应网络和接收方的变化。如果网络拥塞或接收方的处理速度变慢，流量控制可以适时地减少发送速率。\n\n\nTCP 的拥塞控制\n1. 慢启动（Slow Start）在连接开始或发生超时重传后，TCP 进入慢启动阶段。此时，拥塞窗口从一个最大报文段大小（MSS）开始，每收到一个 ACK，拥塞窗口增加一个 MSS，实现指数增长。当拥塞窗口达到慢启动阈值（ssthresh）时，转入拥塞避免阶段。\n2. 拥塞避免（Congestion Avoidance）在此阶段，拥塞窗口以线性方式增长，每经过一个往返时间（RTT），拥塞窗口增加一个 MSS。这种增长方式有助于探测网络的最大承载能力，避免引发拥塞。\n3. 快速重传（Fast Retransmit）当发送方连续收到三个相同的 ACK 时，认为某个数据段可能丢失，立即重传该数据段，而不等待重传定时器超时。这种机制可以加快丢失数据的恢复速度。\n4. 快速恢复（Fast Recovery）在快速重传后，TCP 进入快速恢复阶段。此时，将慢启动阈值设置为当前拥塞窗口的一半，拥塞窗口也调整为该值。随后，进入拥塞避免阶段，继续线性增长。\n\n5.TCP 与 UDP 的区别\n\n连接\nTCP 是面向连接的，在传输前需要三次握手建立连接，在传输结束时需要四次挥手关闭连接。\nUDP 不需要连接，直接发送数据包，没有连接建立和关闭的过程。\n\n\n服务形式\nTCP 是一对一的通信。在 TCP 连接中，一台客户端与一台服务器之间建立一条连接，进行双向通信。\nUDP 是无连接的，一个 UDP 包可以被广播到多个目标主机，或者从多个源主机接收 UDP 包， 可以是一对一、一对多或多对多的通信，这使得 UDP 适用于多播和广播应用。\n\n\n可靠性\nTCP 保证数据可靠交付，拥有确认应答和重传机制，无重复、不丢失、按序到达;\nUDP 尽可能交付，发送数据后不会关心数据包是否成功到达接收方，不会进行重传，不保证可靠性。\n\n\n流量控制和拥塞控制\nTCP 拥有流量控制、拥塞控制，确保数据发送的速率不会超过接收方的处理能力，并防止网络拥塞。\nUDP 不进行流量控制和拥塞控制，数据发送的速率不受限制。\n\n\n首部开销\nTCP 的首部大小通常为 20 字节，但在选项字段被使用的情况下，可能会更大。TCP 首部包含源端口号、目标端口号、序列号、确认号、窗口大小、校验和等字段。\nUDP 的首部大小固定为 8 字节。UDP 首部包含源端口号、目标端口号、包长度和校验和字段（各16位）。\n\n\n传输方式\nTCP 基于字节流，没有边界，但是保证传输顺序和可靠性;\nUDP 继承了 IP 层特性，基于数据包，有边界可能出现乱序和丢包。\n\n\n分片方式\nTCP 数据大于 MSS 时会在 TCP 层将数据进行分片传输，到达目的地后同样在传输层进行合并，如果有某个片丢失则只需要重传丢失的分片即可；\nUDP 数据大于 MTU 时会在 IP 层分片，则会在 IP 层合并，如果某个 IP 分片丢失，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。\n\n\n\n\n6. Cookie 和 Session\nCookie 和 Session 都用于管理用户的状态和身份，Cookie 通过在客户端记录信息确定用户身份，Session 通过在服务器端记录信息确定用户身份。\n\nCookie\n\n  Cookie 是存储在用户浏览器中的小型文本文件，用于在用户和服务器之间传递数据。通常，服务器会将一个或多个 Cookie 发送到用户浏览器，然后浏览器将这些 Cookie 存储在本地。  服务器在接收到来自客户端浏览器的请求之后，就能够通过分析存放于请求头的 Cookie 得到客户端特有的信息，从而动态生成与该客户端相对应的内容。2. #### Session\n  客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是 session。Session 主要用于维护用户登录状态、存储用户的临时数据和上下文信息等。\n\nCookie 和 Session 有什么区别？\n\n\n存储位置：Cookie 数据存储在用户的浏览器中，而 Session 数据存储在服务器上。\n数据容量：Cookie 存储容量较小，一般为几KB。Session 存储容量较大，通常没有固定限制，取决于服务器的配置和资源。\n安全性：由于 Cookie 存储在用户浏览器中，因此可以被用户读取和篡改。相比之下，Session 数据存储在服务器上，更难被用户访问和修改。\n传输方式：Cookie 在每次 HTTP 请求中都会被自动发送到服务器，而Session ID 通常通过 Cookie 或 URL 参数传递。\n\n\n7. TCP 连接时拔掉网线\n1. 不会立即断开连接\nTCP 是有状态的连接协议，它依赖超时机制来判断对方是否“挂了”。\n拔网线后，TCP 不会立刻知道连接已断，它仍会尝试重传数据。\n应用层（如浏览器、Socket 程序）也不会立即报错，直到：\n写数据失败（多次重试后）\n读操作超时或收到 RST 包\n系统检测网络断开，关闭连接（但可能很慢）\n\n\n\n\n2. 发送数据：写操作可能会“阻塞”或抛异常\n如果你在 TCP 连接上 send 数据：\n内核会先将数据写入发送缓冲区；\n由于无法确认接收方是否收到了，会触发TCP 重传机制；\n重传失败后（默认会重试几分钟），才会报错，如：Broken pipe、Connection reset by peer。\n\n\n\n\n3. 接收数据：读操作可能一直“挂起”\nrecv() 会阻塞，直到：\n有数据到达；\n超时；\n连接被对方关闭（通常不会主动发生，因网线已断）；\n系统检测网络断开并返回错误（例如返回 ECONNRESET）。\n\n\n\n\n\n系统检测断网需要时间\n\n\nTCP 是通过ACK + 超时 + 重传机制检测对方是否在线；\n检测网络断开是个延迟过程，比如 Linux 默认 TCP 超时可能是 几分钟；\n可通过 SO_KEEPALIVE 设置心跳检测（默认也可能是 2 小时）。\n\n\n\n如何快速检测连接是否断了？\n\n\n设置 socket 超时时间：settimeout()；\n使用 select() 或 poll() 监听可读可写状态；\n开启 TCP keep-alive 并降低间隔和重试次数（需要操作系统层配置）；\n应用层主动心跳，比如每隔 5 秒发一次 ping 请求。\n\n\n\n四、操作系统1. 进程与线程\n进程（Process）是操作系统分配资源的基本单位，拥有独立地址空间；\n线程（Thread）是进程的执行单元，线程间共享进程资源，切换开销更小但需同步机制避免竞态条件\n\n2. 死锁\n死锁是多个进程或线程因互占资源而永久等待的现象，必要条件有互斥、占有且等待、不可剥夺和循环等待四个条件。可通过资源有序分配、银行家算法等策略避免。\n\n3. 守护进程（Daemon）\n守护进程是运行在后台、脱离终端控制的常驻程序，通常在系统启动时由 init（或 systemd）启动，用于处理日志、计划任务、网络服务等。\n\n4. 缓存\n缓存（Cache）是位于请求者与原始数据存储之间的高速存储层，用于临时保存热点数据或计算结果，以加速后续访问并减轻后端压力。缓存命中（Cache Hit）表示请求数据在缓存中已存在，可立即返回；缓存未命中（Cache Miss）则需回源获取并更新缓存。\n提升缓存命中率的核心策略\n\n合理配置与预热：为不同类型资源（尤其静态资源）设置合适的 Cache-Control/Expires 超时时间，并在系统启动或更新后预先加载热点数据到缓存（Cache Warming），避免冷启动带来的高未命中率。\n优化替换与预取：采用高效的缓存替换算法（如 LRU、LFU 或自适应策略 DRRIP）来淘汰冷数据，并结合硬件或软件预取（Prefetching）技术，提前加载可能被访问的数据，进一步提高命中概率。\n\n\n五、测试基础理论1. 黑盒测试\n定义\n黑盒测试（Black-box Testing）又称功能测试，不关注软件内部实现，而是依据需求规格说明从外部验证功能是否符合预期。\n\n常用方法\n等价类划分：将输入分为有效/无效等价类，从每类中选取代表值测试。\n边界值分析：重点测试输入域的边界点及附近值，以捕捉边界条件缺陷。\n因果图/判定表：根据条件与动作之间的因果关系或判定表，系统性生成测试用例。\n错误推测（猜错法）：基于经验推测易出错场景进行测试。\n场景法（流程测试）：模拟玩家完整操作流程，覆盖常见业务场景与异常场景。\n\n在游戏测试中的应用\n功能测试：如角色移动、战斗逻辑、UI 按钮响应等。\n兼容测试：不同分辨率、机型、系统版本下的功能表现。\n安全/抗作弊：异常数据输入或脚本篡改检测。\n\n\n2. 白盒测试\n定义\n白盒测试（White-box Testing），又称结构测试或逻辑驱动测试，需要了解代码内部结构，通过覆盖逻辑路径来验证软件的内部行为。\n\n常用方法\n语句覆盖：保证每条可执行语句至少被执行一次。\n分支覆盖：测试所有可能的条件分支（true/false）。\n路径覆盖：执行所有独立的逻辑路径，针对循环和嵌套条件进行组合测试。\n条件覆盖：确保每个判断条件的所有取值都被测试。\n循环覆盖：验证循环的零次、一次和多次迭代情况。\n\n在游戏测试中的应用\n脚本/引擎代码：如碰撞检测函数、资源加载模块、AI 行为逻辑等。\n内存管理：检测内存泄漏与越界访问。\n\n\n","categories":["阅读笔记"],"tags":["新玩具"]},{"title":"《面向开发者的LLM入门教程》阅读笔记","url":"/2025/05/12/LangChain%20%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97/","content":"面向开发者的提示工程​\t随着 ChatGPT 等 LLM (大语言模型) 的出现,自然语言处理的范式正在由 Pretrain-Finetune (预训练微调) 向 Prompt Engineering (提示工程) 演变。对于具有较强自然语言理解、生成能力，能够实现多样化任务处理的 LLM 来说，一个合理的 Prompt 设计极大地决定了其能力的上限与下限。Prompt  Engineering，即是针对特定任务构造能充分发挥大模型能力的 Prompt 的技巧。要充分、高效地使用  LLM，Prompt Engineering 是必不可少的技能。\n​\t本部分的主要内容包括：书写 Prompt 的原则与技巧；文本总结 (如总结用户评论)；文本推断 (如情感分类、主题提取)；文本转换 (翻译、自动纠错)；扩展 (如书写邮件) 等。\n一、提示原则（一）编写清晰、具体的指令1.使用分隔符清晰地表示输入的不同部分在编写 Prompt 时，我们可以使用各种标点符号作为分隔符，将不同的文本部分区分开来，避免混淆指令、上下文、输入输出等。分隔符包括\"\"\"，&lt; &gt;，&lt;tag&gt; &lt;/tag&gt;，:，只需要起到明确的隔断作用。\n分隔符还可以防止提示词注入（Prompt Rejection）。提示词注入是用户输入的文本可能包含与预设的 Prompt 冲突的内容，导致模型产生毫无关联的输出。\n2.寻求结构化的输出可以让模型按照要求生成结构化的输出，例如 JSON、HTML 等，结构化输出有利于在代码中进一步解析和处理，例如在 Python 中将其读入字典或列表中进行处理。\n3.要求模型检查是否满足条件如果任务包含不一定能满足的假设(条件)，我们可以告诉模型先检查这些假设，如果不满足，则会指出并停止执行后续的完整流程。还可以考虑可能出现的边缘情况及模型的应对，以避免意外的结果或错误发生。这样做可以使模型按照假设对输入进行判断和分析，有利于模型理解输入内容并按照假设输出预期内容。\n4.提供少量示例Few-shot prompting，即在要求模型执行实际任务之前，给模型一两个已完成的样例，让模型了解我们的要求和期望的输出样式。利用少样本样例，可以让模型按照期望的结构输出内容。\n（二）给模型时间去思考我们通过 Prompt 指引语言模型进行深入思考。可以要求其先列出对问题的各种看法，说明推理依据，然后再得出最终结论。在 Prompt 中添加逐步推理的要求，能让语言模型投入更多时间逻辑思维，输出结果也将更可靠准确。\n1.指定完成任务所需的步骤text = f\"\"\"  在一个迷人的村庄里,兄妹杰克和吉尔出发去一个山顶井里打水。\\  他们一边唱着欢乐的歌,一边往上爬,\\  然而不幸降临——杰克绊了一块石头,从山上滚了下来,吉尔紧随其后。\\  虽然略有些摔伤,但他们还是回到了温馨的家中。\\  尽管出了这样的意外,他们的冒险精神依然没有减弱,继续充满愉悦地探索。  \"\"\"   prompt_1 = f\"\"\"  执行以下操作:  1-用一句话概括下面用三个反引号括起来的文本。  2-将摘要翻译成英语。  3-在英语摘要中列出每个人名。  4-输出一个 JSON 对象,其中包含以下键:english_summary,num_names。  请使用以下格式:  文本:&lt;要总结的文本&gt;  摘要:&lt;摘要&gt;  翻译:&lt;摘要的翻译&gt;  名称:&lt;英语摘要中的名称列表&gt;  输出 JSON:&lt;带有 English_summary 和 num_names 的 JSON&gt;Text:  ```{text}```  \"\"\"\n\n2.指导模型在下结论之前找出一个自己的解法假设我们要语言模型判断一个数学问题的解答是否正确。仅仅提供问题和解答是不够的，语言模型可能会匆忙做出错误判断。 相反，我们可以在 Prompt 中先要求语言模型自己尝试解决这个问题，思考出自己的解法，然后再与提供的解答进行对比，判断正确性。这种先让语言模型自主思考的方式,能帮助它更深入理解问题，做出更准确的判断。\nprompt = f\"\"\"  请判断学生的解决方案是否正确,请通过如下步骤解决这个问题:  步骤:  \t首先,自己解决问题。  \t然后将您的解决方案与学生的解决方案进行比较,对比计算得到的总费用与学生计算的总费用是否一致,  并评估学生的解决方案是否正确。  \t在自己完成问题之前,请勿决定学生的解决方案是否正确。  使用以下格式:  \t问题:问题文本  \t学生的解决方案:学生的解决方案文本  \t实际解决方案和步骤:实际解决方案和步骤文本  \t学生计算的总费用:学生计算得到的总费用  \t实际计算的总费用:实际计算出的总费用  \t学生计算的费用和实际计算的费用是否相同:是或否  \t学生的解决方案和实际解决方案是否相同:是或否  \t学生的成绩:正确或不正确  问题:  \t我正在建造一个太阳能发电站,需要帮助计算财务。  \t- 土地费用为每平方英尺100美元  \t- 我可以以每平方英尺250美元的价格购买太阳能电池板  \t- 我已经谈判好了维护合同,每年需要支付固定的10万美元,并额外支付每平方英尺10美元;  \t\t作为平方英尺数的函数,首年运营的总费用是多少。  \t学生的解决方案:  \t设x为发电站的大小,单位为平方英尺。  \t费用:  \t1. 土地费用:100x美元  \t2. 太阳能电池板费用:250x美元  \t3. 维护费用:100,000+100x=10万美元+10x美元  \t总费用:100x美元+250x美元+10万美元+100x美元=450x+10万美元  实际解决方案和步骤:  \"\"\"\n\n（三）局限性虚假知识：模型偶尔会生成一些看似真实实则编造的知识。在开发与应用语言模型时，需要注意它们可能生成虚假信息的风险。尽管模型经过大规模预训练，掌握了丰富知识，但它实际上并没有完全记住所见的信息，难以准确判断自己的知识边界，可能做出错误推断。若让语言模型描述一个不存在的产品，它可能会自行构造出似是而非的细节。这被称为幻觉 (Hallucination)，是语言模型的一大缺陷。\n语言模型生成虚假信息的幻觉问题，是使用与开发语言模型时需要高度关注的风险。由于幻觉信息往往令人无法辨别真伪，开发者必须警惕并尽量避免它的产生。\n目前 OpenAI 等公司正在积极研究解决语言模型的幻觉问题。在技术得以进一步改进之前，开发者可以通过 Prompt 设计减少幻觉发生的可能。例如，可以先让语言模型直接引用文本中的原句，然后再进行解答。这可以追踪信息来源，降低虚假内容的风险。\n综上，语言模型的幻觉问题事关应用的可靠性与安全性。开发者有必要认识到这一缺陷（注:截至2023  年7月），并采取 Prompt 优化等措施予以缓解，以开发出更加可信赖的语言模型应用。这也将是未来语言模型进化的重要方向之一。\n二、迭代优化与训练机器学习模型类似，设计高效  Prompt 也需要多个版本的试错调整。具体来说，第一版 Prompt 应该满足要求明确和给模型思考时间两个原则。在此基础上，一般的迭代流程是：首先尝试一个初版，分析结果，然后继续改进 Prompt，逐步逼近最优。许多成功的 Prompt 都是通过这种多轮调整得出的。\nprompt = f\"\"\"  您的任务是帮助营销团队基于技术说明书创建一个产品的零售网站描述。  根据```标记的技术说明书中提供的信息,编写一个产品描述。  该描述面向家具零售商,因此应具有技术性质,并侧重于产品的材料构造。  在描述末尾,包括技术规格中每个7个字符的产品ID。  在描述之后,包括一个表格,提供产品的尺寸。表格应该有两列。第一列包括尺寸的名称。第二列只包括英寸的测量值。  给表格命名为“产品尺寸”。  将所有内容格式化为可用于网站的HTML格式。将描述放在&lt;div&gt;元素中。  技术规格:```{fact_sheet_chair}```  \"\"\"\n\n\n\n三、文本概况（一）单一文本概况以商品评论的总结任务为例：对于电商平台来说，网站上往往存在着海量的商品评论，这些评论反映了所有客户的想法。如果我们拥有一个工具去概括这些海量、冗长的评论，便能够快速地浏览更多评论，洞悉客户的偏好，从而指导平台与商家提供更优质的服务。\nprod_review = \"\"\"  这个熊猫公仔是我给女儿的生日礼物,她很喜欢,去哪都带着。  公仔很软,超级可爱,面部表情也很和善。但是相比于价钱来说,  它有点小,我感觉在别的地方用同样的价钱能买到更大的。  快递比预期提前了一天到货,所以在送给女儿之前,我自己玩了会。  \"\"\"\n\n1.限制输出文本长度prompt = f\"\"\"  您的任务是从电子商务网站上生成一个产品评论的简短摘要。 请对三个反引号之间的评论文本进行概括,最多30个字。  评论: ```{prod_review}```  \"\"\"  \n\n熊猫公仔软可爱,女儿喜欢,但有点小。快递提前一天到货。\n\n当在 Prompt 中设置长度限制要求时，语言模型生成的输出长度不总能精确符合要求，但基本能控制在可接受的误差范围内。比如要求生成 50 词的文本，语言模型有时会生成 60 词左右的输出，但总体接近预定长度。 \n 这是因为语言模型在计算和判断文本长度时依赖于分词器，而分词器在字符统计方面不具备完美精度。目前存在多种方法可以尝试控制语言模型生成输出的长度，比如指定语句数、词数、汉字数等。\n2.侧重于价格与质量prompt = f\"\"\"  您的任务是从电子商务网站上生成一个产品评论的简短摘要。  请对三个反引号之间的评论文本进行概括,最多30个词汇,并且侧重在产品价格和质量上。  评论: ```{prod_review}```  \"\"\"  \n\n可爱的熊猫公仔,质量好但有点小,价格稍高。快递提前到货。\n\n3.关键信息提取prompt = f\"\"\"  您的任务是从电子商务网站上的产品评论中提取相关信息。  请从以下三个反引号之间的评论文本中提取产品运输相关的信息,最多30个词汇。评论: ```{prod_review}```  \"\"\"  \n\n产品运输相关的信息:快递提前一天到货。\n\n（二）同时概况多条文本在实际的工作流中，我们往往要处理大量的评论文本，文档中的示例将多条用户评价集合在一个列表中，并利用 for 循环和**文本概括（Summarize）**提示词，将评价概括至小于 20 个词以下，并按顺序打印。\n在实际生产中，对于不同规模的评论文本。除了使用 for 循环以外。还可能需要考虑整合评论、分布式等方法提升运算效率。\n四、推断（一）情感推断1.情感倾向分析prompt = f\"\"\"  以下用三个反引号分隔的产品评论的情感是什么?  用一个单词回答:「正面」或「负面」。  评论文本: ```{lamp_review}```  \"\"\"\n\n2.识别情感类型\nprompt = f\"\"\"  识别以下评论的作者表达的情感。包含不超过五个项目。将答案格式化为以逗号分隔的单词列表。 用一个单词回答:「正面」或「负面」。  评论文本: ```{lamp_review}```  \"\"\"\n\n3.识别愤怒\nprompt = f\"\"\"  以下评论的作者是否表达了愤怒?评论用三个反引号分隔。给出是或否的答案。 用一个单词回答:「正面」或「负面」。  评论文本: ```{lamp_review}```  \"\"\"\n\n（二）信息提取1.商品信息提取prompt = f\"\"\"  从评论文本中识别以下项目:  - 评论者购买的物品  - 制造该物品的公司  评论文本用三个反引号分隔。将你的响应格式化为以 “物品” 和 “品牌” 为键的 JSON 对象。  如果信息不存在,请使用 “未知” 作为值。  让你的回应尽可能简短。  评论文本: ```{lamp_review}```  \"\"\"\n\n2.综合情感推断和信息提取prompt = f\"\"\"  从评论文本中识别以下项目:  - 情绪(正面或负面) - 审稿人是否表达了愤怒?(是或否)  - 评论者购买的物品  - 制造该物品的公司  评论用三个反引号分隔。将你的响应格式化为 JSON 对象,以 “情感倾向”、“是否生气”、“物品类型” 和  “品牌” 作为键。  如果信息不存在,请使用 “未知” 作为值。  让你的回应尽可能简短。  将 “是否生气” 值格式化为布尔值。评论文本: ```{lamp_review}```  \"\"\"\n\n{     \"情感倾向\": \"正面\",     \"是否生气\": false,     \"物品类型\": \"卧室灯\",     \"品牌\": \"Lumina\"  }\n\n（三）主题推断1.推断讨论主题prompt = f\"\"\"  确定以下给定文本中讨论的五个主题。 每个主题用1-2个词概括。  请输出一个可解析的Python列表,每个元素是一个字符串,展示了一个主题。  给定文本: ```{story}```  \"\"\"\n\n['NASA', '满意度', '评论', '管理团队', '社会保障管理局']\n\n2.为特定主题制作新闻提醒假设我们有一个新闻网站或类似的平台，这是我们感兴趣的主题：美国航空航天局、当地政府、工程、员工满意度、联邦政府等。我们想要分析一篇新闻文章，理解其包含了哪些主题。可以使用这样的  Prompt：确定以下主题列表中的每个项目是否是以下文本中的主题。以 0 或 1 的形式给出答案列表。\nprompt = f\"\"\"  判断主题列表中的每一项是否是给定文本中的一个话题,  以列表的形式给出答案,每个元素是一个Json对象,键为对应主题,值为对应的 0 或 1。  主题列表:美国航空航天局、当地政府、工程、员工满意度、联邦政府  给定文本: ```{story}```  \"\"\"\n\n[  \t{\"美国航空航天局\": 1},  \t{\"当地政府\": 1},  \t{\"工程\": 0},  \t{\"员工满意度\": 1},  \t{\"联邦政府\": 1}  ]\n\n\n\n五、文本转换prompt = f\"\"\"  针对以下三个反引号之间的英文评论文本,  首先进行拼写及语法纠错,  然后将其转化成中文,  再将其转化成优质淘宝评论的风格,从各种角度出发,分别说明产品的优点与缺点,并进行总结。  润色一下描述,使评论更具有吸引力。  输出结果格式为:  【优点】xxx  【缺点】xxx  【总结】xxx  注意,只需填写xxx部分,并分段输出。  将结果输出成Markdown格式。  ```{text}```\"\"\"\n\n\n\n六、文本扩展文本扩展是大语言模型的一个重要应用方向，它可以输入简短文本，生成更加丰富的长文。 本章基于 OpenAI API 实现一个客户邮件自动生成的示例，用于根据客户反馈优化客服邮件。同时介绍了**温度（temperature）**这一超参数，它可以控制文本生成的多样性。\n（一）定制客户邮件在这个客户邮件自动生成的示例中，我们将根据客户的评价和其中的情感倾向,使用大语言模型针对性地生成回复邮件。 具体来说，我们先输入客户的评论文本和对应的情感分析结果（正面或者负面）。然后构造一个 Prompt，要求大语言模型基于这些信息来生成一封定制的回复电子邮件。  下面先给出一个实例，包括一条客户评价和这个评价表达的情感。这为后续的语言模型生成回复邮件提供了关键输入信息。通过输入客户反馈的具体内容和情感态度，语言模型可以生成针对这个特定客户、考虑其具体情感因素的个性化回复。\n（二）引入温度系数大语言模型中的温度（temperature）参数可以控制生成文本的随机性和多样性。temperature 的值越大，语言模型输出的多样性越大；temperature 的值越小，输出越倾向高概率的文本。\n**温度（temperature）**参数可以控制语言模型生成文本的随机性。温度为 0时，每次使用同样的  Prompt，得到的结果总是一致的。而在上面的样例中，当温度设为 0.7 时，则每次执行都会生成不同的文本。\n搭建基于 ChatGPT 的问答系统一、语言模型大语言模型（LLM）是通过预测下一个词的监督学习方式进行训练的。具体来说，首先准备一个包含数百亿甚至更多词的大规模文本数据集。然后，可以从这些文本中提取句子或句子片段作为模型输入。模型会根据当前输入 Context 预测下一个词的概率分布。通过不断比较模型预测和实际的下一个词，并更新模型参数最小化两者差异，语言模型逐步掌握了语言的规律，学会了预测下一个词。训练过程中，研究人员会准备大量句子或句子片段作为训练样本，要求模型一次次预测下一个词，通过反复训练促使模型参数收敛，使其预测能力不断提高。经过在海量文本数据集上的训练，语言模型可以达到十分准确地预测下一个词的效果。这种以预测下一个词为训练目标的方法使得语言模型获得强大的语言生成能力。\n大型语言模型主要可以分为两类：基础语言模型和指令调优语言模型。\n**基础语言模型（Base LLM）**通过反复预测下一个词来训练的方式进行训练，没有明确的目标导向。因此，如果给它一个开放式的 prompt，它可能会通过自由联想生成戏剧化的内容。而对于具体的问题，基础语言模型也可能给出与问题无关的回答。例如,给它一个 Prompt，比如“中国的首都是哪里?”，很可能它数据中有一段互联网上关于中国的测验问题列表。这时，它可能会用“中国最大的城市是什么?中国的人口是多少?”等等来回答这个问题。\n相比之下，**指令微调的语言模型（Instruction Tuned LLM）**则进行了专门的训练，以便更好地理解问题并给出符合指令的回答。例如，对“中国的首都是哪里?”这个问题，经过微调的语言模型很可能直接回答“中国的首都是北京”，而不是生硬地列出一系列相关问题。指令微调使语言模型更加适合任务导向的对话应用。它可以生成遵循指令的语义准确的回复，而非自由联想。因此，许多实际应用已经采用指令调优语言模型。熟练掌握指令微调的工作机制，是开发者实现语言模型应用的重要一步。\n基础语言模型转换到指令微调语言模型的步骤：首先，在大规模文本数据集上进行无监督预训练，获得基础语言模型。这一步需要使用数千亿词甚至更多的数据，在大型超级计算系统上可能需要数月时间。\n之后，使用包含指令及对应回复示例的小数据集对基础模型进行有监督 fine-tune，这让模型逐步学会遵循指令生成输出，可以通过雇佣承包商构造适合的训练示例。\n接下来，为了提高语言模型输出的质量，常见的方法是让人类对许多不同输出进行评级，例如是否有用、是否真实、是否无害等。  \n然后，可以进一步调整语言模型，增加生成高评级输出的概率。这通常使用**基于人类反馈的强化学习（RLHF）**技术来实现。\n相较于训练基础语言模型可能需要数月的时间，从基础语言模型到指令微调语言模型的转变过程可能只需要数天时间，使用较小规模的数据集和计算资源。\n二、TokensLLM 实际上并不是重复预测下一个单词，而是重复预测下一个 token 。对于一个句子，语言模型会先使用分词器将其拆分为一个个 token，而不是原始的单词。对于生僻词，可能会拆分为多个 token 。这样可以大幅降低字典规模，提高模型训练和推断的效率。例如，对于 “Learning new things is fun!” 这句话，每个单词都被转换为一个 token，而对于较少使用的单词，如 “Prompting as powerful  developer tool”，单词 “prompting” 会被拆分为三个 token，即”prom”、”pt”和”ing”。\n对于英文输入，一个 token 一般对应 4 个字符或者四分之三个单词；对于中文输入，一个  token 一般对应一个或半个词。不同模型有不同的 token 限制，需要注意的是，这里的 token 限制是输入的 Prompt 和输出的 completion 的 token 数之和，因此输入的 Prompt 越长，能输出的  completion 的上限就越低。 ChatGPT3.5-turbo 的 token 上限是 4096。\n三、Helper Function 辅助函数在AI应用开发领域，Prompt 技术的出现无疑是一场革命性的变革。然而，这种变革的重要性并未得到广泛的认知和重视。传统的监督机器学习工作流程中，构建一个能够分类餐厅评论为正面或负面的分类器，需要耗费大量的时间和资源。\n首先，我们需要收集并标注大量带有标签的数据。这可能需要数周甚至数月的时间才能完成。接着，我们需要选择合适的开源模型，并进行模型的调整和评估。这个过程可能需要几天、几周，甚至几个月的时间。最后，我们还需要将模型部署到云端，并让它运行起来，才能最终调用您的模型。整个过程通常需要一个团队数月时间才能完成。相比之下，基于 Prompt 的机器学习方法大大简化了这个过程。当我们有一个文本应用时，只需要提供一个简单的 Prompt，这个过程可能只需要几分钟，如果需要多次迭代来得到有效的 Prompt 的话，最多几个小时即可完成。在几天内我们就可以通过 API 调用来运行模型并开始使用。一旦我们达到了这个步骤，只需几分钟或几个小时，就可以开始调用模型进行推理。因此，以前可能需要花费六个月甚至一年时间才能构建的应用，现在只需要几分钟或几个小时，最多是几天的时间，就可以使用Prompt构建起来。这种方法正在极大地改变AI应用的快速构建方式。需要注意的是，这种方法适用于许多非结构化数据应用，特别是文本应用，以及越来越多的视觉应用，尽管目前的视觉技术仍在发展中。但它并不适用于结构化数据应用，也就是那些处理 Excel 电子表格中大量数值的机器学习应用。然而，对于适用于这种方法的应用，AI组件可以被快速构建，并且正在改变整个系统的构建工作流。构建整个系统可能仍然需要几天、几周或更长时间，但至少这部分可以更快地完成。总的来说，Prompt 技术的出现正在改变 AI 应用开发的范式，使得开发者能够更快速、更高效地构建和部署应用。然而，我们也需要认识到这种技术的局限性，以便更好地利用它来推动AI应用的发展。\n三、处理输入——思维链推理（一）思维链提示设计思维链提示是一种引导语言模型进行逐步推理的 Prompt 设计技巧。它通过在 Prompt 中设置系统消息，要求语言模型在给出最终结论之前，先明确各个推理步骤。\n具体来说，Prompt 可以先请语言模型陈述对问题的初步理解，然后列出需要考虑的方方面面，最后再逐个分析这些因素，给出支持或反对的论据，才得出整体的结论。这种逐步推理的方式，更接近人类处理复杂问题的思维过程，可以减少语言模型匆忙得出错误结论的情况。因为它必须逐步论证自己的观点，而不是直接输出結论。通过详细的思维链提示，开发者可以获得语言模型生成的结论更加可靠，理由更加充分。这种提示设计技巧值得在需要语言模型进行复杂推理时加以运用。\ndelimiter = \"====\"  system_message = f\"\"\"  请按照以下步骤回答客户的提问。客户的提问将以{delimiter}分隔。  步骤 1:{delimiter}首先确定用户是否正在询问有关特定产品或产品的问题。产品类别不计入范围。  步骤 2:{delimiter}如果用户询问特定产品,请确认产品是否在以下列表中。所有可用产品:  产品:TechPro 超极本  类别:计算机和笔记本电脑  品牌:TechPro  型号:TP-UB100  保修期:1 年  评分:4.5  特点:13.3 英寸显示屏,8GB RAM,256GB SSD,Intel Core i5 处理器  描述:一款适用于日常使用的时尚轻便的超极本。  价格:$799.99  产品:BlueWave 游戏笔记本电脑  类别:计算机和笔记本电脑  品牌:BlueWave  型号:BW-GL200  保修期:2 年 评分:4.7  特点:15.6 英寸显示屏,16GB RAM,512GB SSD,NVIDIA GeForce RTX 3060  描述:一款高性能的游戏笔记本电脑,提供沉浸式体验。  价格:$1199.99  产品:PowerLite 可转换笔记本电脑  类别:计算机和笔记本电脑  品牌:PowerLite  型号:PL-CV300  保修期:1年  评分:4.3  特点:14 英寸触摸屏,8GB RAM,256GB SSD,360 度铰链  描述:一款多功能可转换笔记本电脑,具有响应触摸屏。  价格:$699.99  产品:TechPro 台式电脑  类别:计算机和笔记本电脑  品牌:TechPro  型号:TP-DT500  保修期:1年  评分:4.4  特点:Intel Core i7 处理器,16GB RAM,1TB HDD,NVIDIA GeForce GTX 1660  描述:一款功能强大的台式电脑,适用于工作和娱乐。  价格:$999.99  产品:BlueWave Chromebook  类别:计算机和笔记本电脑  品牌:BlueWave  型号:BW-CB100  保修期:1 年  评分:4.1  特点:11.6 英寸显示屏,4GB RAM,32GB eMMC,Chrome OS  描述:一款紧凑而价格实惠的 Chromebook,适用于日常任务。  价格:$249.99  步骤 3:{delimiter} 如果消息中包含上述列表中的产品,请列出用户在消息中做出的任何假设,\\  例如笔记本电脑 X 比笔记本电脑 Y 大,或者笔记本电脑 Z 有 2 年保修期。  步骤 4:{delimiter} 如果用户做出了任何假设,请根据产品信息确定假设是否正确。  步骤 5:{delimiter} 如果用户有任何错误的假设,请先礼貌地纠正客户的错误假设(如果适用)。\\  只提及或引用可用产品列表中的产品,因为这是商店销售的唯一五款产品。以友好的口吻回答客户。  使用以下格式回答问题:  步骤 1: {delimiter} &lt;步骤 1 的推理&gt;  步骤 2: {delimiter} &lt;步骤 2 的推理&gt;  步骤 3: {delimiter} &lt;步骤 3 的推理&gt;  步骤 4: {delimiter} &lt;步骤 4 的推理&gt;  回复客户: {delimiter} &lt;回复客户的内容&gt;  请确保每个步骤上面的回答中中使用 {delimiter} 对步骤和步骤的推理进行分隔。  \"\"\"\n\n（二）内心独白在某些应用场景下，完整呈现语言模型的推理过程可能会泄露关键信息或答案,这并不可取。例如在教学应用中，我们希望学生通过自己的思考获得结论,而不是直接被告知答案。\n针对这一问题。“内心独白”技巧可以在一定程度上隐藏语言模型的推理链。具体做法是，在 Prompt 中指示语言模型以结构化格式存储需要隐藏的中间推理，例如存储为变量。然后在返回结果时，仅呈现对用户有价值的输出，不展示完整的推理过程。这种提示策略只向用户呈现关键信息，避免透露答案。同时语言模型的推理能力也得以保留。适当使用“内心独白”可以在保护敏感信息的同时，发挥语言模型的推理特长。\n使用 LangChain 开发应用程序虽然 LLM 提供了强大的能力，极大便利了应用程序的开发，个人开发者要基于 LLM 快速、便捷地开发一个完整的应用程序依然是一个具有较大工作量的任务。针对 LLM 开发，LangChain 应运而生。LangChain 是一套专为  LLM 开发打造的开源框架，实现了 LLM 多种强大能力的利用，提供了 Chain、Agent、Tool 等多种封装工具，基于 LangChain 可以便捷开发应用程序，极大化发挥 LLM 潜能。目前，使用 LangChin 已经成为 LLM 开发的必备能力之一。\nLangChain 是用于构建大模型应用程序的开源框架，有 Python 和 JavaScript 两个不同版本的包。LangChain 也是一个开源项目，社区活跃，新增功能快速迭代。LangChain 基于模块化组合，有许多单独的组件，可以一起使用或单独使用。本模块将重点介绍 LangChain 的常用组件： \n\n模型（Models）：集成各种语言模型与向量模型。  \n提示（Prompts）：向模型提供指令的途径。  \n索引（Indexes）：提供数据检索功能。  \n链（Chains）：将组件组合实现端到端应用。  \n代理（Agents）：扩展模型的推理能力。\n\n一、模型、提示和输出解释器（一）通过 LangChain 使用 OpenAI1.模型from langchain.chat_models import ChatOpenAI  # 这里我们将参数temperature设置为0.0,从而减少生成答案的随机性。  # 如果你想要每次得到不一样的有新意的答案,可以尝试调整该参数。  chat = ChatOpenAI(temperature=0.0)  chat\n\nChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None,  metadata=None, client=&lt;class 'openai.api_resources.chat_completion.ChatCompletion'&gt;,  model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='skIBJfPyi4LiaSSiYxEB2wT3BlbkFJjfw8KCwmJez49eVF1O1b', openai_api_base='', openai_organization='',  openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None,  tiktoken_model_name=None)\n\n2.使用提升模板在应用于比较复杂的场景时，提示可能会非常长并且包含涉及许多细节。使用提示模版，可以让我们更为方便地重复使用设计好的提示。\n此外，LangChain 还提供了提示模版用于一些常用场景。比如自动摘要、问答、连接到SQL数据库、连接到不同的API。通过使用 LangChain 内置的提示模版，可以快速建立自己的大模型应用，而不需要花时间去设计和构造提示。\n最后，我们在建立大模型应用时，通常希望模型的输出为给定的格式，比如在输出使用特定的关键词来让输出结构化。\ncustomer_style = \"\"\"正式普通话 \\  用一个平静、尊敬的语气  \"\"\"  customer_email = \"\"\"  嗯呐,我现在可是火冒三丈,我那个搅拌机盖子竟然飞了出去,把我厨房的墙壁都溅上了果汁!  更糟糕的是,保修条款可不包括清理我厨房的费用。  伙计,赶紧给我过来!  \"\"\"  # 使用提示模版  customer_messages = prompt_template.format_messages(    style=customer_style,      text=customer_email)  # 打印客户消息类型 # customer_messages 变量类型为列表( list )print(\"客户消息类型:\",type(customer_messages),\"\\n\") # 打印第一个客户消息类型# 列表里的元素变量类型为langchain自定义消息( langchain.schema.HumanMessage )print(\"第一个客户客户消息类型类型:\", type(customer_messages[0]),\"\\n\")  # 打印第一个元素print(\"第一个客户客户消息类型类型: \", customer_messages[0],\"\\n\")\n\n客户消息类型:  &lt;class 'list'&gt;  第一个客户客户消息类型类型:  &lt;class 'langchain.schema.messages.HumanMessage'&gt;  第一个客户客户消息类型类型:  content='把由三个反引号分隔的文本翻译成一种正式普通话 用一个平静、尊敬的语气\\n风格。文本: ```\\n嗯呐,我现在  可是火冒三丈,我那个搅拌机盖子竟然飞了出去,把我厨房的墙壁都溅上了果汁!\\n更糟糕的是,保修条款可不包括清理我厨房  的费用。\\n伙计,赶紧给我过来!\\n```\\n' additional_kwargs={} example=False\n\n\n\n二、储存（一）对话缓存储存当使用 LangChain 中的**储存（Memory）**模块时，它旨在保存、组织和跟踪整个对话的历史，从而为用户和模型之间的交互提供连续的上下文。  LangChain 提供了多种储存类型。其中，缓冲区储存允许保留最近的聊天消息，摘要储存则提供了对整个对话的摘要。实体储存则允许在多轮对话中保留有关特定实体的信息。这些记忆组件都是模块化的，可与其他组件组合使用,从而增强机器人的对话管理能力。储存模块可以通过简单的 API 调用来访问和更新，允许开发人员更轻松地实现对话历史记录的管理和维护。  \n此次课程主要介绍其中四种储存模块，其他模块可查看文档学习。对话缓存储（ConversationBufferMemory）、对话缓存窗口储存（ConversationBufferWindowMemory）、对话令牌缓存储存（ConversationTokenBufferMemory）、对话摘要缓存储存 （ConversationSummaryBufferMemory）。在 LangChain 中，储存指的是大语言模型（LLM）的短期记忆。LLM 训练好之后（获得了一些长期记忆），它的参数便不会因为用户的输入而发生改变。当用户与训练好的 LLM 进行对话时，LLM 会暂时记住用户的输入和它已经生成的输出，以便预测之后的输出，而模型输出完毕后,  它便会“遗忘”之前用户的输入和它的输出。因此，之前的这些信息只能称作为 LLM 的短期记忆。为了延长 LLM 短期记忆的保留时间，需要借助一些外部储存方式来进行记忆，以便在用户与 LLM 对话中，LLM 能够尽可能的知道用户与它所进行的历史对话信息。\n当我们在使用大型语言模型进行聊天对话时，大型语言模型本身实际上是无状态的。语言模型本身并不记得到目前为止的历史对话。每次调用 API 结点都是独立的。**储存（Memory）**可以储存到目前为止的所有术语或对话，并将其输入或附加上下文到 LLM 中用于生成输出。\n（二）对话缓存窗口储存随着对话变得越来越长，所需的内存量也变得非常长。将大量的 tokens 发送到 LLM 的成本，也会变得更加昂贵，这也就是为什么 API 的调用费用，通常是基于它需要处理的 tokens 数量而收费的。针对以上问题，LangChain 也提供了几种方便的储存方式来保存历史对话。其中，对话缓存窗口储存只保留一个窗口大小的对话。它只使用最近的 n 次交互。这可以用于保持最近交互的滑动窗口，以便缓冲区不会过大。\nllm = ChatOpenAI(temperature=0.0)  memory = ConversationBufferWindowMemory(k=1)  conversation = ConversationChain(llm=llm, memory=memory, verbose=False )  print(\"第一轮对话:\")  print(conversation.predict(input=\"你好, 我叫皮皮鲁\"))  print(\"第二轮对话:\")  print(conversation.predict(input=\"1+1等于多少?\"))  print(\"第三轮对话:\")  print(conversation.predict(input=\"我叫什么名字?\"))\n\n第一轮对话:  你好,皮皮鲁!很高兴认识你。我是一个AI助手,可以回答你的问题和提供帮助。有什么我可以帮你的吗?  第二轮对话:  1+1等于2。  第三轮对话:  很抱歉,我无法知道您的名字。\n\n（三）对话字符缓存储存使用对话字符缓存记忆，内存将限制保存的 token 数量。如果字符数量超出指定数目，它会切掉这个对话的早期部分以保留与最近的交流相对应的字符数量，但不超过字符限制。\nfrom langchain.llms import OpenAI  from langchain.memory import ConversationTokenBufferMemory  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)  memory.save_context({\"input\": \"朝辞白帝彩云间,\"}, {\"output\": \"千里江陵一日还。\"})  memory.save_context({\"input\": \"两岸猿声啼不住,\"}, {\"output\": \"轻舟已过万重山。\"})  memory.load_memory_variables({})\n\n{'history': 'AI: 轻舟已过万重山。'}\n\nChatGPT 使用一种**基于字节对编码（Byte Pair Encoding，BPE）**的方法来进行 tokenization（将输入文本拆分为token）。BPE 是一种常见的 tokenization 技术，它将输入文本分割成较小的子词单元。 \n OpenAI 在其官方 GitHub 上公开了一个最新的开源 Python 库 tiktoken（https://github.com/openai/tiktoken），这个库主要是用来计算 tokens 数量的。相比较 HuggingFace 的 tokenizer，其速度提升了好几  倍。\n（四）对话摘要缓存储存对话摘要缓存储存，使用 LLM 对到目前为止历史对话自动总结摘要，并将其保存下来。\nfrom langchain.chains import ConversationChain  from langchain.chat_models import ChatOpenAI  from langchain.memory import ConversationSummaryBufferMemory  # 创建一个长字符串  schedule = \"在八点你和你的产品团队有一个会议。 \\  你需要做一个PPT。 \\  上午9点到12点你需要忙于LangChain。\\  Langchain是一个有用的工具,因此你的项目进展的非常快。\\  中午,在意大利餐厅与一位开车来的顾客共进午餐 \\  走了一个多小时的路程与你见面,只为了解最新的 AI。 \\  确保你带了笔记本电脑可以展示最新的 LLM 样例.\"  llm = ChatOpenAI(temperature=0.0)  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)  memory.save_context({\"input\": \"你好,我叫皮皮鲁\"}, {\"output\": \"你好啊,我叫鲁西西\"})  memory.save_context({\"input\": \"很高兴和你成为朋友!\"}, {\"output\": \"是的,让我们一起去冒  险吧!\"})  memory.save_context({\"input\": \"今天的日程安排是什么?\"}, {\"output\": f\"{schedule}\"})  print(memory.load_memory_variables({})['history'])\n\nSystem: The human introduces themselves as Pipilu and the AI introduces  themselves as Luxixi. They express happiness at becoming friends and decide to go  on an adventure together. The human asks about the schedule for the day. The AI  informs them that they have a meeting with their product team at 8 o'clock and  need to prepare a PowerPoint presentation. From 9 am to 12 pm, they will be busy  with LangChain, a useful tool that helps their project progress quickly. At noon,  they will have lunch with a customer who has driven for over an hour just to  learn about the latest AI. The AI advises the human to bring their laptop to  showcase the latest LLM samples.\n\n\n\n三、模型链链（Chains）通常将大语言模型（LLM）与提示（Prompt）结合在一起，基于此，我们可以对文本或数据进行一系列操作。链（Chains）可以一次性接受多个输入。例如，我们可以创建一个链，该链接受用户输入，使用提示模板对其进行格式化，然后将格式化的响应传递给 LLM。我们可以通过将多个链组合在一起，或者通过将链与其他组件组合在一起来构建更复杂的链。\n（一）大语言模型链（LLMChain）1.初始化语言模型import warnings  warnings.filterwarnings('ignore')  from langchain.chat_models import ChatOpenAI  from langchain.prompts import ChatPromptTemplate from langchain.chains import LLMChain  # 这里我们将参数temperature设置为0.0,从而减少生成答案的随机性。  # 如果你想要每次得到不一样的有新意的答案,可以尝试调整该参数。  llm = ChatOpenAI(temperature=0.0)\n\n2.初始化提示模板初始化提示，这个提示将接受一个名为 product 的变量。该 prompt 将要求 LLM 生成一个描述制造该产品的公司的最佳名称。\nprompt = ChatPromptTemplate.from_template(\"描述制造{product}的一个公司的最佳名称是什  么?\")\n\n3.构建大语言模型链将大语言模型（LLM）和提示（Prompt）组合成链。这个大语言模型链非常简单，可以让我们以一种顺序的方式去通过运行提示并且结合到大语言模型中。\nchain = LLMChain(llm=llm, prompt=prompt)\n\n4.运行大语言模型链因此，如果我们有一个名为”Queen Size Sheet Set”的产品，我们可以通过使用 chain.run 将其通过这个链运行。\nproduct = \"大号床单套装\"  chain.run(product)\n\n'\"豪华床纺\"'\n\n（二）简单顺序链顺序链（SequentialChains）是按预定义顺序执行其链接的链。具体来说，我们将使用简单顺序链（SimpleSequentialChain—），这是顺序链的最简单类型，其中每个步骤都有一个输入/输出，一个步骤的输出是下一个步骤的输入。\nfrom langchain.chains import SimpleSequentialChain  llm = ChatOpenAI(temperature=0.9)\n\n1.创建两个子链# 提示模板 1 :这个提示将接受产品并返回最佳名称来描述该公司  first_prompt = ChatPromptTemplate.from_template(  \"描述制造{product}的一个公司的最好的名称是什么\"  )  chain_one = LLMChain(llm=llm, prompt=first_prompt)  # 提示模板 2 :接受公司名称,然后输出该公司的长为20个单词的描述  second_prompt = ChatPromptTemplate.from_template(  \"写一个20字的描述对于下面这个\\  公司:{company_name}的\"  ) chain_two = LLMChain(llm=llm, prompt=second_prompt)\n\n2.构建简单顺序链现在我们可以组合两个 LLMChain，以便我们可以在一个步骤中创建公司名称和描述。\noverall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],  verbose=True)\n\n3.运行简单顺序链product = \"大号床单套装\"  overall_simple_chain.run(product)\n\n&gt; Entering new SimpleSequentialChain chain...  优床制造公司  优床制造公司是一家专注于生产高品质床具的公司。  &gt; Finished chain.\n\n'优床制造公司是一家专注于生产高品质床具的公司。'\n\n（三）顺序链当只有一个输入和一个输出时，**简单顺序链（SimpleSequentialChain）即可实现。当有多个输入或多个输出时，我们则需要使用顺序链（SequentialChain）**来实现。\nimport pandas as pd  from langchain.chains import SequentialChain  from langchain.chat_models import ChatOpenAI #导入OpenAI模型  from langchain.prompts import ChatPromptTemplate #导入聊天提示模板  from langchain.chains import LLMChain #导入LLM链。  llm = ChatOpenAI(temperature=0.9)\n\n#子链1  # prompt模板 1: 翻译成英语(把下面的review翻译成英语)  first_prompt = ChatPromptTemplate.from_template(  \"把下面的评论review翻译成英文:\"  \"\\n\\n{Review}\"  )  # chain 1: 输入:Review 输出:英文的 Review  chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")  #子链2  # prompt模板 2: 用一句话总结下面的 review  second_prompt = ChatPromptTemplate.from_template(  \"请你用一句话来总结下面的评论review:\"  \"\\n\\n{English_Review}\"  ) # chain 2: 输入:英文的Review 输出:总结  chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")  #子链3  # prompt模板 3: 下面review使用的什么语言  third_prompt = ChatPromptTemplate.from_template(  \"下面的评论review使用的什么语言:\\n\\n{Review}\"  )  # chain 3: 输入:Review 输出:语言  chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")  #子链4  # prompt模板 4: 使用特定的语言对下面的总结写一个后续回复  fourth_prompt = ChatPromptTemplate.from_template(  \"使用特定的语言对下面的总结写一个后续回复:\"  \"\\n\\n总结: {summary}\\n\\n语言: {language}\"  )  # chain 4: 输入: 总结, 语言 输出: 后续回复  chain_four = LLMChain(llm=llm, prompt=fourth_prompt,  output_key=\"followup_message\")\n\n\n\n（四）路由链到目前为止，我们已经学习了大语言模型链和顺序链。但是，如果我们想做一些更复杂的事情怎么办?  一个相当常见但基本的操作是根据输入将其路由到一条链，具体取决于该输入到底是什么。如果你有多个子链，每个子链都专门用于特定类型的输入，那么可以组成一个路由链，它首先决定将它传递给哪个子链，然后将它传递给那个链。  \n路由器由两个组件组成：\n\n路由链（Router Chain）：路由器链本身，负责选择要调用的下一个链  \ndestination_chains：路由器链可以路由到的链\n\n四、基于文档的问答使用大语言模型构建一个能够回答关于给定文档和文档集合的问答系统是一种非常实用和有效的应用场景。与仅依赖模型预训练知识不同，这种方法可以进一步整合用户自有数据，实现更加个性化和专业的问答服务。例如，我们可以收集某公司的内部文档、产品说明书等文字资料，导入问答系统中。然后用户针对这些文档提出问题时，系统可以先在文档中检索相关信息，再提供给语言模型生成答案。\n这样,语言模型不仅利用了自己的通用知识，还可以充分运用外部输入文档的专业信息来回答用户问题，显著提升答案的质量和适用性。构建这类基于外部文档的问答系统，可以让语言模型更好地服务于具体场景，而不是停留在通用层面。这种灵活应用语言模型的方法值得在实际使用中推广。\n一、直接使用向量储存查询1.导入数据from langchain.chains import RetrievalQA #检索QA链,在文档上进行检索  from langchain.chat_models import ChatOpenAI #openai模型  from langchain.document_loaders import CSVLoader #文档加载器,采用csv格式存储  from langchain.vectorstores import DocArrayInMemorySearch #向量存储  from IPython.display import display, Markdown #在jupyter显示信息的工具  import pandas as pd  file = '../data/OutdoorClothingCatalog_1000.csv'  # 使用langchain文档加载器对数据进行导入  loader = CSVLoader(file_path=file)  # 使用pandas导入数据,用以查看  data = pd.read_csv(file,usecols=[1, 2])  data.head()\n\n2.基本文档加载器创建向量存储#导入向量存储索引创建器  from langchain.indexes import VectorstoreIndexCreator  # 创建指定向量存储类, 创建完成后,从加载器中调用, 通过文档加载器列表加载  index =  VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch).from_loaders([loa  der])\n\n3.查询创建的向量存储query =\"请用markdown表格的方式列出所有具有防晒功能的衬衫,对每件衬衫描述进行总结\"  #使用索引查询创建一个响应,并传入这个查询  response = index.query(query)  #查看查询返回的内容  display(Markdown(response))\n\n\n\n二、结合表征模型和向量存储由于语言模型的上下文长度限制，直接处理长文档具有困难。为实现长文档的问答，可以引入**向量嵌入（Embeddings）和向量存储（Vector Store）**等技术：\n首先，使用向量嵌入算法对文档进行向量化，使语义相似的文本片段具有接近的向量表示。其次，将向量化的文档切分为小块，存入向量数据库，这个流程是创建**索引（Index）**的过程。向量数据库对各文档片段进行索引，支持快速检索。这样，当用户提出问题时，可以先将问题转换为向量，在数据库中快速找到语义最相关的文档片段，然后将这些文档片段与问题一起传递给语言模型，生成回答。\n通过嵌入向量化和索引技术，可以实现对长文档的切片检索和问答，这种流程克服了语言模型的上下文限制，可以构建处理大规模文档的问答系统。\n1.导入数据#创建一个文档加载器,通过csv格式加载  file = '../data/OutdoorClothingCatalog_1000.csv'  loader = CSVLoader(file_path=file)  docs = loader.load()  #查看单个文档,每个文档对应于CSV中的一行数据  docs[0]\n\nDocument(page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This  ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning,  and quality construction for a broken-in feel from the first time you put them  on. \\n\\nSize &amp; Fit: Order regular shoe size. For half sizes not offered, order up  to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair.  \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable  EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish  and camping motif on innersole. Moderate arch contour of innersole. EVA foam  midsole for cushioning and support. Chain-tread-inspired molded rubber outsole  with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for  any inquiries.\", metadata={'source': '../data/OutdoorClothingCatalog_1000.csv',  'row': 0})\n\n2.文本向量表征模型#使用OpenAIEmbedding类  from langchain.embeddings import OpenAIEmbeddings  embeddings = OpenAIEmbeddings()  #因为文档比较短了,所以这里不需要进行任何分块,可以直接进行向量表征  #使用初始化OpenAIEmbedding实例上的查询方法embed_query为文本创建向量表征  embed = embeddings.embed_query(\"你好呀,我的名字叫小可爱\")  #查看得到向量表征的长度  print(\"\\n\\033[32m向量表征的长度: \\033[0m \\n\", len(embed))  #每个元素都是不同的数字值,组合起来就是文本的向量表征  print(\"\\n\\033[32m向量表征前5个元素: \\033[0m \\n\", embed[:5])\n\n向量表征的长度:  1536  向量表征前5个元素:  [-0.019283676849006164, -0.006842594710511029, -0.007344046732916966,  -0.024501312942119265, -0.026608679897592472]\n\n3.基于向量表征创建并查询向量存储# 将刚才创建文本向量表征(embeddings)存储在向量存储(vector store)中  # 使用DocArrayInMemorySearch类的from_documents方法来实现  # 该方法接受文档列表以及向量表征模型作为输入  db = DocArrayInMemorySearch.from_documents(docs, embeddings)  query = \"请推荐一件具有防晒功能的衬衫\"  #使用上面的向量存储来查找与传入查询类似的文本,得到一个相似文档列表  docs = db.similarity_search(query)  print(\"\\n\\033[32m返回文档的个数: \\033[0m \\n\", len(docs))  print(\"\\n\\033[32m第一个文档: \\033[0m \\n\", docs[0])\n\n返回文档的个数:  4  第一个文档:  page_content=\": 535\\nname: Men's TropicVibe Shirt, Short-Sleeve\\ndescription:  This Men’s sun-protection shirt with built-in UPF 50+ has the lightweight feel  you want and the coverage you need when the air is hot and the UV rays are  strong. Size &amp; Fit: Traditional Fit: Relaxed through the chest, sleeve and waist.  Fabric &amp; Care: Shell: 71% Nylon, 29% Polyester. Lining: 100% Polyester knit mesh.  UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry.  Additional Features: Wrinkle resistant. Front and back cape venting lets in cool  breezes. Two front bellows pockets. Imported.\\n\\nSun Protection That Won't Wear  Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of  the sun's harmful rays.\" metadata={'source':  '../data/OutdoorClothingCatalog_1000.csv', 'row': 535}\n\n4.使用查询结果构造提示来回答问题#导入大语言模型, 这里使用默认模型gpt-3.5-turbo会出现504服务器超时,  #因此使用gpt-3.5-turbo-0301  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0301\",temperature = 0.0)  #合并获得的相似文档内容  qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])  #将合并的相似文档内容后加上问题(question)输入到 `llm.call_as_llm`中  #这里问题是:以Markdown表格的方式列出所有具有防晒功能的衬衫并总结  response = llm.call_as_llm(f\"{qdocs}问题:请用markdown表格的方式列出所有具有防晒功能的衬  衫,对每件衬衫描述进行总结\")display(Markdown(response))\n\n5.使用检索问答链来回答问题通过 LangChain 创建一个检索问答链,对检索到的文档进行问题回答。检索问答链的输入包含以下：\n\nllm ：语言模型,进行文本生成\nchain_type：传入链类型，这里使用 stuff，将所有查询得到的文档组合成一个文档传入下一步。其他的方式包括:  \nMap Reduce：将所有块与问题一起传递给语言模型，获取回复，使用另一个语言模型调用将所有单独的回复总结成最终答案，它可以在任意数量的文档上运行。可以并行处理单个问题，同时也需要更多的调用。它将所有文档视为独立的；\nRefine：用于循环许多文档，实际上是迭代的，建立在先前文档的答案之上，非常适合前后因果信息并随时间逐步构建答案，依赖于先前调用的结果。它通常需要更长的时间，并且基本上需要与 Map Reduce 一样多的调用；\nMap Re-rank：对每个文档进行单个语言模型调用，要求它返回一个分数，选择最高分，这依赖于语言模型知道分数应该是什么，需要告诉它，如果它与文档相关，则应该是高分，并在那里精细调整说明。可以批量处理它们相对较快。但是更加昂贵。\n\n\n\n五、评估评估是检验语言模型问答质量的关键环节。评估可以检验语言模型在不同文档上的问答效果，找出其弱点。还可以通过比较不同模型，选择最佳系统。此外，定期评估也可以检查模型质量的衰减。评估通常有两个目的：\n\n检验LLM应用是否达到了验收标准\n分析改动对于LLM应用性能的影响\n\n基本的思路就是利用语言模型本身和链本身，来辅助评估其他的语言模型、链和应用程序。\n（一）创建 LLM 应用from langchain.chains import RetrievalQA #检索QA链,在文档上进行检索  from langchain.chat_models import ChatOpenAI #openai模型  from langchain.document_loaders import CSVLoader #文档加载器,采用csv格式存储  from langchain.indexes import VectorstoreIndexCreator #导入向量存储索引创建器  from langchain.vectorstores import DocArrayInMemorySearch #向量存储  #加载中文数据  file = '../data/product_data.csv'  loader = CSVLoader(file_path=file)  data = loader.load()  #查看数据  import pandas as pd  test_data = pd.read_csv(file,skiprows=0)  display(test_data.head())# 将指定向量存储类,创建完成后,我们将从加载器中调用,通过文档记载器列表加载  index = VectorstoreIndexCreator(      vectorstore_cls=DocArrayInMemorySearch  ).from_loaders([loader]                                                          )#通过指定语言模型、链类型、检索器和我们要打印的详细程度来创建检索QA链  llm = ChatOpenAI(temperature = 0.0)  qa = RetrievalQA.from_chain_type(      llm=llm,  chain_type=\"stuff\",      retriever=index.vectorstore.as_retriever(),      verbose=True,      chain_type_kwargs = {  \"document_separator\": \"&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;\"  }  )\n\n（二）通过 LLM 进行评估实例问答评估的流程：\n\n首先，使用 LLM 自动构建问答测试集，包含问题及标准答案；\n然后，同一 LLM 试图回答测试集中的所有问题，得到响应；\n下一步，需要评估语言模型的回答是否正确，需要使用另一个 LLM 链进行判断。\n\n具体来说，第一个语言模型负责回答问题，第二个语言模型链用来进行答案判定，最后收集判断结果，得到语言模型在这一任务上的效果分数。\nlangchain.debug = False  #为所有不同的示例创建预测  predictions = qa.apply(examples)  # 对预测的结果进行评估,导入QA问题回答,评估链,通过语言模型创建此链  from langchain.evaluation.qa import QAEvalChain #导入QA问题回答,评估链  #通过调用chatGPT进行评估  llm = ChatOpenAI(temperature=0)  eval_chain = QAEvalChain.from_llm(llm)  #在此链上调用evaluate,进行评估  graded_outputs = eval_chain.evaluate(examples, predictions)#我们将传入示例和预测,得到一堆分级输出,循环遍历它们打印答案  for i, eg in enumerate(examples):      print(f\"Example {i}:\")      print(\"Question: \" + predictions[i]['query'])      print(\"Real Answer: \" + predictions[i]['answer'])      print(\"Predicted Answer: \" + predictions[i]['result'])      print(\"Predicted Grade: \" + graded_outputs[i]['results'])      print()\n\nExample 0:  Question: 高清电视机怎么进行护理?  Real Answer: 使用干布清洁。  Predicted Answer: 高清电视机的护理非常简单。您只需要使用干布清洁即可。避免使用湿布或化学清洁  剂,以免损坏电视机的表面。  Predicted Grade: CORRECT  Example 1:  Question: 旅行背包有内外袋吗?  Real Answer: 有。  Predicted Answer: 是的,旅行背包有多个实用的内外袋,可以轻松装下您的必需品。  Predicted Grade: CORRECT  Example 2:  Question: 这款全自动咖啡机的尺寸是多少?  Real Answer: 大型尺寸为13.8'' x 17.3'',中型尺寸为11.5'' x 15.2''。  Predicted Answer: 这款全自动咖啡机有两种尺寸可选:  - 大型尺寸为13.8'' x 17.3''。  - 中型尺寸为11.5'' x 15.2''。  Predicted Grade: CORRECT  Example 3:  Question: 这款电动牙刷的规格是什么?  Real Answer: 一般大小 - 高度:9.5'',宽度:1''。  Predicted Answer: 这款电动牙刷的规格是:高度为9.5英寸,宽度为1英寸。  Predicted Grade: CORRECT  Example 4:  Question: 这种产品的名称是什么?  Real Answer: 这种产品的名称是橙味维生素C泡腾片。  Predicted Answer: 这种产品的名称是儿童益智玩具。  Predicted Grade: INCORRECT  Example 5:  Question: 这款无线蓝牙耳机的尺寸是多少?  Real Answer: 该无线蓝牙耳机的尺寸为1.5'' x 1.3''。  Predicted Answer: 这款无线蓝牙耳机的尺寸是1.5'' x 1.3''。  Predicted Grade: CORRECT  Example 6:  Question: 这款瑜伽垫的尺寸是多少?  Real Answer: 这款瑜伽垫的尺寸是24'' x 68''。  Predicted Answer: 这款瑜伽垫的尺寸是24'' x 68''。  Predicted Grade: CORRECT\n\n与传统手工准备评估集、逐题判断等方式不同，LangChain 使整个评估流程自动化。它可以自动构建包含问答样本的测试集，然后使用语言模型对测试集自动产生回复，最后通过另一个模型链自动判断每个回答的准确性。这种全自动的评估方式极大地简化了问答系统的评估和优化过程，开发者无需手动准备测试用例，也无需逐一判断正确性，大大提升了工作效率。\n六、代理LLM 对逻辑推理、计算和检索外部信息的能力较弱，这与最简单的计算机程序形成对比。例如，语言模型无法准确回答简单的计算问题，还有当询问最近发生的事件时，其回答也可能过时或错误,因为无法主动获取最新信息。这是由于当前语言模型仅依赖预训练数据，与外界隔离。要克服这一缺陷，LangChain 框 架提出了**代理（Agent）**的解决方案。\n代理作为语言模型的外部模块，可提供计算、逻辑、检索等功能的支持，使语言模型获得异常强大的推理和获取信息的超能力。\n（一）使用 LangChain 内置工具 llm-math 和 wikipediafrom langchain.agents import load_tools, initialize_agent  from langchain.agents import AgentType  from langchain.python import PythonREPL  from langchain.chat_models import ChatOpenAI# 参数temperature设置为0.0,从而减少生成答案的随机性。  llm = ChatOpenAI(temperature=0)tools = load_tools(      [\"llm-math\",\"wikipedia\"],      llm=llm #第一步初始化的模型  )# 初始化代理  agent= initialize_agent(      tools, #第二步加载的工具      llm, #第一步初始化的模型      agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, #代理类型      handle_parsing_errors=True, #处理解析错误      verbose = True #输出中间步骤  )\n\n\nagent：代理类型。这里使用的是 AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION。其中  CHAT 代表代理模型为针对对话优化的模型；Zero-shot 意味着**代理（Agents）**仅在当前操作上起作用，即它没有记忆；REACT 代表针对 REACT 设计的提示模版。 DESCRIPTION 根据工具的描述  description 来决定使用哪个工具。 \nhandle_parsing_errors ：是否处理解析错误。当发生解析错误时，将错误信息返回给大模型，让其进行纠正。 \nverbose：是否输出中间步骤结果。\n\n二、使用 LangChain 内置工具 PythonREPLToolfrom langchain.agents.agent_toolkits import create_python_agent  from langchain.tools.python.tool import PythonREPLTool  agent = create_python_agent(      llm, #使用前面一节已经加载的大语言模型      tool=PythonREPLTool(), #使用Python交互式环境工具 REPLTool      verbose=True #输出中间步骤  )  customer_list = [\"小明\",\"小黄\",\"小红\",\"小蓝\",\"小橘\",\"小绿\",]      agent.run(f\"将使用pinyin拼音库这些客户名字转换为拼音,并打印输出列表: {customer_list}。\")\n\n&gt; Entering new AgentExecutor chain...  I need to use the pinyin library to convert the names to pinyin. I can then print  out the list of converted names.  Action: Python_REPL  Action Input: import pinyin  Observation:  Thought:I have imported the pinyin library. Now I can use it to convert the names  to pinyin.  Action: Python_REPL  Action Input: names = ['小明', '小黄', '小红', '小蓝', '小橘', '小绿']  pinyin_names = [pinyin.get(i, format='strip') for i in names]  print(pinyin_names)  Observation: ['xiaoming', 'xiaohuang', 'xiaohong', 'xiaolan', 'xiaoju', 'xiaolv']Thought:I have successfully converted the names to pinyin and printed out the  list of converted names.  Final Answer: ['xiaoming', 'xiaohuang', 'xiaohong', 'xiaolan', 'xiaoju',  'xiaolv']  &gt; Finished chain.\n\n\"['xiaoming', 'xiaohuang', 'xiaohong', 'xiaolan', 'xiaoju', 'xiaolv']\"\n\n三、 定义自己的工具并在代理中使用LangChian tool 函数装饰器可以应用用于任何函数，将函数转化为 LangChain 工具，使其成为代理可调用的工具。\n# 导入tool函数装饰器  from langchain.agents import tool  from datetime import date  @tool  def time(text: str) -&gt; str:      \"\"\"  返回今天的日期,用于任何需要知道今天日期的问题。\\      输入应该总是一个空字符串,\\      这个函数将总是返回今天的日期,任何日期计算应该在这个函数之外进行。      \"\"\"      return str(date.today())  # 初始化代理  agent= initialize_agent(      tools=[time], #将刚刚创建的时间工具加入代理      llm=llm, #初始化的模型      agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, #代理类型      handle_parsing_errors=True, #处理解析错误      verbose = True #输出中间步骤  )  # 使用代理询问今天的日期.  # 注: 代理有时候可能会出错(该功能正在开发中)。如果出现错误,请尝试再次运行它。\n\nagent(\"今天的日期是?\")\n\n&gt; Entering new AgentExecutor chain...  根据提供的工具,我们可以使用`time`函数来获取今天的日期。  Thought: 使用`time`函数来获取今天的日期。  Action:      ```      {  \"action\": \"time\",       \"action_input\": \"\"      }      ```  Observation: 2023-08-09  Thought:我现在知道了最终答案。  Final Answer: 今天的日期是2023-08-09。  &gt; Finished chain.\n\n","categories":["阅读笔记"],"tags":["深度学习","LLM"]},{"title":"RAG技巧与底层代码剖析 阅读笔记","url":"/2025/06/25/RAG%E6%8A%80%E5%B7%A7%E4%B8%8E%E5%BA%95%E5%B1%82%E4%BB%A3%E7%A0%81%E5%89%96%E6%9E%90%20%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"原文链接：RAG技巧与底层代码剖析一、简易 RAG 实现RAG 流程分解\n1.**数据导入：**加载并预处理原始文本数据，为后续处理做好准备。\n2.**文本分块：**将长文本分割成较小的段落或句子，以提高检索效率和相关性。\n3.**创建 Embedding：**使用嵌入模型将文本块转换为向量表示，便于进行语义层面的比较与匹配。\n4.**语义搜索：**根据用户输入的查询内容，在已有向量库中检索出最相关的文本块。\n5.**响应生成：**基于检索到的相关内容，结合语言模型生成最终的回答输出。\n\n代码示例\n# 初始化 DashScope 客户端（使用阿里云通义千问）client = OpenAI(    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 确保提前设置好环境变量    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\")# 设置系统提示SYSTEM_PROMPT = (    \"你是一个 AI 助手，必须严格根据提供的上下文内容进行回答。\"    \"如果无法从提供的上下文中直接得出答案，请回复：'我无法根据现有信息回答这个问题。'\")def generate_response(system_prompt, user_message, model=\"qwen-max\"):    \"\"\"    使用 DashScope 的通义千问模型生成基于上下文的回答。    参数：        system_prompt (str): 控制 AI 行为的系统指令        user_message (str): 用户输入的问题及上下文        model (str): 使用的模型名称，默认为 qwen-plus    返回：        str: 模型生成的回答内容    \"\"\"    response = client.chat.completions.create(        model=model,        temperature=0.0,  # 温度设为0，保证输出确定性        max_tokens=512,   # 可按需调整最大输出长度        messages=[            {\"role\": \"system\", \"content\": system_prompt},            {\"role\": \"user\", \"content\": user_message}        ]    )        return response.choices[0].message.content.strip()# 示例 top_chunks（假设这是 semantic_search 返回的结果）top_chunks = [    \"通义灵码是一个基于 AI 的智能编程助手。\",    \"文件编辑能力包括自动补全、错误修复和代码重构等功能。\"]query = \"通义灵码的智能体能力是什么？\"# 构建用户 prompt（包含上下文 + 问题）user_prompt = \"\\n\".join([f\"上下文 {i + 1}:\\n{chunk}\"for i, chunk in enumerate(top_chunks)])user_prompt += f\"\\n\\n问题：{query}\"# 生成 AI 回答answer = generate_response(SYSTEM_PROMPT, user_prompt)# 输出结果print(\"AI 回答：\")print(answer)\n二、基于语义的文本分块\n在 RAG中，**文本分块（Text Chunking）**是一个至关重要的环节。其核心作用是将一大段连续文本划分为多个具有语义完整性的较小段落，从而提升信息检索的准确性和整体效果。\n传统的分块方式通常采用固定长度的切分策略，例如每 500 个字符或每若干句子进行一次分割。这种方法虽然实现简单，但在实际应用中容易割裂完整的语义单元，导致后续的信息检索与理解受到影响。\n相比之下，一种更智能的分块方法是语义分块（Semantic Chunking）。它不再依据字数或句数进行机械划分，而是通过分析句子之间的内容相似性来判断合适的切分位置。当检测到前后句子在语义上出现明显差异时，就在该位置断开，形成一个新的语义段落。\n\n切分点的判定方法\n为了找到合适的语义切分点，我们可以借助以下几种常见的统计方法：\n1.**百分位法（Percentile）**找出所有相邻句子之间语义相似度差异的“第 X 百分位数”，并在那些差异值超过该阈值的位置进行切分。\n2.**标准差法（Standard Deviation）**当句子间的语义相似度下降幅度超过平均值减去 X 倍标准差时，在该位置进行切分。\n3.**四分位距法（IQR, Interquartile Range）**利用上下四分位数之差（Q3 - Q1）来识别变化较大的位置，并将其作为潜在的切分点。\n\n实际应用示例\n\n创建句子级别的 Embedding# 初始化客户端client = OpenAI(    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  # 如果您没有配置环境变量，请在此处用您的API Key进行替换    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 百炼服务的base_url)# 创建文本块的嵌入向量def create_embeddings(texts, model=\"text-embedding-v3\"):    \"\"\"    输入一组文本（字符串或列表），返回对应的嵌入向量列表    \"\"\"    if isinstance(texts, str):        texts = [texts]  # 确保输入为列表形式    completion = client.embeddings.create(        model=model,        input=text_chunks,        encoding_format=\"float\"    )        # 将响应转换为 dict 并提取所有 embedding    data = json.loads(completion.model_dump_json())    embeddings = [item[\"embedding\"] for item in data[\"data\"]]    return embeddings# 将文本按句号进行初步切分为句子sentences = extracted_text.split(\"。\")# 去除空字符串和前后空格sentences = [sentence.strip() for sentence in sentences if sentence.strip()]# 批量生成所有句子的嵌入向量（推荐做法）embeddings = create_embeddings(sentences)print(f\"成功生成 {len(embeddings)} 个句子的嵌入向量。\")\n\n计算相似度差异import numpy as npfrom sklearn.metrics.pairwise import cosine_similaritydef cosine_similarity(vec1, vec2):    \"\"\"    计算两个向量之间的余弦相似度。    参数:    vec1(np.ndarray): 第一向量。    vec2(np.ndarray): 第二向量。    返回:    float: 余弦相似度。        异常:    ValueError: 如果输入向量不是一维数组或形状不匹配。    \"\"\"    if vec1.ndim != 1 or vec2.ndim != 1:        raise ValueError(\"输入向量必须是一维数组\")    if vec1.shape[0] != vec2.shape[0]:        raise ValueError(\"输入向量必须具有相同的维度\")    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))# 使用sklearn的cosine_similarity函数计算连续句子之间的相似度similarities = [cosine_similarity(embeddings[i].reshape(1, -1), embeddings[i + 1].reshape(1, -1))[0][0] for i in range(len(embeddings) - 1)]\n\n实现语义分块\n基于句子之间的语义相似度变化来决定切分位置。当检测到连续句子之间的语义差异较大时，就认为此处是一个潜在的段落分界点。\n\n\ndef compute_breakpoints(similarity_scores, method=\"percentile\", threshold=90):    \"\"\"    根据相似度下降点计算分段断点。    参数:    similarity_scores(List[float]): 句子之间的相似度列表。    method(str): 阈值计算方法，可选 'percentile', 'standard_deviation', 或 'interquartile'。    threshold(float): 阈值（用于百分位数或标准差法）。    返回:    List[int]: 应该进行分割的索引位置。    \"\"\"    # 根据选择的方法确定阈值    if method == \"percentile\":        # 计算指定百分位数的相似度值作为阈值        threshold_value = np.percentile(similarity_scores, threshold)    elif method == \"standard_deviation\":        # 计算均值和标准差，并通过减去X个标准差确定阈值        mean = np.mean(similarity_scores)        std_dev = np.std(similarity_scores)        threshold_value = mean - (threshold * std_dev)    elif method == \"interquartile\":        # 使用四分位距（IQR）规则确定异常值阈值        q1, q3 = np.percentile(similarity_scores, [25, 75])        iqr = q3 - q1        threshold_value = q1 - 1.5 * iqr    else:        # 如果方法无效则抛出错误        raise ValueError(\"无效方法。请选择 'percentile'、'standard_deviation' 或 'interquartile'。\")    # 找出相似度低于阈值的位置，即分段断点    return [i for i, score in enumerate(similarity_scores) if score &lt; threshold_value]# 使用 percentile 方法并设置阈值为 90% 百分位数计算断点breakpoints = compute_breakpoints(similarity_scores=similarities, method=\"percentile\", threshold=90)\n\n将文本切分为语义块\n接下来我们根据计算出的切分点（Breakpoints），将文本按照其语义内容进行划分。在上一步中，我们已经通过分析句子之间的语义相似度变化，识别出了一些潜在的切分位置。现在，我们将依据这些位置，把原始文本分割为多个具有清晰语义边界的段落，也称为“语义块（Semantic Chunks）”。\n\n\n为语义块创建嵌入向量\n在完成文本的语义切分之后，接下来我们要为每一个语义块（Semantic Chunk）生成嵌入向量（Embedding），以便于后续的检索和使用。\n\n\n进行语义搜索\n我们使用余弦相似度（Cosine Similarity） 来检索与查询内容最相关的语义块（Chunks）\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity# 语义搜索函数def semantic_search(query, text_chunks, embeddings=None, k=2):    \"\"\"    在 text_chunks 中找出与 query 最相关的 top-k 文本块        参数：        query: 查询语句        text_chunks: 候选文本块列表        embeddings: 对应的嵌入向量列表（如果已提前计算）        k: 返回最相关的结果数量            返回：        top_k_chunks: 最相关的 top-k 文本块    \"\"\"    if embeddings is None:        embeddings = create_embeddings(text_chunks)  # 如果没有提供，则自动生成    else:        assert len(embeddings) == len(text_chunks), \"embeddings 和 text_chunks 必须长度一致\"    query_embedding = create_embeddings(query)[0]  # 获取查询的嵌入    # 计算相似度    similarity_scores = []    for i, chunk_embedding in enumerate(embeddings):        score = cosine_similarity([query_embedding], [chunk_embedding])[0][0]        similarity_scores.append((i, score))    # 排序并取 top-k    similarity_scores.sort(key=lambda x: x[1], reverse=True)    top_indices = [index for index, _ in similarity_scores[:k]]    return [text_chunks[index] for index in top_indices]\n\n基于检索到的文本块生成响应\n\n\n三、在 RAG 中引入上下文增强检索\n传统的方法存在一个明显的问题：它只返回一个个孤立的文本块，这些文本块之间缺乏上下文联系，有时会导致 AI 获取的信息不完整，从而出现回答错误或内容不全面的情况。\n为了解决这个问题，我们提出了一种新的方法，叫做 “上下文增强检索”（Context-Enriched Retrieval）。 它的核心思想是： 不只是找出一个最相关的文本块，而是同时返回它的前一个和后一个文本块，帮助 AI 更好地理解上下文，从而生成更准确、更完整的回答。\n\n上下文增强检索流程\n1.数据导入（Data Ingestion）：从 PDF 文件中提取原始文字内容。\n2.带上下文的分块（Chunking with Overlapping Context）：将大段文字划分为多个小块，但每个文本块与前后内容有一定的重叠。 👉 这样做的目的是确保即使某句话被切分到两个文本块之间，在其中一个块中也能看到完整的上下文。\n3.创建嵌入向量（Embedding Creation）：将每个文本块转换为一组数字表示（称为“嵌入向量”），便于后续进行相似度计算。 👉 可以理解为给每个文本块打上“语义标签”，这样就能快速找到语义相近的内容。\n4.上下文感知的检索（Context-Aware Retrieval）：当用户提问时，系统不仅会找到最相关的那个文本块，还会一并返回其前后的文本块。 👉 这样 AI 在回答问题时能获得更丰富的背景信息，避免断章取义。\n5.生成回答（Response Generation）：使用大语言模型（如 Llama、ChatGLM 等），基于包含上下文的检索结果生成自然、准确的回答。 👉 就像你在考试时可以翻书找答案，而且还能看到那一页的前后内容，自然就能答得更准确。\n6.评估效果（Evaluation）：最后，我们会对 AI 的回答进行评估，判断是否因引入上下文而提升了回答的准确性与完整性。 👉 比如可以通过人工评分，或者让另一个 AI 来评估回答的质量。\n\n代码示例\ndef context_enriched_search(search_query, chunked_texts, chunk_embeddings, top_k=1, context_window_size=1):    \"\"\"    在搜索时不仅返回最相关的段落，还包含它前后的上下文段落，以提供更丰富的背景信息。    参数:        search_query(str): 用户的查询语句。        chunked_texts(List[str]): 文本被切分后的段落列表。        chunk_embeddings(List[dict]): 每个文本段落对应的向量表示（通常是从 embedding 模型得到的）。        top_k(int): 要检索的相关段落数量（这里只用 top 1 来找中心段落）。        context_window_size(int): 要包含的上下文段落数量（前后各取几个）。    返回:        List[str]: 包含最相关段落及其上下文的文本段落列表。    \"\"\"    # 第一步：将用户的问题转换为一个向量（embedding），用于和文本段落做相似度比较    query_embedding = create_embeddings(search_query).data[0].embedding    similarity_list = []  # 用于存储每个段落与问题的相似度分数和其索引    # 第二步：遍历所有段落的向量，计算它们与问题向量之间的余弦相似度    for i, chunk_embedding in enumerate(chunk_embeddings):        # 使用 cosine_similarity 函数计算相似度（越接近1越相似）        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))        # 把该段落的索引和相似度保存下来，如：(0, 0.75)        similarity_list.append((i, similarity_score))    # 第三步：根据相似度对所有段落进行排序，从高到低排列    similarity_list.sort(key=lambda x: x[1], reverse=True)    # 第四步：获取最相关的那个段落的索引（即排在第一位的段落）    most_relevant_index = similarity_list[0][0]  # 如：第3个段落    # 第五步：确定要提取的上下文范围（包括当前段落 + 前后 context_window_size 个段落）    start_index = max(0, most_relevant_index - context_window_size)  # 防止超出开头    end_index = min(len(chunked_texts), most_relevant_index + context_window_size + 1)  # 防止超出结尾    # 第六步：返回包含上下文的段落列表    return [chunked_texts[i] for i in range(start_index, end_index)]\n四、添加上下文块标题\nRAG 通过在生成回答之前从外部知识库中检索相关信息，从而提升语言模型的事实准确性。然而，在传统的文本分块方法中，往往会丢失重要的上下文信息，导致检索效果不佳，甚至使模型生成脱离上下文的回答。\n为了解决这个问题，我们引入了一种改进方法：上下文块标题（Contextual Chunk Headers, 简称 CCH）。 这个方法的核心思想是： 在将文本分成小块（chunk）时，将该段内容所属的高级上下文信息（如文档标题、章节标题等）一并加到每个文本块的开头，然后再进行嵌入和检索。 这样做可以让每个文本块都带有其背景信息，帮助模型更好地理解它属于哪个部分，从而提高检索的相关性，并避免模型基于断章取义的内容生成错误答案。\n\n上下文块标题流程分解\n1.数据导入（Data Ingestion）：加载并预处理原始文本数据。\n2.带上下文标题的分块（Chunking with Contextual Headers）：自动识别文档中的章节标题，并将这些标题加到对应段落的前面，形成带有上下文的文本块。👉 例如：\n# 第三章：人工智能的基本技术人工智能的核心方法包括机器学习、深度学习和自然语言处理...\n\n3.创建嵌入向量（Embedding Creation）：将这些带有上下文信息的文本块转换成数字形式（即嵌入向量），以便后续进行语义搜索。\n4.语义搜索（Semantic Search）：当用户提出问题时，系统会基于这些增强后的文本块，找到最相关的内容。\n5.生成回答（Response Generation）：使用大语言模型（如 Llama、ChatGLM 等）基于检索结果生成自然、准确的回答。\n6.评估效果（Evaluation）：通过评分系统对 AI 的回答进行评估，检查加入上下文标题后是否提升了回答的准确性和相关性。\n\n实际应用示例\n\n使用上下文标题对文本进行分块\n为了提升信息检索的效果，我们使用大语言模型（LLM）为每一个文本块自动生成一个描述性的标题（Header），并将其加在该文本块的前面。\n\n\ndef generate_chunk_header(text_chunk, model_name=\"qwen-max\"):    \"\"\"    使用大语言模型（LLM）为给定文本段落生成一个标题/摘要。    参数:        text_chunk(str): 需要生成标题的文本段落。        model_name(str): 用于生成标题的语言模型名称，默认为 \"qwen-max\"。    返回:        str: 由模型生成的标题或摘要内容。    \"\"\"    # 定义系统提示词，指导 AI 的行为    header_system_prompt = \"请为以下文本生成一个简洁且具有信息量的标题。\"    # 调用 LLM 模型生成基于系统提示词和输入文本的响应    llm_response = client.chat.completions.create(        model=model_name,        temperature=0,        messages=[            {\"role\": \"system\", \"content\": header_system_prompt},            {\"role\": \"user\", \"content\": text_chunk}        ]    )    # 提取并返回模型生成的内容，去除前后多余的空白字符    return llm_response.choices[0].message.content.strip()\n\ndef chunk_text_with_headers(input_text, chunk_size, overlap_size):    \"\"\"    将输入文本分割为较小的段落并为每个段落生成标题。    参数:        input_text(str): 需要分割的完整文本。        chunk_size(int): 每个段落的大小（字符数）。        overlap_size(int): 相邻段落之间的重叠字符数。    返回:        List[dict]: 包含 'header' 和 'text' 键的字典列表，分别表示段落的标题和内容。    \"\"\"        text_chunks = []  # 初始化一个空列表，用于存储带有标题的文本段落    # 使用指定的段落大小和重叠长度遍历文本    for start_index in range(0, len(input_text), chunk_size - overlap_size):        current_chunk = input_text[start_index:start_index + chunk_size]  # 提取当前段落        chunk_header = generate_chunk_header(current_chunk)  # 使用大语言模型生成段落标题        text_chunks.append({\"header\": chunk_header, \"text\": current_chunk})  # 将标题和段落内容一起添加到列表中    return text_chunks  # 返回包含标题和内容的段落列表\n\n为标题和正文创建embedding向量\n为了提升信息检索的准确性，我们不仅对正文内容生成Embedding，同时也对每个文本块前面的标题（Header）生成嵌入向量。\n\n\n# 为每个文本块生成嵌入向量chunk_embeddings = []  # 初始化一个空列表，用于存储带有标题、文本及其嵌入向量的字典# 遍历每个文本块并生成嵌入向量（带进度条）for current_chunk in tqdm(text_chunks, desc=\"生成嵌入向量\"):    # 获取当前文本块的文本内容，并生成其嵌入向量    text_embedding = create_embeddings(current_chunk[\"text\"])        # 获取当前文本块的标题，并生成其嵌入向量    header_embedding = create_embeddings(current_chunk[\"header\"])        # 将当前文本块的标题、文本及其对应的嵌入向量存入列表中    chunk_embeddings.append({        \"header\": current_chunk[\"header\"],        \"text\": current_chunk[\"text\"],        \"embedding\": text_embedding,        \"header_embedding\": header_embedding    })\n\n语义检索\n遍历每个文本块，分别计算查询与文本、标题的相似度，并取平均，最后返回最相关的 top-k 个文本块\n\n\nimport numpy as npdef _calculate_similarity(query_vec, chunk_vec):    \"\"\"    计算查询向量与块向量之间的余弦相似度。        参数:    query_vec (np.array): 查询的嵌入向量。    chunk_vec (np.array): 文本块的嵌入向量。        返回:    float: 余弦相似度。    \"\"\"    return cosine_similarity(np.array(query_vec), np.array(chunk_vec))def semantic_search(query, chunks, top_k=5):    \"\"\"    根据查询语义搜索最相关的文本块。        参数:    query (str): 用户输入的查询语句。    chunks (List[dict]): 包含嵌入向量的文本块列表。    top_k (int): 需要返回的最相关结果的数量。        返回:    List[dict]: 最相关的前top_k个文本块。    \"\"\"    # 生成查询语句的向量表示    query_vector = create_embeddings(query)    # 初始化一个列表用于存储每个文本块及其相似度    chunk_similarity_pairs = []    # 遍历每个文本块并计算相似度    for chunk in chunks:        text_vector = chunk[\"embedding\"]     # 获取文本内容的嵌入向量        header_vector = chunk[\"header_embedding\"]  # 获取标题的嵌入向量        # 分别计算查询与文本、标题的相似度，并取平均        similarity_text = _calculate_similarity(query_vector, text_vector)        similarity_header = _calculate_similarity(query_vector, header_vector)        avg_similarity = (similarity_text + similarity_header) / 2        # 存储文本块及其平均相似度        chunk_similarity_pairs.append((chunk, avg_similarity))    # 按照相似度从高到低排序    chunk_similarity_pairs.sort(key=lambda pair: pair[1], reverse=True)    # 返回最相关的 top-k 个文本块    return [pair[0] forpair in chunk_similarity_pairs[:top_k]]\n\n\n\n五、基于问题生成的 RAG\n本节通过在文档处理阶段引入问题生成（Question Generation），对文档内容进行增强。\n我们为每个文本块生成相关的提问，从而提升信息检索的效果，最终帮助语言模型生成更准确、更相关的回答。\n这种方法的核心思想是： 在传统的 RAG（Retrieval-Augmented Generation）中，我们通常只将文本块嵌入后存入向量库。而在这一改进方法中，我们还为每个文本块自动生成一些相关的问题，并将这些问题也进行嵌入。这样，在用户提问时，系统可以更好地理解哪些文本块与问题最相关，从而提高检索效果和回答质量。\n\n问题生成流程分解\n1.数据导入（Data Ingestion）：从 PDF 文件中提取原始文本内容。\n2.文本分块（Chunking）：将大段文字切分成小块，便于后续处理。 👉 每个块通常包含 200~300 字左右的内容。\n3.问题生成（Question Generation）：使用大语言模型（LLM），为每个文本块自动生成几个与该段内容相关的问题。 👉 例如，输入一段关于“机器学习”的内容，输出可能是：\n\n“什么是机器学习？”\n\n“机器学习有哪些常见算法？”\n\n“机器学习和人工智能有什么关系？”\n\n\n4.创建嵌入向量（Embedding Creation）：对每个文本块及其对应的问题分别生成嵌入向量（即转化为数字表示），以便进行语义匹配。\n5.构建向量数据库（Vector Store Creation）：使用 NumPy 构建一个简单的向量数据库，用来存储所有文本块和问题的嵌入向量。\n6.语义搜索（Semantic Search）：当用户提出问题时，系统会先查找与其问题最相似的生成问题（generated questions），然后找到对应的文本块作为上下文。\n7.生成回答（Response Generation）：基于检索到的相关文本块，让语言模型生成自然、准确的回答。\n8.评估效果（Evaluation）：最后，我们会对生成的回答进行评分，评估这种增强型 RAG 是否提升了回答的质量和准确性。\n\n实际应用示例\n\n为文本块生成问题\n为每一个文本块自动生成一些相关问题——也就是那些可以通过这段文字找到答案的问题。\n\n\nimport redef _extract_questions_from_response(response_text):    \"\"\"    从模型返回的文本中提取出以问号结尾的问题。    参数:    response_text (str): 模型返回的原始文本。    返回:    List[str]: 清洗后的有效问题列表。    \"\"\"    questions = []    for line in response_text.split('\\n'):        cleaned_line = line.strip()  # 去除前后空格        if cleaned_line:            # 去除可能存在的编号前缀（如 \"1.\", \"2)\", \"•\", \"-\" 等）            cleaned_line = re.sub(r'^[\\d\\-\\•\\*]+\\s*[\\.\\\\)]?\\s*', '', cleaned_line)            # 判断是否含有问号（中英文都支持）            if '?' in cleaned_line or '？' in cleaned_line:                # 统一转为英文问号结尾                question = cleaned_line.rstrip('?').rstrip('？') + '?'                questions.append(question)    return questionsdef generate_questions(text, question_count=5, model=\"qwen-max\"):    \"\"\"    根据提供的文本块生成可回答的问题。    参数:    text (str): 需要生成问题的文本内容。    question_count (int): 需要生成的问题数量。    model (str): 用于生成问题的语言模型名称。    返回:    List[str]: 生成的问题列表。    \"\"\"    # 系统指令：定义 AI 的行为准则    system_instruction = \"你是一个擅长从文本中生成相关问题的专家。请仅使用提供的文本创建简洁的问题，关注关键信息和概念。\"    # 用户请求模板：提供具体任务和格式要求    user_request = f\"\"\"    请基于以下文本生成 {question_count} 个不同的问题，这些问题必须能通过该文本来回答：    {text}    请以数字编号列表的形式输出问题，不要添加其他内容。    \"\"\"    # 调用大模型 API 生成问题    response = client.chat.completions.create(        model=model,        temperature=0.7,        messages=[            {\"role\": \"system\", \"content\": system_instruction},            {\"role\": \"user\", \"content\": user_request}        ]    )    # 提取原始响应内容并去除前后空格    raw_questions_text = response.choices[0].message.content.strip()    # 使用辅助函数提取并过滤有效问题    filtered_questions = _extract_questions_from_response(raw_questions_text)    return filtered_questions\n\n构建一个简单的向量存储库\nimport numpy as npfrom typing import List, Dict, OptionalclassSimpleVectorStore:    \"\"\"    简单的基于 NumPy 的向量存储实现。    \"\"\"    def __init__(self):        \"\"\"        初始化向量数据库，包含向量、文本和元数据列表。        \"\"\"        self.vectors: List[np.ndarray] = []   # 存储向量        self.texts: List[str] = []            # 存储原始文本        self.metadata_list: List[Dict] = []    # 存储元信息        def add_item(self, text: str, vector: List[float], metadata: Optional[Dict] = None):        \"\"\"        向向量库中添加一个条目。        参数:        text (str): 原始文本内容。        vector (List[float]): 向量嵌入表示。        metadata (Dict, optional): 可选的元数据信息。        \"\"\"        self.vectors.append(np.array(vector))        self.texts.append(text)        self.metadata_list.append(metadata or {})        def similarity_search(self, query_vector: List[float], top_k: int = 5) -&gt; List[Dict]:        \"\"\"        根据查询向量在向量库中查找最相似的 top_k 条记录。        参数:        query_vector (List[float]): 查询向量。        top_k (int): 返回的结果数量。        返回:        List[Dict]: 包含相似文本、元数据和相似度得分的字典列表。        \"\"\"        if not self.vectors:            return []                # 将查询向量转换为 numpy 数组        query_array = np.array(query_vector)                # 计算每个向量与查询向量的余弦相似度        similarities = []        for idx, vector in enumerate(self.vectors):            similarity = np.dot(query_array, vector) / (                np.linalg.norm(query_array) * np.linalg.norm(vector)            )            similarities.append((idx, similarity))                # 按照相似度降序排序        similarities.sort(key=lambda x: x[1], reverse=True)                # 构建结果返回        results = []        for i in range(min(top_k, len(similarities))):            idx, score = similarities[i]            results.append({                \"text\": self.texts[idx],                \"metadata\": self.metadata_list[idx],                \"similarity_score\": float(score)            })                return results\n\n使用问题增强来处理文档\n现在，我们将前面的所有步骤整合在一起，对文档进行完整处理：包括为文本块生成相关问题、创建embedding，并构建一个增强型的向量存储库（Augmented Vector Store）。\n\n\ndef process_document(pdf_path, chunk_size=1000, chunk_overlap=200, questions_per_chunk=5):    \"\"\"    对文档进行处理并生成问题增强。    参数:    pdf_path(str): PDF 文件路径。    chunk_size(int): 每个文本块的字符数。    chunk_overlap(int): 文本块之间的重叠字符数。    questions_per_chunk(int): 每个文本块生成的问题数量。    返回:    Tuple[List[str], SimpleVectorStore]: 处理后的文本块和向量存储。    \"\"\"    print(\"从PDF中提取文本...\")    extracted_text = extract_text_from_pdf(pdf_path)    print(\"分割文本为块...\")    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)    print(f\"共创建 {len(text_chunks)} 个文本块\")    vector_store = SimpleVectorStore()    print(\"处理每个文本块并生成相关问题...\")    for idx, chunk in enumerate(tqdm(text_chunks, desc=\"正在处理文本块\")):        # 为当前文本块生成嵌入        chunk_embedding_response = create_embeddings(chunk)        chunk_embedding = chunk_embedding_response.data[0].embedding        # 将文本块添加到向量库中        vector_store.add_item(            text=chunk,            vectors=chunk_embedding,            metadata={\"type\": \"chunk\", \"index\": idx}        )        # 为当前文本块生成多个问题        questions = generate_questions(chunk, num_questions=questions_per_chunk)        # 为每个问题生成嵌入，并加入向量库        for q_idx, question in enumerate(questions):            question_embedding_response = create_embeddings(question)            question_embedding = question_embedding_response.data[0].embedding            # 将问题添加到向量库            vector_store.add_item(                text=question,                vectors=question_embedding,                metadata={\"type\": \"question\", \"chunk_index\": idx, \"original_chunk\": chunk}            )    return text_chunks, vector_store\n\n提取与处理文档\n在增强型向量库上进行查询\n\n\n六、Query 改写\n本节实现了三种查询转换（Query Transformation），以提升检索增强生成（RAG）系统的信息检索效果。\n核心目标：通过修改或扩展用户的原始查询，帮助系统更准确地理解用户意图，并从向量库中找到更相关的信息。\n\n查询转换技巧\n1. 查询重写（Query Rewriting）将用户的问题变得更具体、更详细，从而提高检索的精准度。\n🔹 示例：\n\n用户原问题：“AI 是什么？”\n\n重写后的问题：“人工智能的定义及其核心技术有哪些？”\n\n\n✅ 提升点：让搜索更精确，避免过于宽泛的结果。\n\n2. 回退提问（Step-back Prompting）生成一个更广泛、更高层次的问题，用于获取更多背景信息，帮助系统更好地理解上下文。\n🔹 示例：\n\n用户原问题：“深度学习在医疗领域有哪些应用？”\n\n回退问题：“人工智能在医疗行业的应用有哪些？”\n\n\n✅ 提升点：有助于找到与问题相关但不直接匹配的重要背景知识。\n\n3. 子查询拆解（Sub-query Decomposition）将一个复杂的问题拆分成多个更简单的小问题，分别进行检索，最后综合所有结果，提供更全面的回答。\n🔹 示例：\n\n用户原问题：“比较机器学习和深度学习的优缺点及应用场景。”\n\n拆解为：\n\n“什么是机器学习？”\n\n“什么是深度学习？”\n\n“机器学习有哪些优缺点？”\n\n“深度学习有哪些优缺点？”\n\n“它们各自适用于哪些场景？”\n\n\n✅ 提升点：确保覆盖问题的所有方面，避免遗漏关键信息。\n\n七、重排序\n重排序是在初步检索结果的基础上进行的第二轮筛选与优化步骤，目的是确保最终用于生成回答的内容是最相关、最准确的部分。\n在传统的语义搜索中，我们通常使用向量相似度（如余弦相似度）来找到最相关的文本块。但这种“初步检索”并不总是完美的，有时会返回一些看似相关但实际上不匹配的内容。\n重排序的作用就是：\n✅ 在初步检索结果中进一步筛选； \n✅ 使用更精确的相关性评分模型对内容重新打分；\n✅ 按照实际相关性重新排序；\n✅ 只保留最相关的文档用于后续的回答生成。\n\n重排序的核心流程\n1. 初步检索（Initial Retrieval）\n使用基础的语义相似度搜索（如向量匹配）快速获取一批候选文本块；\n\n这一步速度快，但准确性有限。\n\n\n2. 文档评分（Document Scoring）\n对每个检索到的文档进行更深入的相关性评估；\n\n可以使用专门的重排序模型（如 BERT reranker、ColBERT、Cross-Encoder 等），根据用户查询和文档内容之间的语义关系打分；\n\n相比简单的向量匹配，这种方式能更好地理解“句子层面”的相关性。\n\n\n3. 重新排序（Reordering）\n根据评分结果对所有候选文档进行重新排序；\n\n最相关的排在最前面，最不相关的被靠后或剔除。\n\n\n4. 内容选择（Selection）\n只选取排名靠前的几个文档作为上下文提供给语言模型；\n\n避免引入噪音信息，提高回答的准确性和可靠性。\n\n\n\n八、用于增强 RAG 的相关段落提取\n不同于传统的做法——仅仅检索出多个孤立的文本块， 我们的目标是：识别并重建连续的文本片段，从而为语言模型提供更完整、更有逻辑性的上下文信息。\n核心理念：在文档中，与用户问题相关的文本块往往集中出现在同一区域或连续段落中。 如果我们能够识别这些相关文本块之间的联系，并将它们按顺序组织成一个连贯的整体段落，就能显著提升语言模型对上下文的理解能力。\n传统 RAG 的问题：\n\n检索结果由多个不相连的文本块组成；\n\n块之间可能缺少过渡和背景信息；\n\n导致语言模型理解困难，甚至出现断章取义的情况。\n\n\n而相关段落提取（Relevant Segment Extraction，RSE）的优势在于： ✅ 将相关文本块组合成连续段落； ✅ 保留原文结构和语义连贯性； ✅ 提供更自然、完整的上下文给语言模型； ✅ 提高最终回答的准确性和流畅度。\n\nRSE 流程分解\n1.初步检索\n使用语义搜索从向量库中找出与用户问题最相关的若干文本块。\n2.位置排序\n如果原始文档中的文本块有编号或位置信息（如页码、段落顺序），我们可以根据这些信息对检索结果进行重新排序。\n3.聚类分析\n分析哪些文本块在原文中彼此靠近且语义相近，将它们归为一组，形成“相关段落簇”。\n4.段落重建\n将属于同一个簇的文本块拼接在一起，形成一个完整的上下文段落。必要时还可以加入相邻的前后内容，以增强上下文连贯性。\n5.输入语言模型\n将重建后的连续段落作为上下文，提供给大语言模型（LLM）生成最终回答。\n\n实际应用示例\n完整的 pipeline\ndef rag_with_rse(pdf_path: str, query: str, chunk_size: int = 800, penalty: float = 0.2) -&gt; Dict:    \"\"\"    完整的 RAG 流程，使用相关段落提取（RSE）策略筛选最有用的文档内容。        参数:        pdf_path(str): PDF 文档路径        query(str): 用户查询        chunk_size(int): 文本切片大小        penalty(float): 不相关切片的惩罚系数            返回:        Dict: 包含查询、选中的段落以及生成回答的结果字典    \"\"\"    print(\"\\n=== 开始执行基于相关段落提取的 RAG 流程 ===\")    print(f\"查询内容: {query}\")    # 步骤 1：处理文档并生成向量存储    text_chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)    # 步骤 2：计算每个文本块的相关性得分与价值值    print(\"\\n正在计算文本块相关性得分与价值值...\")    chunk_values = calculate_chunk_values(query, text_chunks, vector_store, penalty)    # 步骤 3：根据价值值选择最优段落    best_segments, scores = find_best_segments(        chunk_values=chunk_values,        max_segment_length=20,        total_max_length=30,        min_segment_value=0.2    )    # 步骤 4：重建最佳段落    print(\"\\n正在重建最佳文本段落...\")    selected_segments = reconstruct_segments(text_chunks, best_segments)    # 步骤 5：格式化上下文供大模型使用    formatted_context = format_segments_for_context(selected_segments)    # 步骤 6：调用大模型生成最终回复    response = generate_response(query, formatted_context)    # 整理输出结果    result = {        \"query\": query,        \"segments\": selected_segments,        \"response\": response    }    print(\"\\n=== 最终回复如下 ===\")    print(response)    return result\n\n九、上下文压缩技术\n在使用 RAG 系统进行文档检索时，我们通常会得到一些包含混合内容的文本块：\n\n有些句子与用户的问题相关；\n\n有些句子则完全无关或只是背景介绍。\n\n\n例如：\n“人工智能是计算机科学的一个分支。它旨在让机器模拟人类智能行为。许多AI系统依赖于大数据进行训练。深度学习是一种特殊的机器学习方法。”\n如果用户的问题是：“什么是人工智能？” 那么只有第一句是最相关的，其余内容虽然正确，但和当前问题无关。\n我们将对检索到的文本块进行过滤与压缩，只保留其中最相关的内容，从而：\n✅ 减少噪声信息；\n✅ 提高语言模型回答的准确性和相关性；\n✅ 更高效地利用有限的上下文窗口（context window）。\n\n上下文压缩流程分解\n1. 逐句分析相关性将每个文本块拆分为句子，并使用语义模型（如 BERT、Sentence-BERT 等）计算每句话与用户查询之间的相关性得分。\n2. 设定阈值或选择 Top-K 句子我们可以选择两种策略之一来筛选句子：\n✅ 保留得分高于某个阈值的句子；\n✅ 或者保留得分最高的前 K 个句子。\n3. 重建压缩后的上下文将筛选后的句子按原始顺序重新组合成一个新的、更紧凑的上下文段落。这是方法的核心部分。我们将使用一个大语言模型来过滤和压缩检索到的内容，从而保留与用户问题最相关的信息。\n\n代码示例\ndef compress_chunk(chunk: str, query: str, compression_type: str = \"selective\", model: str = \"qwen-max\") -&gt; Tuple[str, float]:    \"\"\"    压缩检索到的文本块，仅保留与查询相关的部分。    参数:        chunk(str): 需要压缩的文本块        query(str): 用户查询        compression_type(str): 压缩方式 (\"selective\", \"summary\", 或 \"extraction\")        model(str): 使用的 LLM 模型名称    返回:        Tuple[str, float]: 压缩后的文本块 和 压缩比例（百分比）    \"\"\"    # 根据不同压缩类型构建系统提示词    if compression_type == \"selective\":        system_prompt = \"\"\"你是一个信息筛选专家。        你的任务是分析文档片段并提取**直接与用户查询相关**的句子或段落。删除所有不相关的内容。        输出要求：        1. 只包含有助于回答问题的文本        2. 保留相关句子的原始措辞（不要改写）        3. 维持原文顺序        4. 包含所有相关内容，即使看起来重复        5. 排除任何与问题无关的文本        请以纯文本格式输出，不要添加额外说明。\"\"\"        elif compression_type == \"summary\":        system_prompt = \"\"\"你是一个摘要专家。        你的任务是对给定的文档片段进行简洁总结，只聚焦于与用户查询有关的信息。        输出要求：        1. 简洁但涵盖所有与问题相关的内容        2. 专注于与查询相关的信息        3. 忽略不相关细节        4. 用中立、客观的语气撰写        请以纯文本格式输出，不要添加额外说明。\"\"\"        else:  # extraction        system_prompt = \"\"\"你是一个信息抽取专家。        你的任务是从文档片段中提取**确切包含相关信息的句子**来回答用户的查询。        输出要求：        1. 仅包含原文中的相关句子        2. 保持原句不变（不要修改）        3. 只包含与问题直接相关的句子        4. 每个句子之间用换行分隔        5. 不添加任何评论或其他内容        请以纯文本格式输出，不要添加额外说明。\"\"\"    # 构建用户提示    user_prompt = f\"\"\"        查询：{query}        文档片段：        {chunk}        提取与该查询相关的内容。    \"\"\"    # 调用大模型 API 进行压缩处理    response = client.chat.completions.create(        model=model,        messages=[            {\"role\": \"system\", \"content\": system_prompt},            {\"role\": \"user\", \"content\": user_prompt}        ],        temperature=0    )    # 获取压缩后的内容    compressed_content = response.choices[0].message.content.strip()    # 计算压缩比例    original_length = len(chunk)    compressed_length = len(compressed_content)    compression_ratio = (original_length - compressed_length) / original_length * 100    return compressed_content, compression_ratio\n十、RAG 中的反馈机制\n在本节中，我将实现一个带有反馈机制的 RAG 系统，它能够随着时间推移不断自我优化。 通过收集并整合用户的反馈信息，它可以：\n✅ 学习哪些回答是有效的，哪些需要改进；\n✅ 持续提升检索结果的相关性和回答质量；\n✅ 在每一次交互中变得“更聪明”。\n我们构建的是一个动态、自适应的 RAG 系统，它具备以下能力：\n✅ 记忆功能：记住哪些文档曾提供过有用的信息，哪些没有；\n✅ 动态调整评分：根据历史反馈更新文档的相关性得分；\n✅ 知识积累：将成功的问答对加入知识库，供未来查询使用；\n✅ 持续进化：每次与用户的互动都是一次学习机会，系统会越用越准、越用越好。\n\n反馈机制流程分解\n1.用户提问\n\n用户输入一个问题，并得到一个由 RAG 系统生成的回答。\n\n2.获取用户反馈\n\n用户可以通过评分、点赞/踩、或者直接评论等方式提供反馈；\n\n3.记录反馈数据\n\n将用户问题、原始回答、反馈内容等信息存储下来，形成反馈日志。\n\n4.分析与学习\n\n使用模型分析哪些文档和段落产生了高质量的回答；\n调整这些文档在未来的检索权重；\n将高质量问答对加入知识库，用于增强未来的语义理解。\n\n5.优化下一次回答\n\n下次遇到类似问题时，系统能更快、更准确地找到最佳答案。\n\n\n","categories":["阅读笔记"],"tags":["Pytorch","深度学习","RAG"]},{"title":"minimind源码 阅读笔记","url":"/2025/06/17/minimind%20%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","content":"一、Minimind 是什么模型类型：基于 Transformer 的自回归语言模型（Causal LM）\n设计目标：\n\n轻量、易用，适合中小规模硬件环境\n支持快速训练和微调（包括 RLHF 等）\n兼顾性能与资源消耗\n\n技术特点：\n\n支持 MoE（Mixture of Experts，专家混合）以提升容量\n支持 rotary position embedding（旋转位置编码）\n集成了混合精度训练、分布式训练支持\n有定制的 RMSNorm 和 FeedForward 组件\n\n\n\n\n\n二、Minimind 仓库结构minimind/├── model/│   ├── tokenizer.json                 # 分词器词汇表（保存 tokenizer 的词汇）│   ├── tokenizer_config.json         # 分词器的配置信息（如是否小写、特殊符号等）│   ├── __init__.py                   # 模块初始化文件│   ├── model_lora.py                 # Lora（Low-Rank Adaptation）模型支持模块│   └── model_minimind.py             # Minimind 模型核心结构定义（含 Transformer + MoE + FFN 等）│├── out/                              # （可能为训练输出目录）│├── scripts/                          # 实用工具脚本和API服务相关│   ├── chat_openai_api.py           # 聊天 API 服务示例，调用 Minimind 模型进行对话│   ├── convert_model.py             # 转换模型格式，如从 HuggingFace 或其他格式转为 Minimind 格式│   ├── serve_openai_api.py          # 本地部署为 OpenAI API 接口服务│   ├── train_tokenizer.py           # 训练自定义 tokenizer 的脚本│   └── web_demo.py                  # 基于网页的推理交互 demo 脚本│├── trainer/                          # 不同训练方式的脚本集合│   ├── train_distill_reason.py      # 含 reasoning 的知识蒸馏训练脚本│   ├── train_distillation.py        # 普通的知识蒸馏训练脚本│   ├── train_dpo.py                 # DPO（Direct Preference Optimization）训练脚本│   ├── train_full_sft.py            # 完整的 SFT（Supervised Fine-tuning）训练脚本│   ├── train_lora.py                # Lora 参数高效微调脚本│   └── train_pretrain.py            # 预训练模型脚本（语言建模预训练）│├── venv/                             # 虚拟环境（通常用于隔离依赖）│├── .gitignore                        # Git 忽略规则├── CODE_OF_CONDUCT.md               # 开源行为准则├── LICENSE                           # 开源许可证（如 MIT, Apache-2.0 等）├── README.md                         # 项目说明文档（中文）├── README_en.md                      # 项目说明文档（英文）├── eval_model.py                     # 用于模型评估的脚本├── requirements.txt                  # 依赖包列表\n\n\n三、训练脚本train_pretrain 模块说明该模块是 Minimind 中的语言预训练脚本，支持单机单卡、多卡混合精度训练，具备如下特性：\n\n支持 DDP 分布式训练\n使用动态余弦学习率调度\n支持梯度累积实现大 batch\n启用混合精度训练（AMP）\n可选接入 Weights &amp; Biases（wandb）实验可视化\n支持 **MoE（Mixture of Experts）**可选切换\n\n该模块可以分为以下几个部分：\n\n模块导入与路径设置import osimport sys__package__ = \"trainer\"sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\n\n设置当前模块包名和路径，以支持从 model/、dataset/ 等父目录导入模块。\n\n\n基础依赖导入import argparse, time, math, warningsimport torchimport torch.distributed as distfrom torch import optim, nnfrom torch.nn.parallel import DistributedDataParallelfrom torch.utils.data import DataLoader, DistributedSamplerfrom contextlib import nullcontextfrom transformers import AutoTokenizerfrom model.model_minimind import MiniMindConfig, MiniMindForCausalLMfrom dataset.lm_dataset import PretrainDataset\n\n\n加载 PyTorch、transformers、数据模块、模型定义等核心依赖。\n\n\n日志与工具函数定义def Logger(content):    # 仅主进程打印日志（DDP 多进程场景）    if not ddp or dist.get_rank() == 0:        print(content)def get_lr(current_step, total_steps, lr):    # 余弦学习率调度函数，随着迭代步数动态调整学习率    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n\n\n\n单论训练函数 train_epoch完整实现一次 epoch 的训练流程：梯度累积，自动混合精度（AMP），loss masking，DDP 检查点保存，学习率调度，wandb 可视化，半精度权重保存\ndef train_epoch(epoch, wandb):    # 使用 逐元素交叉熵损失（不进行 reduction），因为后续要按 loss_mask 掩码处理    # loss_mask 进行加权计算，可以在训练中只计算特定位置的 loss，忽略掉不关心的位置（如 padding、上下文提示、辅助输入等）    loss_fct = nn.CrossEntropyLoss(reduction='none')    start_time = time.time()    # X: 输入 token 序列    # Y: 目标 token 序列（用于预测）    # loss_mask: 表示哪些位置需要计算 loss（可能忽略 padding 等）    for step, (X, Y, loss_mask) in enumerate(train_loader):        # 将数据部署到目标设备        X = X.to(args.device)        Y = Y.to(args.device)        loss_mask = loss_mask.to(args.device)        # 计算当前步的学习率        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)        for param_group in optimizer.param_groups:            param_group['lr'] = lr        with ctx:            res = model(X)            # res.logits：模型对每个 token 的预测概率分布（未经过 softmax）            # 形状一般是 (batch_size, seq_len, vocab_size)            # res.logits从 (B, S, V) → (B*S, V)            # Y 从 (B, S) → (B*S)            # 计算出的损失是每个 token 的标量            # 然后重新 reshape 回 (B, S) 方便后续按位置处理            loss = loss_fct(                res.logits.view(-1, res.logits.size(-1)),                Y.view(-1)            ).view(Y.size())            # 按照 loss_mask 加权平均            # loss_mask 是一个与 (B, S) 形状相同的掩码张量，标记哪些位置需要计算损失            loss = (loss * loss_mask).sum() / loss_mask.sum()            loss += res.aux_loss            # 训练时用了梯度累积（accumulation_steps），即多步累积梯度后再更新参数            # 这里对当前步损失做缩放，保证反向传播梯度是平均的，避免梯度过大            loss = loss / args.accumulation_steps        # 把 loss 乘以一个动态缩放因子（scale），比如 scale=65536.0        # 防止 backward 过程中 float16 的梯度为 0        scaler.scale(loss).backward()        # 累计若干步后，进行一次参数更新        # 在显存受限时实现大 batch 训练效果        if (step + 1) % args.accumulation_steps == 0:            # 解除缩放后的梯度            # 把所有梯度还原成真实大小            scaler.unscale_(optimizer)            # 防止梯度爆炸            # 将所有参数的梯度 L2 范数 限制在 args.grad_clip 范围            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)            # 使用 unscale 后的梯度来更新模型参数            # 和普通训练的 optimizer.step() 类似，但自动判断是否梯度异常（如 NaN）来跳过更新            scaler.step(optimizer)            scaler.update()            # 清空梯度，为下一个 accumulation 循环做准备            optimizer.zero_grad(set_to_none=True)        # 日志打印与 WandB 可视化        if step % args.log_interval == 0:            spend_time = time.time() - start_time            Logger(                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(                    epoch + 1,                    args.epochs,                    step,                    iter_per_epoch,                    loss.item() * args.accumulation_steps,                    optimizer.param_groups[-1]['lr'],                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))            if (wandb is not None) and (not ddp or dist.get_rank() == 0):                wandb.log({\"loss\": loss.item() * args.accumulation_steps,                           \"lr\": optimizer.param_groups[-1]['lr'],                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})        # 模型权重保存        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):            model.eval()            moe_path = '_moe' if lm_config.use_moe else ''            ckp = f'{args.save_dir}/pretrain_{lm_config.hidden_size}{moe_path}.pth'            if isinstance(model, torch.nn.parallel.DistributedDataParallel):                state_dict = model.module.state_dict()            else:                state_dict = model.state_dict()            state_dict = {k: v.half() for k, v in state_dict.items()}  # 半精度保存            torch.save(state_dict, ckp)            model.train()\n\n模型初始化函数 init_modeldef init_model(lm_config):    # 加载模型结构与预训练 tokenizer    tokenizer = AutoTokenizer.from_pretrained('../model/')    model = MiniMindForCausalLM(lm_config).to(args.device)    # 输出参数总量（仅可训练的）    Logger(f'LLM可训练总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')    return model, tokenizer\n\n分布式训练初始化函数 init_distributed_modedef init_distributed_mode():    if not ddp: return    global ddp_local_rank, DEVICE    # 初始化 PyTorch 分布式通信组（ProcessGroup）    # 使用 NCCL 后端：专为 GPU 设计的高性能通信库，适合 NVIDIA GPU 间通信    dist.init_process_group(backend=\"nccl\")    # RANK\t当前进程在全局中的编号（0 ~ world_size-1）    # LOCAL_RANK\t当前进程在单台机器上的 GPU 编号    # WORLD_SIZE\t总进程数量（通常等于 GPU 总数）    ddp_rank = int(os.environ[\"RANK\"])    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])    # 将当前 PyTorch 进程绑定到编号为 LOCAL_RANK 的 GPU 上    # 必须手动设置，否则可能多个进程共享同一个 GPU，导致冲突和 OOM    DEVICE = f\"cuda:{ddp_local_rank}\"    torch.cuda.set_device(DEVICE)\n\n主程序入口\n参数解析，包括总训练轮数、初始学习率、数据精度、梯度累计步数、梯度裁剪阈值、预热轮数、词向量和隐藏层维度、隐藏层数和最大序列长度等\n初始化模型结构的超参数，保存和传递模型架构信息\n启用自动混合精度（AMP）\n设置分布式运行 DDP\n设置随机种子以确保可复现\n初始化模型和分词器\n加载预训练数据集，传入最大序列长度和分词器\n设置混合精度的梯度缩放器，避免数值精度问题\n设置 AdamW 优化器\n\n# torchrun --nproc_per_node 2 1-pretrain.pyif __name__ == \"__main__\":    parser = argparse.ArgumentParser(description=\"MiniMind Pretraining\")    parser.add_argument(\"--out_dir\", type=str, default=\"../out\")    # 若要以最快速度实现zero则epochs设置为1轮；否则应当利用有限的数据训练2~6个epochs。    parser.add_argument(\"--epochs\", type=int, default=1)  # 总训练轮数    parser.add_argument(\"--batch_size\", type=int, default=32)    parser.add_argument(\"--learning_rate\", type=float, default=5e-4)  # 初始学习率    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\")  # 数据精度    parser.add_argument(\"--use_wandb\", action=\"store_true\")  # 是否启用 Weights &amp; Biases 实验可视化    parser.add_argument(\"--wandb_project\", type=str, default=\"MiniMind-Pretrain\")    parser.add_argument(\"--num_workers\", type=int, default=1)    parser.add_argument(\"--ddp\", action=\"store_true\")  # 是否启用分布式训练（DistributedDataParallel）    parser.add_argument(\"--accumulation_steps\", type=int, default=8)  # 梯度累计步数    parser.add_argument(\"--grad_clip\", type=float, default=1.0)  # 梯度裁剪阈值，防止梯度爆炸    parser.add_argument(\"--warmup_iters\", type=int, default=0)  # 预热轮数，在前若干迭代中平滑提升学习率，有助于稳定训练    parser.add_argument(\"--log_interval\", type=int, default=100)    parser.add_argument(\"--save_interval\", type=int, default=100)    parser.add_argument('--local_rank', type=int, default=-1)  # 用于 DDP 中标记当前进程的 GPU 编号，通常由分布式训练系统自动分配    parser.add_argument('--hidden_size', default=512, type=int)  # 词向量和模型内部向量的维度    parser.add_argument('--num_hidden_layers', default=8, type=int)  # 隐藏层数    parser.add_argument('--max_seq_len', default=512, type=int)  # 最大序列长度    parser.add_argument('--use_moe', default=False, type=bool)    parser.add_argument(\"--data_path\", type=str, default=\"../dataset/pretrain_hq.jsonl\")    args = parser.parse_args()    # 初始化模型结构的超参数，保存和传递模型架构信息    lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers,                               use_moe=args.use_moe)    args.save_dir = os.path.join(args.out_dir)    os.makedirs(args.save_dir, exist_ok=True)    os.makedirs(args.out_dir, exist_ok=True)    tokens_per_iter = args.batch_size * args.max_seq_len  # 统计每次训练迭代所处理的总 token 数    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"    args.wandb_run_name = f\"MiniMind-Pretrain-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LearningRate-{args.learning_rate}\"    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()  # 启用自动混合精度（AMP）    # 判断是否是分布式运行（依据环境变量 RANK 是否被设置）    ddp = int(os.environ.get(\"RANK\", -1)) != -1  # is this a ddp run?    ddp_local_rank, DEVICE = 0, \"cuda:0\"    base_seed = 1337  # 设置随机种子以确保可复现性    torch.manual_seed(base_seed)    torch.cuda.manual_seed(base_seed)    # 如果是 DDP 训练，初始化分布式模式，并为每个进程设置不同的种子    if ddp:        init_distributed_mode()        args.device = torch.device(DEVICE)        rank = dist.get_rank()        torch.manual_seed(base_seed + rank)        # 同时设置 CUDA 的随机种子        torch.cuda.manual_seed(base_seed + rank)    if args.use_wandb and (not ddp or ddp_local_rank == 0):        import wandb        wandb.init(project=args.wandb_project, name=args.wandb_run_name)    else:        wandb = None    model, tokenizer = init_model(lm_config)  # 初始化模型和分词器    # 加载预训练数据集，传入最大序列长度和分词器    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=args.max_seq_len)    # 如果是分布式训练，使用 DistributedSampler 保证各进程不重复采样数据    train_sampler = DistributedSampler(train_ds) if ddp else None    train_loader = DataLoader(        train_ds,        batch_size=args.batch_size,        pin_memory=True,        drop_last=False,        shuffle=False,        num_workers=args.num_workers,        sampler=train_sampler    )    # 设置混合精度的梯度缩放器，避免数值精度问题    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))    # 使用 AdamW 优化器，适合 Transformer 结构    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)    if ddp:        model._ddp_params_and_buffers_to_ignore = {\"pos_cis\"}        model = DistributedDataParallel(model, device_ids=[ddp_local_rank])    iter_per_epoch = len(train_loader)    for epoch in range(args.epochs):        train_epoch(epoch, wandb)\n\n\ntrain_lora 模块说明\n大部分内容与 train_pretrain 一致，主要区别包括：\n在 model 部分插入了 LoRA 网络结构，以低秩形式适配预训练模型\n# 初始化模型和分词器model, tokenizer = init_model(lm_config)apply_lora(model)\n\n使用 SFT（Supervised Fine-Tuning）监督微调数据集进行训练\ntrain_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)\n\n冻结原始网络权重矩阵，只对 LoRA 结构的权重参数进行优化\nfor name, param in model.named_parameters():    # 冻结原始权重矩阵，只让 LoRA 参数参与训练    # 相当于在原来模型的输出的基础上，通过 LoRA 增加了新的扰动    if 'lora' not in name:        param.requires_grad = Falselora_params = []for name, param in model.named_parameters():    if 'lora' in name:        lora_params.append(param)     # 只对 LoRA 参数进行优化optimizer = optim.AdamW(lora_params, lr=args.learning_rate)\n\n初始化模型时从 pretrain_*.pth 加载预训练模型权重用于微调\ndef init_model(lm_config):    # 使用预训练的分词器    tokenizer = AutoTokenizer.from_pretrained('../model/')    # 初始化一个 MiniMind 因果语言模型（Causal LM）    model = MiniMindForCausalLM(lm_config)    moe_path = '_moe' if lm_config.use_moe else ''    # 加载模型权重文件（.pth），并映射到当前设备（如 cuda:0 或 cpu）    ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'    state_dict = torch.load(ckp, map_location=args.device)    model.load_state_dict(state_dict, strict=False)    return model.to(args.device), tokenizer\n\n\n\n\n\n维度\ntrain_pretrain.py（从头全参训练）\ntrain_lora.py（微调部分参数）\n\n\n\n训练目标\n从头预训练整个模型\n在全模型基础上微调（SFT）\n\n\n模型初始化\n全部参数随机初始化（或从头加载）\n加载已有全模型权重（full SFT checkpoint）\n\n\n参数更新范围\n所有可训练参数 model.parameters()\n只更新包含 'lora' 的参数\n\n\n模型保存方式\n保存全模型（含全部参数）\n仅保存 LoRA 权重（节省空间）\n\n\n训练数据集类\nPretrainDataset\nSFTDataset\n\n\n优化器配置\nAdamW(model.parameters())\nAdamW(lora_params)\n\n\n梯度裁剪对象\nmodel.parameters()\nlora_params\n\n\n模型正向传播输出\n含 res.aux_loss（假设 MOE 或其他结构用到了）\n同上（仍然计算 res.aux_loss）\n\n\nLoRA应用\n无\n明确调用 apply_lora(model)\n\n\n训练参数量\n通常为上亿\n通常为百万量级，&lt;1% 全参数量\n\n\n模型冻结逻辑\n无（全部参数可训练）\n非 LoRA 参数全部 requires_grad = False\n\n\n\nLoRA 网络结构说明：# 定义低秩增量(LoRA)网络结构class LoRA(nn.Module):    def __init__(self, in_features, out_features, rank):        super().__init__()        self.rank = rank  # LoRA的秩（rank），控制低秩矩阵的大小        # 将一个高维线性层 W ∈ R^{out×in} 拆成 B·A        # 线性降维层 A：从原输入维度 → 降到 rank        # 线性升维层 B：从 rank → 输出维度        self.A = nn.Linear(in_features, rank, bias=False)  # 低秩矩阵A        self.B = nn.Linear(rank, out_features, bias=False)  # 低秩矩阵B        # 矩阵A高斯初始化，有助于在初期训练时快速激活        self.A.weight.data.normal_(mean=0.0, std=0.02)        # 矩阵B全0初始化，初始化时 LoRA 模块输出恒为 0，不影响原始模型        self.B.weight.data.zero_()    def forward(self, x):        return self.B(self.A(x))\n\ndef apply_lora(model, rank=8):    # 输出 = 原始输出 + 可学习的增量（通过低秩矩阵表示）    # 不改变原始预训练模型的主干，但在训练过程中允许它通过“LoRA 模块”学会一些新的东西    for name, module in model.named_modules():        # 只对方阵型 Linear 层（输入输出维度相同）应用 LoRA。可以根据需要放宽条件        if isinstance(module, nn.Linear) and module.weight.shape[0] == module.weight.shape[1]:            # 实例化一个 LoRA 模块            lora = LoRA(module.weight.shape[0], module.weight.shape[1], rank=rank).to(model.device)            # 把 LoRA 模块挂载到原 Linear 层上            setattr(module, \"lora\", lora)            # 保存原始前向函数            original_forward = module.forward            # 显式绑定            # 新的前向函数：原始 Linear(x) + LoRA(x)，这就是论文提出的低秩“增量”            def forward_with_lora(x, layer1=original_forward, layer2=lora):                return layer1(x) + layer2(x)            # 替换原始 forward 方法，实现功能注入            module.forward = forward_with_lora\n\n该函数会遍历模型所有模块，查找满足条件的 nn.Linear 层，并为其插入一个对应的 LoRA 模块，并替换其 forward 函数。\ndef load_lora(model, path):    state_dict = torch.load(path, map_location=model.device)    for name, module in model.named_modules():        if hasattr(module, 'lora'):            lora_state = {k.replace(f'{name}.lora.', ''): v for k, v in state_dict.items() if f'{name}.lora.' in k}            module.lora.load_state_dict(lora_state)\n\n从给定路径中读取保存的权重，只加载带有 .lora. 前缀的参数子集，并将其分别赋值到对应层上的 LoRA 模块中。\ndef save_lora(model, path):    state_dict = {}    for name, module in model.named_modules():        if hasattr(module, 'lora'):            lora_state = {f'{name}.lora.{k}': v for k, v in module.lora.state_dict().items()}            state_dict.update(lora_state)    torch.save(state_dict, path)\n\n保存所有 LoRA 模块的参数到文件。\n\n注意：\n线性升维层 B 被初始化为全 0 的目的是为了在训练初始阶段不影响模型的输出，并且没有突然的性能震荡，不需要重新 warmup\n因为原模型参数 不变或冻结，保留已有通用能力，LoRA 能在不改变主干模型的前提下高效注入任务特定知识\nMinimind 只对方阵型 Linear 层（输入输出维度相同）应用 LoRA，但是也可以根据需要放宽条件\nLoRA 是基于低秩分解的，因此在极端高宽差异（比如 in=1024, out=8）时，rank 太高可能失效\n\n\n\n\ntrain_full_sft 模块说明\n大部分内容与 train_pretrain 一致，主要区别包括：\n初始化模型时从 pretrain_*.pth 加载预训练模型权重用于微调\ndef init_model(lm_config):    # 使用预训练的分词器    tokenizer = AutoTokenizer.from_pretrained('../model/')    # 初始化一个 MiniMind 因果语言模型（Causal LM）    model = MiniMindForCausalLM(lm_config)    moe_path = '_moe' if lm_config.use_moe else ''    # 加载模型权重文件（.pth），并映射到当前设备（如 cuda:0 或 cpu）    ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'    state_dict = torch.load(ckp, map_location=args.device)    model.load_state_dict(state_dict, strict=False)    return model.to(args.device), tokenizer\n\n使用 SFT（Supervised Fine-Tuning）监督微调数据集进行训练\ntrain_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)\n\n\n\n\n\n维度\ntrain_pretrain.py（从头全参训练）\ntrain_full_sft.py（基于全参 SFT 微调）\n\n\n\n训练目标\n从头预训练整个语言模型（未监督训练）\n使用标注数据进行监督式微调（SFT）\n\n\n模型初始化\n全部参数随机初始化（或初始化结构）\n从预训练 .pth 权重中加载全模型参数\n\n\n参数更新范围\n所有可训练参数 model.parameters()\n所有可训练参数 model.parameters()\n\n\n模型保存方式\n保存全模型（通常路径如 pretrain_*.pth）\n也保存全模型（通常路径如 full_sft_*.pth）\n\n\n训练数据集类\nPretrainDataset（无监督 corpus）\nSFTDataset（带有 prompt/response 等监督结构）\n\n\n优化器配置\nAdamW(model.parameters())\nAdamW(model.parameters())\n\n\n梯度裁剪对象\nmodel.parameters()\nmodel.parameters()\n\n\n训练参数量\n全部参数（上亿）\n全部参数（上亿）\n\n\n\n\n\ntrain_dpo 模块说明\n大部分内容与 train_pretrain 一致，主要区别包括：\n将模型的输出（logits）转换为目标 token 的对数概率（log-prob）\ndef logits_to_probs(logits, labels):    # logits shape: (batch_size, seq_len, vocab_size)    # labels shape: (batch_size, seq_len)    # probs shape: (batch_size, seq_len)    log_probs = F.log_softmax(logits, dim=2)    probs = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(-1)    return probs\n\n设计 dpo_loss 使主模型在训练时更偏好生成 chosen 的回答，并且更偏离 rejected 的回答\nlogits = (chosen_probs - reject_probs) - (chosen_ref_probs - reject_ref_probs)\n\nloss = -log(σ(β * (log π(c) - log π(r) - log π_ref(c) + log π_ref(r))))\n\ndef dpo_loss(ref_probs, probs, mask, beta):    # ref_probs 和 probs 都是 shape: (batch_size, seq_len)    # beta: DPO中的温度参数，影响策略更新强度    # probs 是模型对 ground-truth token 的 log 概率（log-probability）    # 形状为 (batch_size, seq_len)，每个元素是对应位置上模型预测目标 token 的 log 概率值    # https://github.com/jingyaogong/minimind/issues/298    seq_lengths = mask.sum(dim=1, keepdim=True)  # (batch_size, 1)    ref_probs = (ref_probs * mask).sum(dim=1) / seq_lengths.squeeze()    probs = (probs * mask).sum(dim=1) / seq_lengths.squeeze()    # 将 chosen 和 rejected 数据分开    batch_size = ref_probs.shape[0]    chosen_ref_probs = ref_probs[:batch_size // 2]    reject_ref_probs = ref_probs[batch_size // 2:]    chosen_probs = probs[:batch_size // 2]    reject_probs = probs[batch_size // 2:]    pi_logratios = chosen_probs - reject_probs    ref_logratios = chosen_ref_probs - reject_ref_probs    logits = pi_logratios - ref_logratios    loss = -F.logsigmoid(beta * logits)    return loss.mean()\n\n使用自定义 DPODataset，返回含有正负样本（chosen 和 rejected）对的 batch，其中包含两个输入集合和对应标签、掩码。\ntrain_ds = DPODataset(args.data_path, tokenizer, max_length=args.max_seq_len\n\n训练阶段，train_dpo 会对 chosen 和 rejected 两部分进行拼接，然后使用主模型和参考模型对拼接后的数据进行推理，然后计算 dpo_loss，训练模型在给定参考模型的基础上更偏好人类认为好的回答，从而实现偏好对齐（preference alignment）训练目标。\nfor step, batch in enumerate(train_loader):    # x_chosen：表示某个问题对应的人类偏好的回答（positive example）    # x_rejected：表示同样问题的低质量回答（negative example）    # 模型通过比较这两者，学习「人类偏好」的模式，从而优化输出质量    x_chosen = batch['x_chosen'].to(args.device)    x_rejected = batch['x_rejected'].to(args.device)    y_chosen = batch['y_chosen'].to(args.device)    y_rejected = batch['y_rejected'].to(args.device)    mask_chosen = batch['mask_chosen'].to(args.device)    mask_rejected = batch['mask_rejected'].to(args.device)         # 将选中和拒绝的两批数据在batch维度上拼接，组成一个2倍大小的 batch    x = torch.cat([x_chosen, x_rejected], dim=0)    y = torch.cat([y_chosen, y_rejected], dim=0)    mask = torch.cat([mask_chosen, mask_rejected], dim=0)         # 计算当前 step 对应的动态学习率，随着训练进度逐步调整    lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)    for param_group in optimizer.param_groups:        param_group['lr'] = lr         # 混合精度自动转换上下文    with ctx:        # 用 ref_model 计算参考 logits，不计算梯度（固定模型）        with torch.no_grad():            ref_outputs = ref_model(x)            ref_logits = ref_outputs.logits        # 计算 ref_model 对应的概率        ref_probs = logits_to_probs(ref_logits, y)        # 应用掩码过滤无效位置        ref_probs = ref_probs * mask             # 计算当前训练模型输出        outputs = model(x)        logits = outputs.logits        probs = logits_to_probs(logits, y)        probs = probs * mask             # 计算 DPO 损失（Discriminative Preference Optimization）        loss = dpo_loss(ref_probs, probs, mask, beta=0.1)        # 梯度累积需要平均分摊损失        loss = loss / args.accumulation_steps\n\n训练时加载一个预训练模型权重作为主模型和参考模型，主模型和参考模型有相同的初始权重，但是参考模型被设置为推理模式，权重参数被冻结，不参与反向传播。在训练过程中，主模型会逐渐优化，而参考模型保持不变，作为 固定的基线 来对比偏好差异。\ndef init_model(lm_config):    tokenizer = AutoTokenizer.from_pretrained('../model/')    # 初始化主模型    model = MiniMindForCausalLM(lm_config)    moe_path = '_moe' if lm_config.use_moe else ''    ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'    state_dict = torch.load(ckp, map_location=args.device)    model.load_state_dict(state_dict, strict=False)    # 初始化参考模型    ref_model = MiniMindForCausalLM(lm_config)    ref_model.load_state_dict(state_dict, strict=False)    # eval() 设置为推理模式（禁用 Dropout、LayerNorm 训练时行为）    ref_model.eval()    # 冻结参考模型，不参与 DPO 的反向传播    ref_model.requires_grad_(False)    Logger(f'LLM总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')    model = model.to(args.device)    ref_model = ref_model.to(args.device)    return model, ref_model, tokenizer\n\n\n\n\ntrain_distillation 模块说明\n大部分内容与 train_pretrain 一致，主要区别包括：\n使用学生模型和教师模型，训练学生模型，使其同时拟合真实标签（交叉熵损失）和教师模型输出（蒸馏损失）。目标是用较小或简化的学生模型学习教师模型的知识，提高推理效率.\n# 定义学生模型和教师模型lm_config_student = MiniMindConfig(hidden_size=512, num_hidden_layers=8)lm_config_teacher = MiniMindConfig(hidden_size=768, num_hidden_layers=16)     # 初始化学生模型和教师模型model, tokenizer = init_student_model(lm_config_student)teacher_model = init_teacher_model(lm_config_teacher)\n\n使用 SFT（Supervised Fine-Tuning）监督微调数据集进行训练\ntrain_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)\n\n初始化模型时从 full_sft_*.pth 加载有监督微调后的模型权重\ndef init_student_model(lm_config):    tokenizer = AutoTokenizer.from_pretrained('../model/')    model = MiniMindForCausalLM(lm_config)    moe_path = '_moe' if lm_config.use_moe else ''    ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'    state_dict = torch.load(ckp, map_location=args.device)    model.load_state_dict(state_dict, strict=False)    Logger(f'学生模型(LLM)总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')    model = model.to(args.device)\n\n学生模型和教师模型均使用预训练权重进行初始化，两者的区别只在隐藏层维度（512/768）和层数（8/16）\nmodel = MiniMindForCausalLM(lm_config)moe_path = '_moe' if lm_config.use_moe else ''ckp = f'{args.save_dir}/full_sft_{lm_config.hidden_size}{moe_path}.pth'state_dict = torch.load(ckp, map_location=args.device)model.load_state_dict(state_dict, strict=False)\n\n使用知识蒸馏中的 KL 散度损失，用于衡量学生模型输出的概率分布和教师模型输出的概率分布之间的差异，其中教师模型 logits 除以温度后做 softmax，得到软化的概率分布。with torch.no_grad(): 保证教师模型的梯度不会被计算，.detach() 保证教师模型的梯度不会被追踪。返回值乘以温度的平方，这是知识蒸馏中标准做法，目的是对损失做适当缩放，使得温度调节后梯度大小保持合适 \ndef distillation_loss_fn(student_logits, teacher_logits, temperature=1.0, reduction='batchmean'):    with torch.no_grad():        teacher_probs = F.softmax(teacher_logits / temperature, hidden_size=-1).detach()    student_log_probs = F.log_softmax(student_logits / temperature, hidden_size=-1)    kl = F.kl_div(        student_log_probs,        teacher_probs,        reduction=reduction    )    return (temperature ** 2) * kl\n\n计算两个损失：\n\n真实标签的交叉熵损失（CE Loss）\n学生与教师输出的KL散度损失（Distillation Loss）\n\n最终损失是两者加权和：loss = alpha * CE + (1-alpha) * Distill\n# 学生模型前向传播（在autocast混合精度上下文中）        with ctx:            res = model(X)            # student_logits [batch_size, seq_len, vocab_size]            student_logits = res.logits        # 教师模型前向传播（只在eval &amp; no_grad）        if teacher_model is not None:            with torch.no_grad():                teacher_logits = teacher_model(X).logits                vocab_size_student = student_logits.size(-1)  # N                # 教师模型词表为 758，学生模型为 512，切片保持两个 logits 最后一维一致                teacher_logits = teacher_logits[..., :vocab_size_student]        # ========== 计算损失 ==========        # 1) Ground-Truth CE Loss（可选）        loss_mask_flat = loss_mask.view(-1)        ce_loss = F.cross_entropy(            student_logits.view(-1, student_logits.size(-1)),       # flatten成 [batch*seq, vocab]            Y.view(-1),     # flatten 真实标签            ignore_index=0,     # 标签中 0 的 token 忽略不计损失（padding或特殊token）            reduction='none'        # 计算每个 token 的 loss，不做均值        )        ce_loss = torch.sum(ce_loss * loss_mask_flat) / loss_mask_flat.sum()        if lm_config_student.use_moe:            ce_loss += res.aux_loss        # 2) Distillation Loss（可选）        if teacher_model is not None:            # 只在有效token位置做蒸馏，筛选loss_mask为1的token位置            distill_loss = distillation_loss_fn(                # 布尔索引只能用来对第一个维度进行筛选                student_logits.view(-1, student_logits.size(-1))[loss_mask_flat == 1],                teacher_logits.view(-1, teacher_logits.size(-1))[loss_mask_flat == 1],                temperature=temperature            )        else:            distill_loss = torch.tensor(0.0, device=args.device)        # 3) 总损失 = alpha * CE + (1-alpha) * Distill        loss = (alpha * ce_loss + (1 - alpha) * distill_loss) / args.accumulation_steps\n\n\n\n\ntrain_distill_reason 模块说明\n大部分内容与 train_pretrain 一致，主要区别包括：\n使用 SFT（Supervised Fine-Tuning）监督微调数据集进行训练。\ntrain_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)\n\n加载 人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF) 预训练模型参数。\ndef init_model(lm_config):    tokenizer = AutoTokenizer.from_pretrained('../model')    model = MiniMindForCausalLM(lm_config)    moe_path = '_moe' if lm_config.use_moe else ''    ckp = f'{args.save_dir}/rlhf_{lm_config.hidden_size}{moe_path}.pth'    state_dict = torch.load(ckp, map_location=args.device)    model.load_state_dict(state_dict, strict=False)    Logger(f'LLM总参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f} 百万')    model = model.to(args.device)    return model, tokenizer\n\n在传统的监督训练基础上，通过加权 loss，使模型更加关注 &lt;think&gt;...&lt;/think&gt; 和 &lt;answer&gt;...&lt;/answer&gt; 标签包裹的内容，从而强化对思维过程和最终答案的学习质量。\n\n通过人为标注 &lt;think&gt; 和 &lt;answer&gt;，并在 loss 函数中重点惩罚这些部分的预测误差，可以实现以下训练效果：\n\n增强模型生成推理链的能力\n提高答案相关内容的准确性与连贯性\n使训练 loss 更聚焦于有价值的 token\n\n# 思考标签占位符    # 获取思考/回答标签 token 的 ID    start_of_think_ids = tokenizer('&lt;think&gt;').input_ids    end_of_think_ids = tokenizer('&lt;/think&gt;').input_ids    start_of_answer_ids = tokenizer('&lt;answer&gt;').input_ids    end_of_answer_ids = tokenizer('&lt;/answer&gt;').input_ids    loss_fct = nn.CrossEntropyLoss(reduction='none')    start_time = time.time()    for step, (X, Y, loss_mask) in enumerate(train_loader):        X = X.to(args.device)        Y = Y.to(args.device)        loss_mask = loss_mask.to(args.device)        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)        for param_group in optimizer.param_groups:            param_group['lr'] = lr        with ctx:            res = model(X)            loss = loss_fct(                res.logits.view(-1, res.logits.size(-1)),                Y.view(-1)            ).view(Y.size())            # sp_ids [batch_size * seq_len]            sp_ids = torch.isin(Y.view(-1),                                # 列表拼接                                torch.tensor(start_of_think_ids + end_of_think_ids                                             + start_of_answer_ids + end_of_answer_ids                                             ).to(args.device))            # 在 sp_ids 对应的位置增加额外的惩罚            loss_mask = loss_mask.view(-1)            loss_mask_sum = loss_mask.sum()            # 将包含 &lt;think&gt; / &lt;answer&gt; 的 token 的 loss 放大 10 倍，其他 token 不变            # 引导模型更关注思维过程和答案的生成质量。            loss_mask[sp_ids] = 10            loss_mask = loss_mask.view(Y.size())            # 将每个 token 的 loss 乘以对应的权重            # 除以原始 mask 的总和            loss = (loss * loss_mask).sum() / loss_mask_sum            # 添加模型中定义的额外损失项            loss += res.aux_loss            loss = loss / args.accumulation_steps        scaler.scale(loss).backward()\n\n\n\n\n四、模型架构均方根归一化利用输入的均方根（Root Mean Square）来归一化每个样本的特征，而不是均值 + 方差的方式（如 LayerNorm），从而减少计算开销归一化使用最后一维，是因为这通常是 特征维度，对该维度归一化可以 规范每个 token 的表达强度，保持训练的稳定性和语义一致性\nclass RMSNorm(torch.nn.Module):    def __init__(self, dim: int, eps: float = 1e-5):        super().__init__()        self.eps = eps      # 避免除零的微小常数，默认 1e-5        self.weight = nn.Parameter(torch.ones(dim))     # 可训练的缩放参数（类似 LayerNorm 的 gamma 权重），初始化为 1    # L2 范数归一化    def _norm(self, x):        # 每个样本的最后一维上独立归一化（和 LayerNorm 类似）        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)    def forward(self, x):        # x.float() 将输入转为 float32 防止数值不稳定        # .type_as(x) 最后转换回原来的数据类型（如 float16 或 bfloat16）        return self.weight * self._norm(x.float()).type_as(x)\n\n\n\n\n特性\nLayerNorm\nRMSNorm\n\n\n\n使用均值\n✅ 是\n❌ 否\n\n\n使用方差\n✅ 是（带均值）\n✅ 是（仅平方均值）\n\n\n是否有偏置参数\n✅ 有 bias\n❌ 无 bias\n\n\n计算复杂度\n高\n更低\n\n\n效果\n更稳定（但略慢）\n接近甚至优于 LayerNorm（LLM场景）\n\n\n\n旋转位置编码(26 封私信 / 80 条消息) 一文看懂 LLaMA 中的旋转式位置编码（Rotary Position Embedding） - 知乎\nRotary Positional Embedding（RoPE，旋转位置编码）是一种相对位置编码方法，它通过向量旋转将位置信息引入到 Transformer 模型中，是目前 LLaMA、ChatGLM、GPT-NeoX 等大模型的主流选择。RoPE 将 token 的表示视为二维坐标，通过不同位置的旋转角度差异引入相对位置信息，从而避免传统位置编码在超长序列下的泛化差问题。\n1. 向量旋转构造相对位置差设  是 token 的向量（比如 Q 或 K 向量），我们把它看作由  个二维向量组成：每个  被视为二维空间中一个点，我们要对它做一个旋转变换:$$\\operatorname{Rot}{\\theta}(x^{(i)}) =  \\begin{bmatrix}x{2i} \\x_{2i+1}\\end{bmatrix}其中旋转角度与位置有关：\\theta_i = \\text{position} \\cdot \\frac{1}{10000^{2i/d}}$$，这类似于 sinusoidal 位置编码的频率结构。\n2. 引入相对位置信息的机制当对 query 和 key 都进行这种按位置的旋转编码，那么它们点积时，旋转角度的差值正好反映了两个 token 之间的相对位置关系。设:\n\n：第  个位置的 query 向量\n：第  个位置的 key 向量\n\n它们 RoPE 编码后的点积满足如下性质：，也就是说，RoPE 将注意力从绝对位置依赖转为显式建模相对位置差 ，这是它最大的优势。\n传统位置编码只告诉模型“这是第几个 token”；RoPE 则让模型知道“它和别的 token 相差多少”，后者更适合超长文本建模。\n3. 传统位置编码在超长序列下的泛化问题\nSinusoidal 绝对位置编码不可泛化\n位置编码由固定公式生成，与模型参数无关：\n\n虽然可推算任意位置的编码，但：\n\n模型训练只见过 pos ≤ 2048，遇到 pos = 8192 时模式未见过，无法泛化。\n\n\n\n\n无法建模相对位置关系\n每个位置的编码是唯一的，与上下文无关；\n对于两个语义上相关的 token，若它们位置差很远（如 512 vs 6144），编码将完全不同；\n模型难以感知 token 之间的距离或对齐关系。\n\n\nLearnable 位置编码不支持超长序列\n如 BERT 的 learnable embedding 表为 max_position × hidden_dim；\n超出 max_position 的位置在推理时直接 查不到（OOV）；\n无法泛化，硬性限制最大序列长度。\n\n\n\n注意力机制\n1. 多查询注意力（MQA）结构支持self.num_key_value_heads = args.num_key_value_heads or args.num_attention_heads\n\n\n如果启用 num_key_value_heads &lt; num_attention_heads，表示使用 多查询注意力（MQA），从而减少内存开销；\nrepeat_kv 函数将 KV 复制多份来适配 Q 的维度；\n优点：更节省显存，特别适合多请求并发场景（如推理部署）。\n\n\n2. Rotary Position Embedding (RoPE)xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])\n\n\n对 query 和 key 应用 旋转位置编码，将位置信息通过旋转矩阵注入向量；\n相比传统位置编码更适合超长文本处理，具有良好的泛化性。\n\n\n3. KV Cache 增量拼接if past_key_value is not None:    xk = torch.cat([past_key_value[0], xk], dim=1)    xv = torch.cat([past_key_value[1], xv], dim=1)\n\n\n用于 加速解码阶段的推理，只需拼接新 token 的 key/value；\n避免重复计算已有 token 的 KV，提高生成效率。\n\n\n4. Flash Attention 支持if self.flash and seq_len != 1:    F.scaled_dot_product_attention(...)\n\n\n支持使用 torch 2.0+ 的 Flash Attention 实现，显著提升显存利用率和速度；\n自动降级到普通 attention 逻辑；\n非常适合长文本训练/推理。\n\n\n5. Causal Mask + Padding Mask 双重屏蔽scores = scores + torch.triu(...)  # causalscores = scores + extended_attention_mask  # padding\n\n\n保证每个位置只能看到自己和之前位置（自回归）；\n忽略 padding 等无效 token 对注意力的干扰。\n\nxq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)        cos, sin = position_embeddings        # 对q和k应用旋转位置编码（rotary positional embedding），用于提升模型对位置信息的感知        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])        # kv_cache实现        # 在生成推理时复用之前的key和value，拼接当前step的key和value，形成完整的上下文key和value        # 根据是否使用缓存，决定是否返回最新的缓存用于后续步骤        # 加快自回归模型生成 token 时的计算速度        if past_key_value is not None:            xk = torch.cat([past_key_value[0], xk], dim=1)            xv = torch.cat([past_key_value[1], xv], dim=1)        past_kv = (xk, xv) if use_cache else None        # 为了后续计算注意力时，将 batch 和 head 维度分开处理，方便矩阵乘法        # 因为 k 和 v 的头数 num_key_value_heads 比 q 和头数 num_attention_heads，需要重复 n_rep 次来对齐        xq, xk, xv = (            xq.transpose(1, 2),            repeat_kv(xk, self.n_rep).transpose(1, 2),            repeat_kv(xv, self.n_rep).transpose(1, 2)        )        # 是否启用 Flash Attention        if self.flash and seq_len != 1:            dropout_p = self.dropout if self.training else 0.0            attn_mask = None            # 如果传入了 attention_mask，调整其形状以适配 Flash Attention 的要求            # 变成 (bsz, num_heads, seq_len, seq_len) 布尔类型            if attention_mask is not None:                attn_mask = attention_mask.view(bsz, 1, 1, -1).expand(bsz, self.n_local_heads, seq_len, -1)                attn_mask = attn_mask.bool() if attention_mask is not None else None            # 调用 PyTorch 的内置高效 scaled dot-product attention            # 带 dropout 和因果掩码（is_causal=True，保证每个位置只能看到之前位置，防止信息泄露）            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=True)        else:            # xq (batch_size, num_heads, seq_len, hidden_dim)            # xk (batch_size, num_heads, seq_len, hidden_dim)            # scores (batch_size, num_heads, seq_len, seq_len)            # 对于每个 batch 和每个注意力头（head），计算的是：            # query 序列中每个位置的向量，与 key 序列中每个位置的向量的相似度分数矩阵            # 所以得到的 scores 是一个大小为 (seq_len, seq_len) 的矩阵，表示序列中每个位置与所有其他位置的注意力分数            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)            # 上三角矩阵掩码，实现自回归，保证位置 i 只能关注到当前位置和之前位置的内容            scores = scores + torch.triu(                torch.full((seq_len, seq_len), float(\"-inf\"), device=scores.device),                diagonal=1            ).unsqueeze(0).unsqueeze(0)  # scores+mask            # 应用额外的注意力掩码            # attention_mask 形状通常是 (batch_size, seq_len)，里面元素是 1（有效位置）或 0（padding位置）            # 目的是让模型忽略 padding 位置，避免无效信息影响注意力分数            if attention_mask is not None:                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)                # 将 attention_mask 中的 1 和 0 互换，再乘以 -1e9，使无效位置的分数很小                # 通过对注意力分数加负无穷的方式，实现对padding等无效位置的屏蔽，保证模型不关注这些无意义的token                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9                scores = scores + extended_attention_mask            # 对最后一个维度做 softmax，得到归一化的注意力权重            # attn_dropout 随机丢弃一些权重，防止过拟合            scores = F.softmax(scores.float(), dim=-1).type_as(xq)            scores = self.attn_dropout(scores)            output = scores @ xv\n\nFeedForward\n中间层维度自动计算 + 对齐intermediate_size = int(config.hidden_size * 8 / 3)config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)\n\n\n自动设置 intermediate_size = 8/3 * hidden_size，并向上对齐到 64 的倍数；\n有利于 内存对齐、计算加速。\n\n\n2. 门控结构：gate_proj + up_projself.gate_proj = nn.Linear(...)self.up_proj = nn.Linear(...)output = act_fn(gate_proj(x)) * up_proj(x)\n\n\ngate_proj(x) + 激活函数后在 0~1 之间 → 作为“门”；\nup_proj(x) 提供候选信息；\n两者逐元素乘 → 控制哪些信息通过；\n提升信息筛选能力、降低冗余计算，效果优于传统 MLP。\n\n\n3. Dropout + 下采样return self.dropout(self.down_proj(...))\n\n\n最后用 down_proj 投影回原维度；\nDropout 防止过拟合。\n\nclass FeedForward(nn.Module):    # 提升模型的表达能力与信息选择性    def __init__(self, config: MiniMindConfig):        super().__init__()        # 计算中间层维度 intermdeidate_size (如果未指定)        # intermediate_size = hidden_size * 8/3，且向上对齐 64 的整数倍        if config.intermediate_size is None:            intermediate_size = int(config.hidden_size * 8 / 3)            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)        # gate_proj: 将输入从hidden_size 映射到 intermediate_size，作为门控输入        # 经过一个线性变换，得到一个门控向量        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)        # down_proj：将激活后中间层映射回 hidden_size        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)        # up_proj：另一条线性映射，将输入映射到 intermediate_size，与 gate_proj 输出相乘        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)        # dropout层        self.dropout = nn.Dropout(config.dropout)        # 激活函数        self.act_fn = ACT2FN[config.hidden_act]    def forward(self, x):        # 计算 门控向量 gate_proj(x) 和 信息传递向量 up_proj(x)        # gate_proj(x) 经过激活函数 act_fn，输出范围变为（0，1），用来筛选信息        # 两者逐元素相乘，相当于“门控机制”，控制信息流        # 再经过 down_proj 降维回 hidden_size        # 最后 dropout        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))\n\n专家混合机制MoE(Mixture-of-Experts)大模型架构的优势是什么？为什么？\nMoE（Mixture of Experts，专家混合机制）是一种 稀疏激活 的深度学习结构，它的核心思想是：\n\n在每次前向传播时，只激活部分子模型（专家），以提升模型容量的同时降低计算成本。\n\n\nMoE 的一个显著优势是它们能够在远少于 Dense 模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。\nMoE基于Transformer架构，主要由两部分组成：\n\n**稀疏 MoE 层：**这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构。\n\n门控网络或路由: 这个部分用于决定哪些 token 被发送到哪个专家。例如，在下图中，“More”这个 token 可能被发送到第二个专家，而“Parameters”这个 token 被发送到第一个专家。有时，一个 token 甚至可以被发送到多个专家。token 的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。\n\n\n\n\n五、数据集\n\n\n数据集类名\n用于阶段\n样本结构\n核心训练目标\n损失计算位置\n主要使用方法\n特殊处理逻辑 / 技术点\n\n\n\nPretrainDataset\n无监督预训练\n{ \"text\": \"...\" }\n语言建模（预测下一个 token）\n除去 padding 位置的全部\n用于 LM 预训练阶段\n利用 .input_ids[:-1] 和 .input_ids[1:] 构造训练对；padding mask 控制损失计算\n\n\nSFTDataset\n监督微调（SFT）\n{ \"conversations\": [...] }\n模拟对话，优化 assistant 回复\n仅 assistant 回复部分\n用于 supervised finetune\n通过 `&lt;\n\n\nDPODataset\n偏好微调（DPO）\n{ \"chosen\": [...], \"rejected\": [...] }\n学习选择更优回答（无需 reward 模型）\nchosen vs rejected 的回复部分\n用于 DPO loss 训练\n返回两个输入对 (x/y/mask)，分别传入 policy 与 reference model，比较 log prob 差异\n\n\nRLAIFDataset\n强化学习阶段（RLHF）\n{ \"conversations\": [...] }\n基于反馈 reward 优化策略（如 PPO）\n生成阶段 + 回复打分\n用于 RL rollout / PPO\n返回 (prompt, answer) 对，reward 模型根据 answer 打分，适用于在线或离线 RL\n\n\n","categories":["阅读笔记"],"tags":["Pytorch","深度学习","大模型"]}]